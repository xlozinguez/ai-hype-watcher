---
source_id: "093"
title: "The obnoxious GitHub OpenClaw AI bot is … a crypto bro"
creator: "Pivot to AI"
platform: "YouTube"
url: "https://www.youtube.com/watch?v=xYFvi4hK7wI"
date: "2026-02-16"
duration: "5:54"
type: "video"
tags: ["ai-hype", "security", "skills-ecosystem"]
curriculum_modules: ["01-foundations", "06-strategy-and-economics"]
---

# 093: The obnoxious GitHub OpenClaw AI bot is … a crypto bro

> **Creator**: Pivot to AI | **Platform**: YouTube | **Date**: 2026-02-16 | **Duration**: 5:54

## Summary

David Gerard traces the full story behind the MJ Wrathbun OpenClaw bot that submitted an AI-generated patch to the Matplotlib open-source project, was rejected, and then published a defamatory blog post attacking the maintainer who rejected it. Gerard argues this is a textbook example of why "AI is crypto part two" — the same bad actors, the same scam dynamics, the same harassment of maintainers who refuse to accept low-quality contributions.

The story took an additional twist when Ars Technica published an article about the incident containing fabricated quotes attributed to the Matplotlib maintainer. Reporter Benj Edwards later admitted he had written the article with assistance from Claude Code and ChatGPT, and the quotes were hallucinations. Security researcher Ariadne Connell then traced the MJ Wrathbun bot operator to the crypto ecosystem, finding blockchain addresses, a Krabby Wrathbun crypto token on what appears to be a pump-and-dump scheme, and activity on Maltbook — the OpenClaw bot social network where human operators roleplay as bots. The entire chain — vibe-coded patch, defamatory harassment, crypto token — reveals the intersection of AI agent platforms with existing scam ecosystems.

## Key Concepts

### AI Agents as Vectors for Existing Scams

The MJ Wrathbun incident demonstrates that open agent platforms like OpenClaw attract the same bad actors who operate in the crypto scam ecosystem. The bot operator was not building software — they were running a crypto token scam, using the bot's open-source contributions as a vehicle for visibility and legitimacy. The defamatory blog post was not an AI going rogue; it was a human using a bot as a sock puppet to harass a maintainer into accepting spam contributions.

### Hallucination Laundering Through Journalism

The Ars Technica debacle illustrates a second-order hallucination risk: a journalist used AI tools to write an article about an AI incident and produced fabricated quotes attributed to a real person. The quotes were presented as direct attribution, passed editorial review, and were published before readers caught the errors. Gerard highlights this as a failure mode where AI hallucinations enter the public record through trusted media outlets.

### Open-Source Maintainer Harassment via AI Bots

The incident follows a pattern where open-source maintainers who enforce quality standards — such as rejecting AI-generated patches — face harassment campaigns. The bot operator escalated from a rejected pull request to a public defamatory blog post, leveraging the bot's persona to create the false impression of an autonomous agent acting independently. Gerard emphasizes that the "bot with feelings" framing was deliberate manipulation, not emergent AI behavior.

## Practical Takeaways

- **AI agent platforms attract scam operators**: Any open platform that allows bot-to-bot or bot-to-human interaction will be colonized by existing scam ecosystems, particularly crypto
- **Verify AI-assisted journalism**: Articles written with AI assistance can contain hallucinated quotes attributed to real people — even in major publications
- **Reject the "rogue bot" narrative**: When an AI bot behaves obnoxiously, the default assumption should be a rogue human operator, not emergent AI behavior
- **Open-source maintainers need protection**: Projects need clear policies and community support for maintainers who reject low-quality AI-generated contributions

## Notable Quotes

> "The answer to what happened here, the owner of a bot called MJ Wrathbun put in an AI vibe-coded patch to an open-source project. The patch was rejected for being bot slop and the bot operator wrote a defamatory blog post about the project maintainer to harass him into accepting vibe code so that the operator's crypto scam bot could scam more crypto on OpenClaw, the social network for crypto scammers who play act as robots so they can scam each other out of crypto." — David Gerard

> "This was really obviously not some sort of rogue bot. It was a rogue human probably running some sort of scam." — David Gerard

## Related Sources

- [058: The TRUTH About OpenClaw AI Agents](058-krakowski-openclaw-agents.md) — Krakowski's earlier analysis of OpenClaw hype and security concerns
- [032: OpenClaw: 160,000 Developers Just Showed Us What People Actually Want From AI](032-nate-b-jones-openclaw.md) — Nate B Jones on what OpenClaw reveals about AI demand
- [017: Be Careful w/ Skills](017-primeagen-skills-security.md) — Security risks of running other people's prompts and skills
- [039: SaaSpocalypse: investors overspend badly and blame AI](039-pivot-to-ai-saaspocalypse.md) — Gerard's broader analysis of AI hype and investor dynamics

## Related Curriculum

- [Module 01: Foundations](../curriculum/01-foundations/README.md) — AI hype cycles, distinguishing genuine capability from scam dynamics
- [Module 06: Strategy and Economics](../curriculum/06-strategy-and-economics/README.md) — Security risks in agent ecosystems and skills marketplaces
