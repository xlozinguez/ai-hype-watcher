# Raw Transcript: 041

[0:00] AI agents just dropped another banger of a product and this one is even funnier than previous attempts. If you are following the tech news, you probably know that the new trend now is to push the AI agenda by having AI agents work independently for weeks at a time in an attempt to create a barely working prototype. The results are questionable at best. The experiment burns through tens of thousands of dollars and the AI companies declare the entire thing a great success. All these just to justify the trillions of dollars investors are pouring into the idea that software

[0:30] pouring into the idea that software development is about to become fully autonomous. A few weeks back, Cursor CEO made a really big deal about cursor agents independently building a browser from scratch. Of course, that was just another AI marketing stunt. What they actually accomplished was to generate 3 million lines of slop code that leveraged a bunch of open- source software and still failed to do the most basic browser tasks. And now it's Anthropic's turn to push forward their marketing stunt. In an article titled building a C compiler with a team of parallel clouds, they explain that agent

[1:00] parallel clouds, they explain that agent teams, which are basically multiple cloud instances that work in parallel on a shared codebase, are one of the ways in which we'll be able to overcome the obvious limitations the current LLM models have reached. The project was assigned to 16 agents which were tasked with writing a Rasb compiler capable of compiling the Linux kernel. And after nearly 2,000 code sessions and $20,000 in API cost, the agent team produced a 100,000line compiler. Okay, granted that's pretty impressive. Since metrics

[1:30] that's pretty impressive. Since metrics like number of lines of code are all of a sudden relevant again, it's worth mentioning that the cost for generating one line of code was around $5, which is pretty funny when you realize that the resulting compiler sometimes has problems compiling the Hello World program. But we don't have to take a Redditor's word for it because the main problems of the compiler are actually clearly stated in the article. Props to Anthropic for being transparent about it. First of all, the result lacks the 16-bit compiler necessary to boot Linux out of real mode, so it has to rely on

[2:00] out of real mode, so it has to rely on GCC for it. However, the AI was able to build 32 and 64-bit compilers, most likely because these are much better documented and even replicated on the web. We'll get back to this later. Second of all, it does not have its own assembler and linker. So, their demos are produced with the GCC assembler and linker. For those unfamiliar with compilers, the assembler and the linker are actually the parts that turn the compiler from a demo into a usable tool. The compiler translates source code into intermediate assembly. The assembler

[2:30] intermediate assembly. The assembler converts that assembly into machine code. The linker then resolves symbols, combines object files, and produces an executable that can actually run on a system. Without those pieces, you only have a partial pipeline that still depends on existing battle tested software to finish the job. But it gets worse because the compiler successfully builds many projects, but not all. And the generated code is not very efficient. Even with all optimizations enabled, it outputs less efficient code than GCC with all optimizations disabled. So, we got a company that

[3:00] disabled. So, we got a company that spent $20,000 to build an incomplete compiler that's magnitudes worse than the previous compiler and sometimes fails to compile even simple projects. And don't think that the $20,000 was the limit here. Anthropic has pretty much infinite resources at this point and they would probably be willing to pay 10 times that amount if these AI agent teams were capable of producing something meaningfully competitive with existing tools. But that's precisely where the illusion breaks down because the entire framing of the experiment quietly shifts the definition of success. The goal is no longer to

[3:30] success. The goal is no longer to produce a better or even a working compiler. The goal is to demonstrate that something resembling a compiler can emerge from autonomous iteration. One particular paragraph stood out to me. I tried hard to fix several of the above limitations, but wasn't fully successful. New features and bug fixes frequently broke existing functionality. This is the state-of-the-art model Dario promises it will replace all developers in 6 months. And this is not even the sad part. These models are trained on

[4:00] sad part. These models are trained on publicly available articles, books, and GitHub repositories. And the topic of creating a compiler is one of the most thoroughly documented problems in computer science. Compiler construction has decades of textbooks, courses, reference implementations, and production grade open source code. Don't get me wrong, it is not an easy task, but it is a task thoroughly documented and detailed every step of the way. And that is exactly why this experiment is more revealing than impressive. If a system trained on enormous amounts of publicly available knowledge, struggles

[4:30] publicly available knowledge, struggles in a domain where the theory, the architecture, and even the common pitfalls are already written down in detail, then all these stories about artificial intelligence, reasoning, and AGI are really starting to crack. So, it's probably safe to say that no agent is coming for our job in the near future. If you like this video, you should consider joining our community where I'm posting more dedicated content. Please don't forget to smash all the buttons.
