# Raw Transcript: Before You Build Another Agent, Understand This MIT Paper

- **URL**: https://www.youtube.com/watch?v=m1Tc5Xzw1tM
- **Duration**: 17:47
- **Captured**: 2026-02-13
- **Method**: yt-dlp VTT extraction

---

[0:01]

agents can finally handle high agents can finally handle high complexity work outside of software complexity work outside of software complexity work outside of software engineering. Honestly, this feels like engineering. Honestly, this feels like engineering. Honestly, this feels like the reasoning moment all over again, but the reasoning moment all over again, but the reasoning moment all over again, but for AI agents. What's most surprising is for AI agents. What's most surprising is for AI agents. What's most surprising is the fix is almost absurdly simple. Code the fix is almost absurdly simple. Code the fix is almost absurdly simple. Code execution plus recursion. I've read execution plus recursion. I've read execution plus recursion. I've read through the RLM's paper and distilled it through the RLM's paper and distilled it through the RLM's paper and distilled it into the important mental models that into the important mental models that into the important mental models that you're going to need to apply this. So you're going to need to apply this. So you're going to need to apply this. So the first mental model that I kind of the first mental model that I kind of the first mental model that I kind of want to communicate is why context want to communicate is why context want to communicate is why context length is only half the size when we're length is only half the size when we're length is only half the size when we're dealing with these high complexity dealing with these high complexity dealing with these high complexity tasks. So one of the insights from the

[0:32]

tasks. So one of the insights from the tasks. So one of the insights from the RLM paper is that it's not just about RLM paper is that it's not just about RLM paper is that it's not just about context length. It's not just about context length. It's not just about context length. It's not just about having legal contracts that are like a having legal contracts that are like a having legal contracts that are like a million tokens long, let's say, or million tokens long, let's say, or million tokens long, let's say, or having data rooms that span across having data rooms that span across having data rooms that span across millions of tokens or large code bases. millions of tokens or large code bases. millions of tokens or large code bases. It's also about the inherent complexity It's also about the inherent complexity It's also about the inherent complexity of those documents of those types of those documents of those types of those documents of those types of data assets. So if you look at legal data assets. So if you look at legal data assets. So if you look at legal contracts, take a merger agreement for contracts, take a merger agreement for contracts, take a merger agreement for example, there are a lot of internal example, there are a lot of internal example, there are a lot of internal self- references that introduce a high self- references that introduce a high self- references that introduce a high degree of complexity of that type of

[1:02]

degree of complexity of that type of degree of complexity of that type of asset. So it turns out you can't read asset. So it turns out you can't read asset. So it turns out you can't read those things just like you read a book those things just like you read a book those things just like you read a book end to end. There are clauses that end to end. There are clauses that end to end. There are clauses that reference other clauses that might have reference other clauses that might have reference other clauses that might have come earlier or might come later. There come earlier or might come later. There come earlier or might come later. There are things in there that create that are things in there that create that are things in there that create that complex structure. So context window is complex structure. So context window is complex structure. So context window is basically only half the story. The other basically only half the story. The other basically only half the story. The other half of the story is task complexity. half of the story is task complexity. half of the story is task complexity. And when we talk about task complexity, And when we talk about task complexity, And when we talk about task complexity, we're talking specifically about the we're talking specifically about the we're talking specifically about the complexity of the documents we're trying complexity of the documents we're trying complexity of the documents we're trying to have the agent work with. So let's

[1:33]

to have the agent work with. So let's to have the agent work with. So let's talk a little bit about why LLMs have talk a little bit about why LLMs have talk a little bit about why LLMs have struggled with this kind of task struggled with this kind of task struggled with this kind of task complexity and high context workloads. complexity and high context workloads. complexity and high context workloads. It's called context rot. So context rot It's called context rot. So context rot It's called context rot. So context rot is this phenomenon where the more is this phenomenon where the more is this phenomenon where the more context you stuff into a large language context you stuff into a large language context you stuff into a large language model the more the performance model the more the performance model the more the performance deteriorates and before we were looking deteriorates and before we were looking deteriorates and before we were looking at context rot kind of unid at context rot kind of unid at context rot kind of unid dimensionally as just a function of dimensionally as just a function of dimensionally as just a function of context but it's actually a function of context but it's actually a function of context but it's actually a function of context and task complexity. So a model context and task complexity. So a model context and task complexity. So a model that has a million token context will that has a million token context will that has a million token context will deteriorate long before that million

[2:05]

deteriorate long before that million deteriorate long before that million tokens is hit if your task complexity is tokens is hit if your task complexity is tokens is hit if your task complexity is also high. that leads to a lot of also high. that leads to a lot of also high. that leads to a lot of instability in how effectively a model instability in how effectively a model instability in how effectively a model can utilize its context. There's also can utilize its context. There's also can utilize its context. There's also another misconception. So we've had this another misconception. So we've had this another misconception. So we've had this kind of lost middle problem for a while kind of lost middle problem for a while kind of lost middle problem for a while and a lost in the middle problem is and a lost in the middle problem is and a lost in the middle problem is basically give a large language model a basically give a large language model a basically give a large language model a huge amount of context, put some needles huge amount of context, put some needles huge amount of context, put some needles in there. So to represent a needle in a in there. So to represent a needle in a in there. So to represent a needle in a haystack which is basically small pieces haystack which is basically small pieces haystack which is basically small pieces of information that the model has to of information that the model has to of information that the model has to retrieve from that large context and retrieve from that large context and retrieve from that large context and that could be distributed anywhere that could be distributed anywhere that could be distributed anywhere across the context and assess how well

[2:37]

across the context and assess how well across the context and assess how well the model is able to pull that back. So the model is able to pull that back. So the model is able to pull that back. So this has been a relatively solved this has been a relatively solved this has been a relatively solved problem for a while now and the RLS problem for a while now and the RLS problem for a while now and the RLS paper does reference that but what we're paper does reference that but what we're paper does reference that but what we're talking about is different. We're not talking about is different. We're not talking about is different. We're not talking about retrieving needles from talking about retrieving needles from talking about retrieving needles from haystacks. We're talking about reasoning haystacks. We're talking about reasoning haystacks. We're talking about reasoning over document complexity which is an over document complexity which is an over document complexity which is an entirely different problem. So it's a entirely different problem. So it's a entirely different problem. So it's a different class of problem because it different class of problem because it different class of problem because it requires something called multihop requires something called multihop requires something called multihop reasoning. And you know I experimented a reasoning. And you know I experimented a reasoning. And you know I experimented a while back with a chatbot called Jared while back with a chatbot called Jared while back with a chatbot called Jared to build multihop reasoning. And what I to build multihop reasoning. And what I to build multihop reasoning. And what I found was that the scaffolds I used found was that the scaffolds I used found was that the scaffolds I used whether I was using langraph or anything

[3:08]

whether I was using langraph or anything whether I was using langraph or anything else were really fragile. Once you got else were really fragile. Once you got else were really fragile. Once you got to two or three hops the chatbot would to two or three hops the chatbot would to two or three hops the chatbot would completely break down. That is what completely break down. That is what completely break down. That is what you're doing when you're analyzing a lot you're doing when you're analyzing a lot you're doing when you're analyzing a lot of these legal agreements or a lot of these legal agreements or a lot of these legal agreements or a lot of these complex policy documents. You're these complex policy documents. You're these complex policy documents. You're doing multihop reasoning. you're looking doing multihop reasoning. you're looking doing multihop reasoning. you're looking at one part and then having to reason at one part and then having to reason at one part and then having to reason about it and then go and find another about it and then go and find another about it and then go and find another part that might be relevant and reason part that might be relevant and reason part that might be relevant and reason about that and applying some conditions about that and applying some conditions about that and applying some conditions to it to do your analysis. Raw language to it to do your analysis. Raw language to it to do your analysis. Raw language models deteriorate when the task models deteriorate when the task models deteriorate when the task requires that type of complex multihop requires that type of complex multihop requires that type of complex multihop reasoning. They can do it over a small reasoning. They can do it over a small reasoning. They can do it over a small context, but as soon as you extend that

[3:39]

context, but as soon as you extend that context, but as soon as you extend that context, you see quickly that the context, you see quickly that the context, you see quickly that the performance falls off a cliff. What this performance falls off a cliff. What this performance falls off a cliff. What this leads to is models that are confidently leads to is models that are confidently leads to is models that are confidently wrong. And that's the quickest way to wrong. And that's the quickest way to wrong. And that's the quickest way to break down trust in any AI agent that break down trust in any AI agent that break down trust in any AI agent that you produce. So, let's talk a bit about you produce. So, let's talk a bit about you produce. So, let's talk a bit about the strategies that have been tried in the strategies that have been tried in the strategies that have been tried in the past. The first one is the naive one the past. The first one is the naive one the past. The first one is the naive one of just simply taking everything and of just simply taking everything and of just simply taking everything and stuffing it into an LLM and hoping and stuffing it into an LLM and hoping and stuffing it into an LLM and hoping and praying for the best. So, I've explained praying for the best. So, I've explained praying for the best. So, I've explained before the context rock phenomenon, and before the context rock phenomenon, and before the context rock phenomenon, and you should understand it's pretty you should understand it's pretty you should understand it's pretty self-explanatory why this doesn't work. self-explanatory why this doesn't work. self-explanatory why this doesn't work. It is also incredibly expensive and slow It is also incredibly expensive and slow It is also incredibly expensive and slow and have been shown to lead to subpar

[4:10]

and have been shown to lead to subpar and have been shown to lead to subpar results. Just stuffing more context in a results. Just stuffing more context in a results. Just stuffing more context in a model can actually reduce reliability model can actually reduce reliability model can actually reduce reliability rather than improve it because sometimes rather than improve it because sometimes rather than improve it because sometimes that context is noise. So sometimes you that context is noise. So sometimes you that context is noise. So sometimes you are actually burying the signal even are actually burying the signal even are actually burying the signal even further. The next approach that we further. The next approach that we further. The next approach that we explore is summarization. You would have explore is summarization. You would have explore is summarization. You would have dealt with summarization if you've used dealt with summarization if you've used dealt with summarization if you've used claude because they have this feature claude because they have this feature claude because they have this feature called the autocompat feature. What that called the autocompat feature. What that called the autocompat feature. What that autocompat feature will do is once you autocompat feature will do is once you autocompat feature will do is once you reach a certain saturation of the reach a certain saturation of the reach a certain saturation of the context window, the large language model context window, the large language model context window, the large language model itself will hand off that context to itself will hand off that context to itself will hand off that context to another model which will compact and

[4:42]

another model which will compact and another model which will compact and summarize it freeing up the context of summarize it freeing up the context of summarize it freeing up the context of the main model to continue work based on the main model to continue work based on the main model to continue work based on that summary. Now the problem with this that summary. Now the problem with this that summary. Now the problem with this is that summarization is lossy. So is that summarization is lossy. So is that summarization is lossy. So information is lost about the task and information is lost about the task and information is lost about the task and deciding which information to keep and deciding which information to keep and deciding which information to keep and which to get rid of to get an effective which to get rid of to get an effective which to get rid of to get an effective summary is a difficult task in and of summary is a difficult task in and of summary is a difficult task in and of itself. And this has been shown by the itself. And this has been shown by the itself. And this has been shown by the R&amp;M's paper to actually be an expensive R&amp;M's paper to actually be an expensive R&amp;M's paper to actually be an expensive approach because that resummarization approach because that resummarization approach because that resummarization requires the entire context of the requires the entire context of the requires the entire context of the previous task to do. So what ends up previous task to do. So what ends up previous task to do. So what ends up happening with summarization is you

[5:13]

happening with summarization is you happening with summarization is you often lose important context in the often lose important context in the often lose important context in the summary and then the agent will summary and then the agent will summary and then the agent will gradually drift off task until it's way gradually drift off task until it's way gradually drift off task until it's way off from where it initially started off from where it initially started off from where it initially started with. And some of you might have already with. And some of you might have already with. And some of you might have already experienced this with C code. People experienced this with C code. People experienced this with C code. People have come up with scaffolds to present have come up with scaffolds to present have come up with scaffolds to present this but a lot of the time what people this but a lot of the time what people this but a lot of the time what people are trying to do is they're trying to are trying to do is they're trying to are trying to do is they're trying to create some kind of elaborate create some kind of elaborate create some kind of elaborate summarization which is brittle in the summarization which is brittle in the summarization which is brittle in the end. Another approach that was used is end. Another approach that was used is end. Another approach that was used is rag retrieval augmented generation. So rag retrieval augmented generation. So rag retrieval augmented generation. So this came in really I would say 2022 this came in really I would say 2022 this came in really I would say 2022 2023 when people started to really see 2023 when people started to really see 2023 when people started to really see the power of large language models and

[5:45]

the power of large language models and the power of large language models and what rag was trying to solve was what rag was trying to solve was what rag was trying to solve was extremely small context windows of extremely small context windows of extremely small context windows of around 8K. Rag turned out to be quite around 8K. Rag turned out to be quite around 8K. Rag turned out to be quite powerful for things like question and powerful for things like question and powerful for things like question and answer because what you're doing with answer because what you're doing with answer because what you're doing with rag is you're doing the retrieval which rag is you're doing the retrieval which rag is you're doing the retrieval which is usually just a similarity search. So is usually just a similarity search. So is usually just a similarity search. So it's looking for semantic similarity or it's looking for semantic similarity or it's looking for semantic similarity or keyword matching. So if you imagine with keyword matching. So if you imagine with keyword matching. So if you imagine with question and answer pairs, you can question and answer pairs, you can question and answer pairs, you can easily do semantic matching on the easily do semantic matching on the easily do semantic matching on the questions themselves and retrieve questions themselves and retrieve questions themselves and retrieve answers. But rag turned out to be quite answers. But rag turned out to be quite answers. But rag turned out to be quite a brittle approach when the task a brittle approach when the task a brittle approach when the task complexity grows because you're only complexity grows because you're only complexity grows because you're only doing semantic similarity for retrieval.

[6:17]

doing semantic similarity for retrieval. doing semantic similarity for retrieval. There isn't much flexibility and the There isn't much flexibility and the There isn't much flexibility and the types of things that you can pull back types of things that you can pull back types of things that you can pull back from the context are rudimentary. So for from the context are rudimentary. So for from the context are rudimentary. So for example, you're only being able to pull example, you're only being able to pull example, you're only being able to pull similar documents. should not similar documents. should not similar documents. should not necessarily be able to pull logical necessarily be able to pull logical necessarily be able to pull logical relationships that you might need to do relationships that you might need to do relationships that you might need to do that multihob reasoning that you need to that multihob reasoning that you need to that multihob reasoning that you need to do over a legal contract or over a do over a legal contract or over a do over a legal contract or over a codebase. So if any programmers you codebase. So if any programmers you codebase. So if any programmers you should think does rag actually ever make should think does rag actually ever make should think does rag actually ever make sense on a codebase. I wouldn't say it sense on a codebase. I wouldn't say it sense on a codebase. I wouldn't say it does. The other thing that makes rag does. The other thing that makes rag does. The other thing that makes rag brittle is that you have to actually brittle is that you have to actually brittle is that you have to actually decide the chunking strategy. So the decide the chunking strategy. So the decide the chunking strategy. So the chunking strategy is how you break up

[6:47]

chunking strategy is how you break up chunking strategy is how you break up and atomize that context so that it can and atomize that context so that it can and atomize that context so that it can be retrieved. You don't retrieve the be retrieved. You don't retrieve the be retrieved. You don't retrieve the whole context. You only retrieve what whole context. You only retrieve what whole context. You only retrieve what you need. The type of trunking strategy you need. The type of trunking strategy you need. The type of trunking strategy you use can change depending on the type you use can change depending on the type you use can change depending on the type of document that you have. A legal of document that you have. A legal of document that you have. A legal contract for a merger for a particular contract for a merger for a particular contract for a merger for a particular type of entity might be completely type of entity might be completely type of entity might be completely different trunking strategy to a different trunking strategy to a different trunking strategy to a research document for a pharmaceutical research document for a pharmaceutical research document for a pharmaceutical company for example. And then how do you company for example. And then how do you company for example. And then how do you expand that? How do you scale that expand that? How do you scale that expand that? How do you scale that strategy to thousands of documents that strategy to thousands of documents that strategy to thousands of documents that you might want to work with or to you might want to work with or to you might want to work with or to hundreds of documents that you might hundreds of documents that you might hundreds of documents that you might want to work with. This is what makes want to work with. This is what makes want to work with. This is what makes rag brittle when you want to scale it to

[7:18]

rag brittle when you want to scale it to rag brittle when you want to scale it to real production use cases. Now I want to real production use cases. Now I want to real production use cases. Now I want to give you some mental models about give you some mental models about give you some mental models about understanding the type of complexity understanding the type of complexity understanding the type of complexity we're dealing with. Right? So when we're we're dealing with. Right? So when we're we're dealing with. Right? So when we're talking about a legal contract or code talking about a legal contract or code talking about a legal contract or code base, we're not really talking about base, we're not really talking about base, we're not really talking about something that necessarily is going to something that necessarily is going to something that necessarily is going to be read end to end. We're talking about be read end to end. We're talking about be read end to end. We're talking about something that has a high degree of something that has a high degree of something that has a high degree of internal self- refferencing. So in a internal self- refferencing. So in a internal self- refferencing. So in a legal contract, one clause might legal contract, one clause might legal contract, one clause might reference another clause. In a codebase, reference another clause. In a codebase, reference another clause. In a codebase, one function might call another function one function might call another function one function might call another function or you might have a class abstraction or you might have a class abstraction or you might have a class abstraction that is used in multiple places in the that is used in multiple places in the that is used in multiple places in the codebase. that class can be called by

[7:50]

codebase. that class can be called by codebase. that class can be called by another function. So there's a high another function. So there's a high another function. So there's a high degree of self-referencing and that is degree of self-referencing and that is degree of self-referencing and that is what makes these types of data assets what makes these types of data assets what makes these types of data assets complex to reason over. Just think about complex to reason over. Just think about complex to reason over. Just think about it yourself. Think about following all it yourself. Think about following all it yourself. Think about following all of the different links in a legal of the different links in a legal of the different links in a legal contract or following and finding out contract or following and finding out contract or following and finding out all of the different places a function all of the different places a function all of the different places a function is used in a massive codebase. That is used in a massive codebase. That is used in a massive codebase. That is what makes it cognitively demanding and what makes it cognitively demanding and what makes it cognitively demanding and that is actually what makes it complex. that is actually what makes it complex. that is actually what makes it complex. So rather than trying to model these So rather than trying to model these So rather than trying to model these things as like a story book that you things as like a story book that you things as like a story book that you read end to end, I think it helps much

[8:21]

read end to end, I think it helps much read end to end, I think it helps much more to model these things as dependency more to model these things as dependency more to model these things as dependency graphs on the nodes. For a legal graphs on the nodes. For a legal graphs on the nodes. For a legal contract, you might have certain clauses contract, you might have certain clauses contract, you might have certain clauses and then how they relate to other and then how they relate to other and then how they relate to other clauses on the edges. On a codebase, clauses on the edges. On a codebase, clauses on the edges. On a codebase, you'll have certain functions and APIs you'll have certain functions and APIs you'll have certain functions and APIs and how they call or how they interact and how they call or how they interact and how they call or how they interact with other functions and APIs as notes. with other functions and APIs as notes. with other functions and APIs as notes. that is a much better mental model of that is a much better mental model of that is a much better mental model of mapping the complexity of the type of mapping the complexity of the type of mapping the complexity of the type of data assets we're dealing with here when data assets we're dealing with here when data assets we're dealing with here when we're talking about these real we're talking about these real we're talking about these real workflows. So in short, when we're workflows. So in short, when we're workflows. So in short, when we're modeling these types of complex long

[8:52]

modeling these types of complex long modeling these types of complex long documents, we really want to be modeling documents, we really want to be modeling documents, we really want to be modeling them as dependency graphs rather than them as dependency graphs rather than them as dependency graphs rather than just long pieces of text. So hopefully just long pieces of text. So hopefully just long pieces of text. So hopefully by now I'm building this picture of why by now I'm building this picture of why by now I'm building this picture of why the previous approaches didn't work. the previous approaches didn't work. the previous approaches didn't work. Once you understand that these things Once you understand that these things Once you understand that these things are dependency graphs and once you are dependency graphs and once you are dependency graphs and once you understand context rot, you understand context rot, you understand context rot, you understand why stuffing everything into an LLM even why stuffing everything into an LLM even why stuffing everything into an LLM even though the reasoning power is quite high though the reasoning power is quite high though the reasoning power is quite high nowadays won't work. And you understand nowadays won't work. And you understand nowadays won't work. And you understand why rag by semantic retrieval doesn't why rag by semantic retrieval doesn't why rag by semantic retrieval doesn't work. And you also understand why the work. And you also understand why the work. And you also understand why the summarization and compaction method that

[9:23]

summarization and compaction method that summarization and compaction method that we see so commonly nowadays also is we see so commonly nowadays also is we see so commonly nowadays also is brittle. So let's explain how the RLM brittle. So let's explain how the RLM brittle. So let's explain how the RLM actually changes that. So the RLM is the actually changes that. So the RLM is the actually changes that. So the RLM is the recursive language models. RLM is recursive language models. RLM is recursive language models. RLM is incredibly simple. You have what you incredibly simple. You have what you incredibly simple. You have what you call a ripple. Ripple is simply a read call a ripple. Ripple is simply a read call a ripple. Ripple is simply a read evaluate print loop. That's all it is, evaluate print loop. That's all it is, evaluate print loop. That's all it is, right? So what you do is instead of right? So what you do is instead of right? So what you do is instead of stuffing the entire context into the stuffing the entire context into the stuffing the entire context into the language model, you provide the model language model, you provide the model language model, you provide the model with a data asset and that data asset is with a data asset and that data asset is with a data asset and that data asset is a variable in a Python script. So a

[9:55]

a variable in a Python script. So a variable in a Python script. So a legal contract can be assigned to a legal contract can be assigned to a legal contract can be assigned to a variable in a Python script and instead variable in a Python script and instead variable in a Python script and instead of having that inside the context of the of having that inside the context of the of having that inside the context of the language model, the language model language model, the language model language model, the language model programmatically operates on that using programmatically operates on that using programmatically operates on that using the functions read, evaluate, print and the functions read, evaluate, print and the functions read, evaluate, print and loop. And there's an additional point loop. And there's an additional point loop. And there's an additional point here. There is a recursive factor. So here. There is a recursive factor. So here. There is a recursive factor. So that language model can call a smaller that language model can call a smaller that language model can call a smaller language model or a language model of language model or a language model of language model or a language model of the same type and recursively operate the same type and recursively operate the same type and recursively operate over that. So the way to think about over that. So the way to think about over that. So the way to think about recursion here is like a handoff. You

[10:26]

recursion here is like a handoff. You recursion here is like a handoff. You have one model operating over that data have one model operating over that data have one model operating over that data object and then handing that off to a object and then handing that off to a object and then handing that off to a smaller model to focus on different smaller model to focus on different smaller model to focus on different parts of that data object. So what you parts of that data object. So what you parts of that data object. So what you get there is effectively a much more get there is effectively a much more get there is effectively a much more sophisticated way to do multihop sophisticated way to do multihop sophisticated way to do multihop reasoning because that setup gives the reasoning because that setup gives the reasoning because that setup gives the agent the ability to search over the agent the ability to search over the agent the ability to search over the context flexibly depending on the task context flexibly depending on the task context flexibly depending on the task and find out exactly where the relevant and find out exactly where the relevant and find out exactly where the relevant information is to answer your query or information is to answer your query or information is to answer your query or to deliver a task. And that turns out to

[10:56]

to deliver a task. And that turns out to deliver a task. And that turns out to use a lot less context than stuffing the use a lot less context than stuffing the use a lot less context than stuffing the model with context. What we built there model with context. What we built there model with context. What we built there with Ripple is the ability to with Ripple is the ability to with Ripple is the ability to intelligently search and synthesize. So intelligently search and synthesize. So intelligently search and synthesize. So we're talking about intelligent we're talking about intelligent we're talking about intelligent decomposition of long legal contracts of decomposition of long legal contracts of decomposition of long legal contracts of code bases and intelligent symphysis of code bases and intelligent symphysis of code bases and intelligent symphysis of those code bases all with very simple those code bases all with very simple those code bases all with very simple primitives of a code interpreter and primitives of a code interpreter and primitives of a code interpreter and recursion. So let's look at the recursion. So let's look at the recursion. So let's look at the individual components. Read is obvious. individual components. Read is obvious. individual components. Read is obvious. Read is just reading the data object at Read is just reading the data object at Read is just reading the data object at the point in time of what it is.

[11:27]

the point in time of what it is. the point in time of what it is. Evaluate is where a lot of the magic Evaluate is where a lot of the magic Evaluate is where a lot of the magic happens. So evaluate can be any happens. So evaluate can be any happens. So evaluate can be any programmatic function over that data programmatic function over that data programmatic function over that data object. So it could be a slice. It could object. So it could be a slice. It could object. So it could be a slice. It could even be a keyword match on that data even be a keyword match on that data even be a keyword match on that data object. It could be any programmatic object. It could be any programmatic object. It could be any programmatic function on that data object. And then function on that data object. And then function on that data object. And then print is how we return the result back print is how we return the result back print is how we return the result back to the interpreter. So that is how the to the interpreter. So that is how the to the interpreter. So that is how the overall system keeps track of where overall system keeps track of where overall system keeps track of where things are. And then loop is continuing things are. And then loop is continuing things are. And then loop is continuing to do that until the task is solved. And to do that until the task is solved. And to do that until the task is solved. And it's this it's with this approach that it's this it's with this approach that it's this it's with this approach that we're able to build that dependency

[11:59]

we're able to build that dependency we're able to build that dependency graph. And that is how we actually are graph. And that is how we actually are graph. And that is how we actually are able to model the complexity and reason able to model the complexity and reason able to model the complexity and reason over those complex documents. Not by over those complex documents. Not by over those complex documents. Not by treating them as a story book and trying treating them as a story book and trying treating them as a story book and trying to stuff the model in and read them auto to stuff the model in and read them auto to stuff the model in and read them auto reggressively end to end, but by reggressively end to end, but by reggressively end to end, but by actually giving the model the ability to actually giving the model the ability to actually giving the model the ability to search over that codebase intelligently search over that codebase intelligently search over that codebase intelligently and build that dependency graph or and build that dependency graph or and build that dependency graph or search over that legal contract search over that legal contract search over that legal contract intelligently and build that dependency intelligently and build that dependency intelligently and build that dependency graph. And that dependency graph is what graph. And that dependency graph is what graph. And that dependency graph is what is needed to actually answer those is needed to actually answer those is needed to actually answer those complex legal queries or those complex legal queries or those complex legal queries or those complex code queries in your codebase. So what

[12:30]

code queries in your codebase. So what code queries in your codebase. So what are some of the implications of this? are some of the implications of this? are some of the implications of this? Well, they ran this over a few Well, they ran this over a few Well, they ran this over a few experiments and they found that for most experiments and they found that for most experiments and they found that for most of the runs the RLMs for the most part of the runs the RLMs for the most part of the runs the RLMs for the most part they were cheaper and higher they were cheaper and higher they were cheaper and higher performance. And they did this on GBT5 performance. And they did this on GBT5 performance. And they did this on GBT5 and also on Quen 340 billion parameter and also on Quen 340 billion parameter and also on Quen 340 billion parameter coding model. The model did perform coding model. The model did perform coding model. The model did perform slightly worse than the GPT5 model, slightly worse than the GPT5 model, slightly worse than the GPT5 model, which tells us that to use this scaffold which tells us that to use this scaffold which tells us that to use this scaffold appropriately, you still probably need appropriately, you still probably need appropriately, you still probably need high performance models. But the high performance models. But the high performance models. But the implications downstream are huge because implications downstream are huge because implications downstream are huge because what they were able to do was reason what they were able to do was reason what they were able to do was reason about context that were orders of

[13:01]

about context that were orders of about context that were orders of magnitude larger than the advertised magnitude larger than the advertised magnitude larger than the advertised context windows of the models without context windows of the models without context windows of the models without the same level of deterioration and for the same level of deterioration and for the same level of deterioration and for a cost that was in the neighborhood of a cost that was in the neighborhood of a cost that was in the neighborhood of the other approaches that we talked the other approaches that we talked the other approaches that we talked about. So the rag type of approaches, about. So the rag type of approaches, about. So the rag type of approaches, stuff in the context window approaches. stuff in the context window approaches. stuff in the context window approaches. So what are the limitations? Because So what are the limitations? Because So what are the limitations? Because there are limitations with this there are limitations with this there are limitations with this approach. So the paper I thought was approach. So the paper I thought was approach. So the paper I thought was brilliant. But with all papers you want brilliant. But with all papers you want brilliant. But with all papers you want to apply to your own situation in to apply to your own situation in to apply to your own situation in production and depending on what you production and depending on what you production and depending on what you have available to you in production and have available to you in production and have available to you in production and depending on the sensitivity of the

[13:32]

depending on the sensitivity of the depending on the sensitivity of the data, this might not be unlocked for you data, this might not be unlocked for you data, this might not be unlocked for you yet. If you can only use very small yet. If you can only use very small yet. If you can only use very small models, there is nothing in this paper models, there is nothing in this paper models, there is nothing in this paper that shows that it works with very small that shows that it works with very small that shows that it works with very small models. In fact, we see a deterioration models. In fact, we see a deterioration models. In fact, we see a deterioration in capability even between GBT5 and the in capability even between GBT5 and the in capability even between GBT5 and the 340 billion parameter model, the Quen 340 billion parameter model, the Quen 340 billion parameter model, the Quen coding model. There is also this fear of coding model. There is also this fear of coding model. There is also this fear of infinite recursion. So, it's not infinite recursion. So, it's not infinite recursion. So, it's not necessarily infinite recursion, but necessarily infinite recursion, but necessarily infinite recursion, but there's a fear that sometimes because there's a fear that sometimes because there's a fear that sometimes because you're letting the agentic system run you're letting the agentic system run you're letting the agentic system run itself that it can get into these itself that it can get into these itself that it can get into these recursion loops if it goes off on the recursion loops if it goes off on the recursion loops if it goes off on the wrong tangent and that can become very

[14:03]

wrong tangent and that can become very wrong tangent and that can become very expensive. So there was a distribution expensive. So there was a distribution expensive. So there was a distribution of the cost of running these tasks and of the cost of running these tasks and of the cost of running these tasks and for the 95th percentile of tasks, it for the 95th percentile of tasks, it for the 95th percentile of tasks, it became very expensive relative to the became very expensive relative to the became very expensive relative to the other approaches when it hit those other approaches when it hit those other approaches when it hit those recursion loops that would actually go recursion loops that would actually go recursion loops that would actually go off in directions that you don't want it off in directions that you don't want it off in directions that you don't want it to go off in. But the paper specified to go off in. But the paper specified to go off in. But the paper specified how they put guard rails in place. And how they put guard rails in place. And how they put guard rails in place. And this is one of the most important things this is one of the most important things this is one of the most important things if you're going to use this approach. if you're going to use this approach. if you're going to use this approach. You need the guardrails in place. So You need the guardrails in place. So You need the guardrails in place. So first of all, they specified that it's first of all, they specified that it's first of all, they specified that it's only one layer deep of recursion. And

[14:33]

only one layer deep of recursion. And only one layer deep of recursion. And then they made the workflow synchronous. then they made the workflow synchronous. then they made the workflow synchronous. But I see that as an opportunity to But I see that as an opportunity to But I see that as an opportunity to actually expand this because imagine if actually expand this because imagine if actually expand this because imagine if it was asynchronous. So that would mean it was asynchronous. So that would mean it was asynchronous. So that would mean being able to call multiple sub LLM at being able to call multiple sub LLM at being able to call multiple sub LLM at the same time to do different parts of the same time to do different parts of the same time to do different parts of the work. And you know, you can imagine the work. And you know, you can imagine the work. And you know, you can imagine that would be extremely useful if you're that would be extremely useful if you're that would be extremely useful if you're reasoning over a data room for example reasoning over a data room for example reasoning over a data room for example with 100 legal type documents in there with 100 legal type documents in there with 100 legal type documents in there and really complex maybe even some code and really complex maybe even some code and really complex maybe even some code base in there and really complex things. base in there and really complex things. base in there and really complex things. All right. So that asynchronity has not All right. So that asynchronity has not All right. So that asynchronity has not been tested in this paper yet. It leaves

[15:03]

been tested in this paper yet. It leaves been tested in this paper yet. It leaves it open for experimentation. I'm sure it open for experimentation. I'm sure it open for experimentation. I'm sure we're going to see lots of that type of we're going to see lots of that type of we're going to see lots of that type of stuff in production. So that's one of stuff in production. So that's one of stuff in production. So that's one of the limitations there that you are kind the limitations there that you are kind the limitations there that you are kind of letting go of control of the system a of letting go of control of the system a of letting go of control of the system a bit which gives it that flexibility but bit which gives it that flexibility but bit which gives it that flexibility but can also run away from you if you don't can also run away from you if you don't can also run away from you if you don't put the guardrails in place. One of the put the guardrails in place. One of the put the guardrails in place. One of the other limitations is this is not an other limitations is this is not an other limitations is this is not an approach to be used everywhere. The approach to be used everywhere. The approach to be used everywhere. The paper actually mentions that when you're paper actually mentions that when you're paper actually mentions that when you're dealing with really small context, just dealing with really small context, just dealing with really small context, just doing a one shot with the LLM often doing a one shot with the LLM often doing a one shot with the LLM often performs better. So you have to know performs better. So you have to know performs better. So you have to know when to apply these paper also mentions when to apply these paper also mentions when to apply these paper also mentions a slight subtlety. It says that if

[15:35]

a slight subtlety. It says that if a slight subtlety. It says that if you're just dealing with long context you're just dealing with long context you're just dealing with long context sometimes it's better to not use the sometimes it's better to not use the sometimes it's better to not use the recursion but still provide the ripple recursion but still provide the ripple recursion but still provide the ripple environment for the LLM to operate in. environment for the LLM to operate in. environment for the LLM to operate in. So effectively you're getting the large So effectively you're getting the large So effectively you're getting the large language model to reason over that long language model to reason over that long language model to reason over that long context without the handoff process to a context without the handoff process to a context without the handoff process to a smaller agent for that recursion factor. smaller agent for that recursion factor. smaller agent for that recursion factor. And that works better when you're And that works better when you're And that works better when you're dealing with something that's long dealing with something that's long dealing with something that's long context but low complexity where you context but low complexity where you context but low complexity where you have that high complexity and the long have that high complexity and the long have that high complexity and the long context. That's when the recursion context. That's when the recursion context. That's when the recursion factor comes in. So that's one of the factor comes in. So that's one of the factor comes in. So that's one of the limitations understanding where it limitations understanding where it limitations understanding where it should be applied and when it shouldn't

[16:06]

should be applied and when it shouldn't should be applied and when it shouldn't be applied. And the third limitation as be applied. And the third limitation as be applied. And the third limitation as well is complexity. These are much more well is complexity. These are much more well is complexity. These are much more complex systems to monitor. They're much complex systems to monitor. They're much complex systems to monitor. They're much more complex systems to implement at more complex systems to implement at more complex systems to implement at production. The other thing to consider production. The other thing to consider production. The other thing to consider as well is the nature of your prompt as well is the nature of your prompt as well is the nature of your prompt matters because the nature of your matters because the nature of your matters because the nature of your prompt will steer that evaluation prompt will steer that evaluation prompt will steer that evaluation process. So you should be wary of that process. So you should be wary of that process. So you should be wary of that as well. The observability does become as well. The observability does become as well. The observability does become more difficult. So I want to leave you more difficult. So I want to leave you more difficult. So I want to leave you with the implications here. The with the implications here. The with the implications here. The implications are that look we can unlock implications are that look we can unlock implications are that look we can unlock quite a lot of tasks outside of software quite a lot of tasks outside of software quite a lot of tasks outside of software engineering that have previously been engineering that have previously been engineering that have previously been unaccessible to AI agents. And some of

[16:39]

unaccessible to AI agents. And some of unaccessible to AI agents. And some of those I think I've mentioned before like those I think I've mentioned before like those I think I've mentioned before like legal analysis but there is also policy legal analysis but there is also policy legal analysis but there is also policy review. There is also information review. There is also information review. There is also information symphysis on internal documents. So symphysis on internal documents. So symphysis on internal documents. So there's a lot of companies that have there's a lot of companies that have there's a lot of companies that have thousands of internal documents and thousands of internal documents and thousands of internal documents and history of different types Excel history of different types Excel history of different types Excel spreadsheets, word documents in the spreadsheets, word documents in the spreadsheets, word documents in the organization that they just cannot make organization that they just cannot make organization that they just cannot make sense of. And this symphysis layer does sense of. And this symphysis layer does sense of. And this symphysis layer does provide a way to do that. Now of course provide a way to do that. Now of course provide a way to do that. Now of course this doesn't get rid of hallucinations. this doesn't get rid of hallucinations. this doesn't get rid of hallucinations. So you need to ensure proper data So you need to ensure proper data So you need to ensure proper data provenence, but that's standard if provenence, but that's standard if provenence, but that's standard if you're building AI agents at this point.

[17:10]

you're building AI agents at this point. you're building AI agents at this point. So if I'm going to leave you with a few So if I'm going to leave you with a few So if I'm going to leave you with a few points, the points are that when you points, the points are that when you points, the points are that when you model complex documents, the mental model complex documents, the mental model complex documents, the mental model you should hold is not reading model you should hold is not reading model you should hold is not reading them as long textbooks, but instead them as long textbooks, but instead them as long textbooks, but instead modeling them as dependency graphs. Code modeling them as dependency graphs. Code modeling them as dependency graphs. Code execution and recursion allows you to execution and recursion allows you to execution and recursion allows you to intelligently search over that context intelligently search over that context intelligently search over that context and build those dependency graphs that and build those dependency graphs that and build those dependency graphs that allow you to then synthesize correct allow you to then synthesize correct allow you to then synthesize correct answers or synthesize better responses answers or synthesize better responses answers or synthesize better responses while reasoning over that long context. while reasoning over that long context. while reasoning over that long context. It's not a one-sizefits-all. There are It's not a one-sizefits-all. There are It's not a one-sizefits-all. There are places it should be applied and there places it should be applied and there places it should be applied and there are places it shouldn't be applied. And are places it shouldn't be applied. And are places it shouldn't be applied. And you should apply it if you're doing

[17:41]

you should apply it if you're doing you should apply it if you're doing large context, complex retrieval, large context, complex retrieval, large context, complex retrieval, information synthesis, or research. information synthesis, or research. information synthesis, or research. Thank you.
