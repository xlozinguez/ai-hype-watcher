# Raw Transcript: Claude's Best Release Yet + 10 Tricks That Gave Me An Unfair Advantage

- **URL**: https://www.youtube.com/watch?v=TmsH-RIHvas
- **Creator**: AI LABS
- **Date**: 2026-02-11
- **Duration**: 12:20
- **Captured**: 2026-02-12
- **Method**: Playwright automated extraction
- **Segments**: 396 segments extracted

---

[0:00] Even though cloud code is one of the most powerful tools for AI development, why does it fall apart on certain tasks? And between the features Anthropic has been dropping recently and the workflows we've been building around it, the way you're supposed to use this thing looks completely different from a few weeks ago. Our team has been using Cloud Code every day. And it's not just for development, but also for research, managing our production pipeline, and automating tasks that have nothing to do with code.

[0:27] Anthropic recently added the insights command for claude code. It analyzes all your past claude code sessions over a certain time period and generates a report. The report analyzes your working style, roasts your working patterns, highlights what you were doing right and what you weren't, and tells you how to improve. The main thing we were interested in was identifying where things went wrong because that's where we can learn to improve ourselves. The report highlighted the areas where we had the most friction and also suggested features we could add to make the workflow better.

[0:55] For example, we remember a session where the main agent repeatedly pulled the task list for a long time when we were using agent teams. It caused the session to take too long and we had to end it ourselves. To prevent this from happening in the future, we can copy this prompt into claude.md so that whenever we're using claude code with multi-agents, Claude doesn't poll indefinitely and acts upon it.

[1:21] Our team has spent a lot of time working with claude code and the most important step is still how well you give context to the agent. This can be project requirements broken down into sub parts or documentation of the frameworks and libraries you're using because when you give it the right context, errors basically drop to zero because it knows what to act upon.

[1:37] For project documentation, we prefer using Claude to write it rather than doing it ourselves. We gave Claude a specific prompt that contained all the information needed to break down the project's idea into the required documents. We asked it to create four documents each focused on the specific aspect of the app. The most important one is the PRD which contains information about the project requirements and scope. Then there's architecture.md which has data formatting, file structure, APIs and all the architecture details written out. Then decision.md which contains all the decisions Claude made during the creation of this project acting as a reference for future use. And then the most important one is feature.json which contains all the features in a specific JSON format with all the details about each feature in a token efficient way and contains criteria for what makes a feature complete along with a passes key for keeping track of what's been implemented and what hasn't.

[2:32] Now that your large task is split into smaller sections, we need to provide documentation on what tools it needs for implementation through the context 7 MCP. It has documentation for all the libraries and frameworks and gets updated frequently so that agents can pull the latest docs and fill the gap between what the model knows and what actually the current update.

[2:57] Now, hooks are another underutilized feature. The hooks in Claude Code are shell commands that fire at specific points in the life cycle. There are many types that trigger at certain times, like session start, before any tool is used, or after a tool is used. But the most important part is setting them up with specific exit codes. Exit code zero means success. Exit code 2 means a blocking error. So whenever Claude tries to do something it shouldn't, it hits exit code two, it gets an error message back and can correct itself.

[3:34] This exit code 2 is important because using it, you can control the agents behavior. If you've ever worked with test-driven development using claude code, you might have noticed that it tends to modify the tests if it fails to meet them. To prevent that, we set up a custom hook that triggers on pre-tool use. The hook protects the test scripts from modification. If the path it's trying to work on is a test directory or contains the word test, it shows an error message and returns exit code too.

[4:15] So, if you've worked with MCPS, you know they bloat the context window and when you're working on a large scale project, the number of connected MCPS increases. For this exact purpose, Claude Code has an experimental MCP CLI mode that solves this. We set the experimental MCPCLI flag to true. Once we set it, all the MCPS that were showing up in the context disappeared. Instead of loading all the tool schemas up front, Cloud Code uses MCPCLI info and MCPCLI calls, and it runs all the connected MCPS through these tools via bash.

[5:16] We have stressed using Git to have all the agents work tracked in version control. We also covered a video where we used Git to run an agent on a long horizon task. We used parallel agents to work on different work trees so they could create all of the projects features while staying isolated from each other. This way we could merge their output together later without interference because agents working on the same files cause conflicts. Branches aren't preferred because they cause conflicts. Agents have difficulty checking out different branches since branches share the same working directory but work trees don't.

[6:14] Strict mode is essential for shifting the burden of error checking to the agent. This is something you should be setting up for whatever language you're using because it catches bugs when you build instead of when users hit them at runtime. Since our primary language is TypeScript, we always set strict mode to true in our projects.

[6:50] Instead of letting the project be tested only by scripts, there's an additional layer of testing worth adding. You write user stories that describe how the user interacts with the system in order to guide the testing process once the app is built. We actually define the user stories before implementing our projects because this sets a standard that the implementation should follow.

[8:16] We need to make use of parallelization as much as we can because this is how the agent speeds up its workflow. We set up a research task where we wanted to compare the agent swarm capabilities using two agents, one to do the research and another to fact check the research agent. The key idea was to have both agents communicate with each other to make sure the findings were accurate. In this setup, one agent does the task while the other critically analyzes it, giving them an adversarial way of working.

[10:01] According to the words of Claude codes creator, the agent works better if it has some way to verify its own work. The core idea here is giving the agent eyes, meaning the ability to check whether the implemented feature is correct and meets expectations. We use multiple ways to verify the agents work. The first is the Claude Chrome extension. Another tool is the Puppeteer MCP. But our preferred option is Vercel's agent browser. This isn't an MCP, but a CLI tool that gives agents browser testing capabilities. Unlike the other tools, it uses the accessibility tree where each element has a unique reference. This compacts the full DOM from thousands of tokens down to around 200 to 400 tokens.

[11:27] Testing is always important, but there's a way to reduce errors that doesn't involve tests or code reviews. We ask Claude to predict things that haven't happened yet â€” to check the implementation and identify areas where the app could fail. This works because we're giving Claude a chance to predict potential issues by pattern matching against failures that already existed in other apps. It identified critical gaps that passed even our multi-layer testing process and found 18 issues that could have been harmful in production.
