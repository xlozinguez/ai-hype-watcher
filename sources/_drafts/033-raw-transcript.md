# Raw Transcript: 033

[0:00] Do you think we should agegate synthetic relationships? >> I don't think that agegating is a particularly bad idea for synthetic AI characters that try and act like people because we don't know what the effects are. Um I think it's easy to be alarmist and catastrophic about it. The effects may end up being very good. I don't know, but neither does anyone else. Ethan Mollik is a professor at the Wharton School and a leading voice on how AI is changing work, creativity, and education. Ethan also writes the popular

[0:30] Substack, One Useful Thing. He joins it. Where does this podcast find you, Ethan? >> I'm outside of beautiful Philadelphia, Pennsylvania. >> There you go. Are you at school or is that your home or >> I'm at my home? Yeah, that's my game collection back there. So >> Oh, I like it. Uh, so let's bust right into it. Anthropic CEO Dario Amote recently released a 38 page essay in which he delivers a very ominous warning about AI and the threat it poses to our society. Why? Why do you think the CEO of one of the largest AI companies in the world seems to be so pessimistic

[1:00] about AI and what do you make of this view? Is it is it more of this kind of virtue signaling and not meaning it or do you think he's generally trying to build a better AI? >> So I think that there's always debates right there's like external facing but like when you talk to these people internally I think uh Anthropic is fairly sincere about their views about how AI works. You may or may not agree with them. He actually has a pair of essays, one on like the bright future ahead of all of us and the other about uh our uh potential doom. Um and pointing out issues that may actually

[1:30] occur. So, you know, it always is a question of weirdness that you're building this thing if uh if you're so worried about it. But I think it is a sincere anxiety. >> A quote from the essay, humanity is about to be handed almost unimaginable power and it's deeply unclear whether our social, political, and technological systems possess the maturity to wield it. Uh do you agree with that? And also what does Ethan Mollik think are the biggest dangers of AI or what are you worried about? >> So you know I'm in kind of in a weird boat here which is I think that there's

[2:00] a lot of worries about the existential risks of AI and so people leap ahead five years assume the turn current path continues and there's no sign yet by the way that AI is slowing down development but there's a move towards sort of existential you know Dario in that essay talks about what would a group of geniuses in a data center they're smarter than any human what would they do? I'm actually much more concerned and thinking about how we guide the next few years to make AI help people thrive and succeed rather than the negative, you know, consequences that could happen. How do we mitigate those negative risks?

[2:30] So, I think there's a nitty-gritty path between here and some imagined future. We don't know if AI is going to get there to sort of super powerful and autonomous, but we do know it's disruptive today. So I worry a lot about how do we model the right kinds of work so that when we start using AI at work uh that we do it in ways that empower people rather than fire people. How do we think about AI in education so that it helps students learn rather than undermines learning? How do we think about using society in ways that don't lead to deep fakes and dependencies? I think there's two sides to each of these

[3:00] coins and we need to get very nitty-gritty about which things we care about. >> Well, I'll put forward a thesis and you tell me where I've got it right or wrong. I'm actually an AI optimist and I think it's easy you just sound smarter when you catastrophize and I do a lot of that but the existential risk of it turning into you know a sentient being and deciding that in a millisecond that we should no longer exist or self-healing weapons. I don't see any reason why AI couldn't use as much for defensive measures as offensive

[3:30] inequality that's already here. We've opted for that. Um, and I'll come back to something I'm more worried about, but what I see, I'm an investor in a company called Section AI that helps corporations upskill the enterprise for AI. And what we have seen or what they have seen is that the adoption is woefully under penetrated within the actual organizations. At least in the enterprise, individuals are using AI for therapy or how to reduce their workload

[4:00] on a Friday. I mean, is all of this quite frankly and is and also I wonder if the CEOs have a vested interest in catastrophizing because it makes it sound like the technology is world changing and that much more powerful and please sign up for my $350 billion round at Anthropic. Is some of this quite frankly just is some of the dread and doom just quite is just inflated? I >> I mean sure. I mean, but some of it is also they drink their own Kool-Aid like they believe this stuff whether that serves marketing or not. But I do

[4:30] want to take a step back. I'm a business school professor, right? You know, like you and so I've been doing a lot of work with my colleagues on impacts of AI at work. And there's a few things. One is there are fairly large impacts in any randomized control trial. We see we did early experiment with my colleagues at Harvard, MIT, University of Warwick, at Boston Consulting Group. We found 40% improvements in quality using the now obsolete GPT4 with people weren't even trained. 26% faster work. Penetration rates are up there. It's interesting. Companies, people are using AI, but they're not talking about it. They're not using the corporate AI. So about 50%

[5:00] of American workers use AI. They report, by the way, three times productivity gains on the tasks they use AI for. They're just not giving that to companies, right? Because why would you? Like you're worried you'll get fired if AI shows you that you're more efficient. You look like a genius right now and maybe you know the AI is the genius. You're doing less work. So I think that there's a difference between what companies are seeing about adoption, what's actually happening with adoption. Um the CEO of section says that right now it's being used at work for therapy and so someone who understands AI is giving themsel another day off that

[5:30] why sharpen the sword publicly that you cuts your own head off with what specific tasks at work have you seen in your studies in your research have registered the greatest increases in productivity? What's been overestimated and what's underestimated in terms of the disruption or the improvements in productivity at the workplace? So I think the big picture overestimation and underestimation is work is complicated and organizations are complicated right so you can get lots of individual

[6:00] productivity game but if that's producing 10 times more powerp points than you did before that's not necessarily going to translate to any actual benefit for the company so leadership needs to start thinking about how do you build organizations around this at the individual level though huge impacts and you know coding especially has taken this massive leap we have earlier evidence that you saw about a 38% improvement in uh in the amount of code people were writing when They started using agentic coding tools with no increase in error rates. But that's even increased further. The newest coding tools um both the um people in charge at on the research level at OpenAI and Anthropic have said 100% of

[6:30] their code is now written by AI. That's actually quite believable um given how good these tools have become. We're seeing similar things um you know managerial task medicine. We're seeing impacts in um scientific publication super interesting area. uh people who started using AI early to write scientific papers and we know this because there's a great study that looks at when they started using the word delve which was a dead giveaway you were using AI back in 2023 if you use delve a lot in 2023 then you actually publish about a third more papers in higher

[7:00] quality journals afterwards now the question is that good for science to have more AI writing separate issue so there's that's the sort of process versus detail problem right people are becoming individually more productive the system isn't built to handle a mix of high quality and lowquality and is more work and that's where the bottleneck often is. >> You coined this great term um to describe AI called the jagged frontier. I love that. Which encapsulate encapsulates how AI is really good at certain things but really bad at others. I'm I'm the CEO of a Fortune 500

[7:30] company. I've just spent a bunch of money on an anthropic site license and I've got actually, you know, the music has to match the words in terms of my embracing AI on earnings calls. If you were advising me and I said look where should I be overinvesting and underinvesting and where can you what areas of the organization should I focus on to try and deploy AI for meaningful productivity gains and which areas should I avoid that aren't yielding the type of benefit that was once advertised. So I think that equation

[8:00] starts with a realization which is nobody knows what's going on, right? Like I talk to all the AI labs on a regular basis. I don't take money from them, but I talk to them all. Um I you do research on this. I talk to policy makers and CEOs and it's not like there's a playbook out there, right? This is a we're a thousand days into after the release of Chat GPT. Like everyone's figuring this out at the same time. I'm seeing companies getting incredible amounts of benefit um and other companies struggle and part of that is how much they're willing to embrace the fact that they have to do R&D themselves. So part of the value of giving people access to these tools is

[8:30] experts figure out use cases, right? If you're doing something in a field you know well, it's very cheap to experiment with AI and figure out what's good or bad at because you're doing the job anyway and you easily look at the results and see whether they're good or bad results. If you're paying someone to do R&D for you, that's a very expensive process. So people are inventing uses all the time. So the most successful case I'm seeing are a combination of what I call leadership lab and crowd. The leaders of the company have a clear direction, set right incentives to make things happen. think about process. They give the crowd, everybody in the

[9:00] organization access to these tools to use advanced tools like you know anthropics tools or open AAI or Gemini. Uh and then they have an internal team that is actually thinking about what you build. So they're harvesting ideas from other people. So I'm seeing this happening everywhere from you know there's certainly a lot of stuff happening with internal processes, security, customer service, um like lots of stuff on analytics like the AIS are quite smart. So if you let them do analysis work, you can actually get really big impacts from that as well. Just wide ranges, but very different across organizations depending on where

[9:30] their expertise is and how aggressive they are about trying to experiment. >> I know Mark Beni off and it's just so it's borderline obnoxious how many times he'll figure out a way to insert the term agentic AI or the agentic layer. And to be blunt, I'm not sure I entirely understand the difference between AI and agentic AI. Can you break it down for us and why so many really smart people such as Mark Ben off seem to be talking about egentic AI?

[10:00] >> It's a great question. Um, first of all, you know, you started this off by talking about marketing. Part of the problem with AI is there's a anytime a new phrase comes out, there's a blur of confusing different interpretations of it because everyone wants to sell AI product right now. So, it's really easy to get to get uh bogged down. So agents basically can be defined as an AI tool that is given an AI that's given access to tools. So it can do things like write code, search the web, and do things that when given a goal can autonomously try and accomplish that goal on its own and

[10:30] correct its course if it needs to. So an agent would typically be something where you could say, "Hey, you know, I'm going to have uh Ethan on this podcast, research everything about him, come up with a, you know, a pitch deck on what, you know, why we might want to have him on the cast. talk about interesting things that he might have said before and then boil this down to five really good questions to ask and it would go out and do the research and 20 minutes later you get kind of a complete result. That's an agent at work. So agents are basically the chat bots that you use today when you go to chatbt plus we call an agentic harness a set of

[11:00] tools and capabilities they have searching the web writing code connecting to your data that lets them do more work. So when you combine those two together that's where you get semi-aututonomous AI. Give me I'm a CEO, a student, a mid-level professional. What is and I've done very little so far around AI and I want to catch up. What is the Ethan Mullik AI tech stack? What should I be downloading, subscribing to? Uh how do I get started here? What LLMs,

[11:30] agents, whatever the term is, would you recommend investing in right now? Okay. So, the the good thing about AI is it's very democratic, right? There's no better model than the ones you have access to today. You or every kid in Mozan beek has access to the exact same tools that are at Goldman Sachs or Department of Defense or anywhere else. There's no better models. They're basically being released as soon as they come out. That being said, the really good models tend to be cost you at least 20 bucks a month. So, you are probably going to

[12:00] want to subscribe to either Google's Gemini product, Anthropics Cloud product, or OpenAI's uh chat GBT product for 20 bucks a month. And you're going to want to when you do a serious work, pick their advanced thinking model. So, GPT 5.2 thinking is important to use. Anthropic 4.5 Opus and Gemini 3 Pro. Those are the sort of starting pack of tools you can use. They're all capable of doing agentic work. Um, you can access them through the chatbot. And I always recommend people just start by trying to do stuff they do for their job. Ask it for everything you do that

[12:30] day. Just ask the also generate some ideas for me. Give me feedback on this. Help me write this email. Create the presentation that will help you map the jagged frontier of what AI is good or bad at. And it's a really good starting point. Like there's a lot of other complicated stuff. If you want to do, you know, research, uh, the deep research tools for Google are currently better through this product called Notebook LM. And that's free and that's very good. If you want to do coding, you probably want to use Claude Code, which you have to download. But the basics are pick one of the big three, pay the 20 bucks a month, and then start using

[13:00] them. You need eight or 10 hours of just talking to it like a person and seeing what results you get. >> And give us a the lay of the land. My sense is that OpenAI was dominant. still dominant but that the empire strikes back specifically Gemini is making inroads capturing share and anthropic has made real progress in the enterprise market. So that's the limit of my knowledge about the playing field. Can you add color to that around the dynamics uh the intraplay here if this

[13:30] were a league? What what teams are coming up and what is what is descending? >> Yeah. So to take half a step back right on what drives the underlying dynamic is something called the scaling laws. Um and the scaling laws basically tell you the larger your AI model is which means the more data you need to build it the more data centers the more electricity the more you know the more chips the better your AI model is. And it's very hard to build a small model to compete against the larger model. They're just better at everything. You could build you know once you have one of those you

[14:00] could do all kinds of variations but you have to build a big model. And um there's a bunch of other tricks that you could do on top of that, but that's that's pretty critical. And because of that, there's only a few companies that could actually play in this space, right? So in the US, we mentioned the big three, which is Google, Anthropic, and OpenAI. There's also Elon Musk's X, which has been scaling quite quickly. XAI, and um there's also Meta, which has been quiet recently, but is spending a lot of money in this space. Outside of that, there's a lot of people with smaller competitive, but they're not really competitive. Amazon, Apple, they don't really have their own models that

[14:30] compete. There's also three or four big Chinese companies that are producing very good models or releasing them for free to the world and one French company in the same boat. So within that dynamic there's there's this competition about who could build the biggest data set or who could train the biggest model because bigger models are smarter, who could put the most research and tricks into them. And it the it really is interpersonal in some ways like the heads of these companies are really out to get each other, right? Like they do care about winning this race. They think they should be dominant. And so there is a lot of resources being put

[15:00] into getting ahead of the other people in this space uh one way or another. So right now the sort of three most polished models are Google's OpenAI's and anthropics and again which one is better is changing on a day-by-day basis as or at least weekby-eek basis as each one releases new approaches. Um and then you know we're waiting to see if anyone else kind of catches up to them. But those three are in a very tight race. As soon as one of them comes up with a product that uses AI in a new way, the other two copy it, right? So, Claude code is currently the very hot

[15:30] coding tool. OpenAI has codeex which is a very similar thing. Gemini has its own set of tools. Deep research was invented or first came out from Google. Now, there's deep research projects for anthropic and from OpenAI. So, you could kind of pick any of the three of them and be in good shape as long as they could keep growing and spending money and they don't hit a wall in development, which hasn't happened yet. Is it essentially that, you know, different flavors of the same ice cream here? Or if I think of luxury brands, BMW, Mercedes, and Audi, I think I could

[16:00] do a reasonable job of attempting to outline how they're they differentiate from one another and who is the right customer for each of those brands. Can you do the same thing for those big three or are they all just the kind of mostly the same? >> I can. Right. What I worry about is trying to talk to all the various levels, right? What do you do if you're just starting off? Pick any of the three, you'll be fine. But I think people who use them a lot, they have personalities, right? Those personalities are shaped by the companies, the way they train. I mean, it's amazing that they're all so similar to each other that things basically work across all three. Like, you won't expect

[16:30] Microsoft and um you know, and uh Apple to produce a system that works exactly the same. These are similar enough that for most people doesn't matter, but if you care, right, um Opus 4.5 anthropics models are tend to be known as the best writers of the bunch. They're often quite good at sort of intellectual topics. They're a little fussy in terms of um you know they have high ethical standards relative to the other models. Um chatbt is um has is really two different flavors of models. There's a set of chat models that are

[17:00] really optimized for you to have conversations with and roleplay and be friendly. Um I don't tend to use those much because I tend to focus more on the work aspect. And they have a series of very logical, very good at long task models that are very good at producing a lot of work. And Gemini is an interesting set. Very smart overall model. Um, weirdly neurotic like it actually gets kind of if you get self flagagillating if you tell it did a bad job. It it it apologizes and kind of gravels. Um, weird kind of dynamic there. So, they all have their

[17:30] own sets of personalities and approaches. >> I find that um anthropic is more politically correct. chat GPT will give it to me straighter and then when I go to XAI it's it seems like it's purposely trying to offend people. It's going the other way. Um it's interesting you say that they both take on personalities. With respect to differentiation, the data I've seen is that most of these models are converging towards parody that

[18:00] it is very hard to maintain any sort of substantial or sustainable differentiation because AI just reverse engineers other AI. Do you see the same regression to the mean that I'm seeing? >> Well, I wouldn't call regression of the mean. We are seeing a race, right? There is there is huge impact. Each model generation is much more capable than the one before, right? So we keep crossing these lines where oh the AI can't do you know um it can't work with Excel and suddenly it works with Excel better than you know and does a dis you know

[18:30] discounted cash flow analysis better than most bankers right or you know the AI can't can't produce a PowerPoint and suddenly can do that or it can't do math and suddenly you know last year it won the both two models won gold the international math Olympiad so like there is not it's not a regression in the mean because there's no drop down of ability level the ability levels keep going up but all of the companies in the space are on roughly the same development curve, right? Their models are keep leaprogging each other by a fairly predictable amount over time and you can draw a pretty good curve on any

[19:00] benchmark that you want that shows the same exponential gain in AI abilities. So which raises the big question of like so what happens in the long term and I think that depends on one of what the long term of AI looks like. There's one version where we just keep having a race of capabilities and you need to stay ahead and you pick a model maker and as long as they stick with you keep paying them money. There's a version where one of them achieves what's called takeoff. Their AI models become self-improving and they build the smartest possible model. No one can catch them and build artificial general intelligence a machine smarter than a

[19:30] human every intellectual task. That there's some sort of apotheiois or endgame. Or there's a version where everything sort of plateaus out and then people spend billions of dollars building models and eventually free Chinese models or another company catches up and there's no money and becomes commoditized. I don't know which of those three scenarios dominates. We'll be right back after a quick break. Support for today's show comes from Hungry Root. Habits are hard to change, and often times it's not about a lack of motivation, but more about not having

[20:00] the right options at your disposal. Like, if you're looking to change up your diet, you can't expect to make the move if all you have are snacks and junk food. That's why there's Hungry Root. For those of you looking to up your nutrition and eat healthier, Hungry Root basically works like a personal nutrition coach and shopper in one by planning, recommending, and shopping everything for you. They take care of the weekly grocery shopping, recommending healthy groceries tailored to your taste, nutrition preferences, and health goals. We've gotten to try Hungry Root on the PopG team, and people

[20:30] reported back that it was surprisingly easy and good tasting. And people were able to spend less time worrying about grocery shopping. Take advantage of this exclusive offer. For a limited time, get 40% off your first box. Plus, get a free item in every box for life. Go to hungerroot.com/propg and use code propg. That's hungry.com/propg. Code propg to get 40% off your first box and a free item of your choice for life.

[21:00] Support for Prop comes from Lisa. Your mornings can't get off to a good start if you don't get a good night's sleep and tossing and turning because of your mattress not being up to par. It's time to re-evaluate what you're sleeping on. Lisa mattresses are designed to help you get the muchneeded RM sleep at night so you can tackle all the challenges thrown at you during the day. Lisa has a lineup of beautifully crafted mattresses tailored to how you sleep. Each mattress is designed with specific sleep positions and feel preferences in mind from night one. Lisa believes that you'll feel the difference with their premium materials that deliver serious comfort and full body support. no matter

[21:30] how you sleep. Plus, Lisa mattresses are meticulously designed and assembled in the USA for exceptional quality. Go to lisa.com for 30% off. Plus, get an extra $50 off with promo code PropG exclusive for our listeners. That's lessa.com, promo code propg for 30% off, plus an extra $50 off. Support our show and let them know we sent you after checkout. lisa.com, promo code propg.

[22:00] I wonder if or one of the thesis we had for 26 was what I see or potential for is similar to how the Chinese engaged in dumping of steel uh predatory pricing hoping to basically consolidate put American steel producers out of business consolidate the market and then have pricing power. I wonder if the Chinese are now engaging in what I would refer to loosely as AI dumping. And that is

[22:30] some of these models appear to be really strong. Sort of the Old Navy of AI. 80% of the best models for 10 20 40 50% of the price. And a lot of VCs and big firms have said, "We're using these models. They're just a better value." Do you see any sort of geopolitical chess here around the Chinese engaging in some form of what I would refer to as AI dumping? I mean there's something interesting going on because an open weights model which is a model that you release publicly to the world that anyone can run right so if I want to use

[23:00] chat GBT I have to go to open AI and use chat GBT to do that if I want to use one of the Chinese models like Quen um any company in the US can download that model and run it themselves so that model based on open source made sense for software because I could give away my core software for free but then sell you services it doesn't actually make a lot of sense for AI eye companies because they're building a model and giving away for free. There's no ancillary benefit to that. They don't get a gain in the long term. They're not selling other solutions. They have no

[23:30] special prize or tool left behind in most cases. So, there is a little bit of weirdness about how long will Chinese companies sustain releasing free models. They're about eight months behind, you know, and consistently eight months behind the frontier of US models. And, you know, what's driving that, right? Is this a state sponsored effort in the long term? right now it's not clear that it is but it might be that there is some sort of you know dumping kind of effort on the other hand I mean the matter the degree of intelligence is funible like if you are talking to a CEO and they're

[24:00] saying we're going to use a Chinese model because it's cheaper the cost of models has dropped 99.9% for the same intelligence level in three years you'd be like you actually for most applications want the smartest model that's most capable of doing tasks as cheaply as possible so fixating on a model that's not as good may end up being a problem like this isn't an equation where we're done yet and we could pick among roughly equivalent products because we're racing up a curve of ability that's still changing over time. >> When you look at the AI supply chain, and my guess is you can articulate the

[24:30] actual supply chain much more cogently than me, but I think about the infrastructure layer, the chips. Then I think about the LLMs and the apps on top of it and then services for adoption here. But I also think about power and data centers. I'm not even sure where that comes into the stack, but if there's a choke point here, and it might be just capital to fund all of this, what do you think are the biggest choke points into uh that stands in between these CEOs talking about the brave new world of AI?

[25:00] And um you know, I heard that Nvidia, it takes 5 years to hook up a data center in some parts of the nation to the power grid. What do you see as the choke points that get in the way of this brave new world, so to speak? >> Yeah. And there's a few of them, right? And they're kind of jockeying against each other. So as you pointed out, data centers are the sort of choke point, right? How fast can I build one and especially how fast can I power one and can I get enough chips to put in one, right? So that the power and um and uh building and chips are all a big deal.

[25:30] For a while, data was the bottleneck, but AI companies have increasingly found that they can make their own data. So it turns out as long as you have some human data, large language models can create their own data and other models can train on that and that you get good results. So data is not the choke point. it was, but it could be again. There's also a research choke point. Um, there's a lot of things that LM do really well, but there's some parts of the jagged frontier that are still very jagged, right? LM don't have memory. They don't learn things over time. So, I have to instruct them every time. It's like I'm talking to an amnesiac every time I

[26:00] speak with an LLM. So, continual learning is a is a problem that gets in the way of building these amazing models for the future. They don't keep learning, you know, what like that humans keep learning otherwise you have to train them every time. So there's research bottlenecks, there are energy, power and data center building bottlenecks. Um and um and those are the sort of big ones right now. From a policy perspective, energy is the big one that all the AI labs are worried about. They could turn reliably turn energy and chips into money right now. Um and the question is how fast can they

[26:30] build those data centers? >> I look at these things and you're you're at the business school. I'm at the business school. I look at the valuations of these companies and I see one of two things needs to happen. the valuations need to be cut in half or we're going to see such an incredible destruction in human capital or the labor force to justify the expense here through efficiencies because I don't see a lot of new AI cars or AI moisturizers. What I see is opportunities for efficiencies which is Latin for cost cutting. But my

[27:00] thesis is you're either going to see a really significant uh begin to see a significant and we haven't seen it so far to be fair destruction in the labor force and in more information intensive industries or we're going to see valuations come down dramatically. I'm having a difficult time understanding how any of these valuations can be justified uh over the medium-term much less the long term unless these companies begin to register massive efficiencies again layoffs. What do you think of that thesis?

[27:30] >> So I this is where I think that first of all I think you've laid off the trade-off really well, right? Which is I think that people tend to view valuations as either bubble or not. But the truth is valuations are justified if the revenues can be made to justify them, right? And the revenue targets are potentially achievable in a world that AI actually gets as good as the AI labs say it's going to get. And we could argue whether that's going to happen or not or that there'll be a financial bubble. I can't tell you the answer to that. But I think the real trade-off is what you just articulated, which is what

[28:00] what it means for an AI company to achieve that revenue, right? Let's assume that they succeed at doing that. And that's where I think the starkkest problem is because I do worry a lot when I talk to CEOs of companies. They're used to seeing technology as efficiency gains, right? Which, as you said, it means layoffs, right? I want to see this as like, okay, if one person could do 40% more work, I need 40% less people. My desperate desire is to try and communicate to companies something I think the AI labs try and say which is this is also about an expansion of capabilities right if you could do more work and different kinds of work

[28:30] the boundaries of what a firm could do could change the capabilities of what people you expect for people can do this could be a growth opportunity I mean you know whether or not you believe them like Walmart for example has publicly been stating that they want to keep all their current employees and figure out new ways to expand what they do right as opposed to Amazon which has been kind of saying we have to cut because of AI there are other models out there and I do worry about the lack of imagination in corporate America where the model is ah great we could just keep cutting down our number of people because AI does the

[29:00] work as opposed to how does everyone working as a manager what happens if we get 10 times more code that doesn't mean we should have 90% less coders maybe that means we can do different things than we could do before what happens if everyone's an analyst what happens if we can give better experience to every customer and the failure of imagination there makes me very nervous >> my first job out of UCLA was a Morgan Stanley I was an analyst in the fixed income department. And I look back on that, I even found some old PowerPoint decks I used to pull together to pitch companies on debt offerings. And I don't think the two years I spent there, I

[29:30] don't think it could be distilled to two weeks, but it could probably be distilled to 3 months if I just learned the basics of AI. Having said that, I haven't seen a huge destruction in jobs across those information. And my understanding, unless he's lying to me, I spoke to David Solomon. They same levels of hiring. Big law firms appear. at least they're saying same levels of hiring. Do you where do you see the greatest threat in terms of especially amongst young people uh coming out of college? I've

[30:00] seen all of this doom and gloom about young people but the reality is youth unemployment is at 10% which is by no mean alarm means alarming. Do you think there's a wave of labor destruction at kind of the entrylevel information intensive industry? I think that people overestimate the speed at which large companies change, right? And so I think you're right. Like I'd be shocked when has there ever been a technology invented 3 years ago that affects the labor market that quickly? It just doesn't happen, right? I I think that

[30:30] there is change in the system. I think it's baked in, but I don't think it's there yet. As you said, companies are just adopting this now. They're just telling their employees, "Everyone use AI for something with no centralized idea about what that's doing or how it's valuable." No one has been rebuilding their process in a serious way around AI. They're all in their first AI projects. There's no consultant you could hire who does this. So there is I think that you're right in that as far as we can tell and there's some debate. Eric Bolson argues that we're seeing canaries in the coal mine. Other people disagree, but there's no giant signal

[31:00] that AI is responsible for labor changes right now. Companies are blaming AI everywhere. But I realistically if you look inside organizations, there's no wave yet. That doesn't mean there isn't going to be. Like it's very hard to see. For example, let's just take something that's very well understood which is coding, computer programming. Like it is very clear that AI is going to change how programming works. You could talk to any coder, any elite coder, and they know it's going to happen. It privileges people who know what they're doing. The experts become more expert. You get a huge multiplier. Man, it becomes a

[31:30] management job, not a coding job. And that's going to change the hiring market. It just hasn't done it yet. And I think it's going to take a while for companies to figure out what that looks like and what that means. We'll be right back. Support for today's show comes from Northwest Registered Agent. Starting a business can be overwhelming, but if you want to take that first step, look no further than Northwest Registered Agent. You can get access to thousands of free resources, forms, and step-by-step guides without even creating an account. And when you sign up for a free account,

[32:00] you can get access to corporate guides. These are real experts who answer your questions, no strings attached. And they don't just help you form your business, they give you the free tools you need after you form it, like operating agreements, meeting minutes, and thousands of how-to guides that explain the complicated ins and outs of running a business. Northwest Registered Agent has been helping small business owners and entrepreneurs launch and grow businesses for nearly 30 years. And when you're launching your next big thing, you need experts in your corner to help you navigate the unique hurdles your business might face. Don't wait. Protect

[32:30] your privacy, build your brand, and get your complete business identity in just 10 clicks and 10 minutes. Visit northwestregisteredagent.com/profg free and start building something amazing. Get more with Northwestregistered agent at northwestregistered.com/prof

[33:00] So, let's shift to academia. all of these articles over the last two years and um and I'll put forward this is a comment posing as a question. I hear people say, "Oh, you don't need we're not going to need college with AI." And I find that people saying that are because their kid didn't get into and scored a 22 on the ACT and is trying to make themselves feel better. Uh I see absolutely no evidence that AI is disrupting higher ed. Applications are up. Your school, my school are both still figuring out ways to raise tuition faster than inflation. the whole AI will

[33:30] make higher education obsolete. I just don't see it happening. I don't see it happening. Um your thoughts. So I mean a few things. I think my personal feeling is education gets a boost from this for reasons I can discuss in a second, but I mean I think it's disrupting higher education that everybody is cheating with AI and essays are no longer a valuable way of assigning. There's a lot of disruption at the school level at the teaching level. We'll get through that. We always do. But um but I agree there isn't a sign that this is devastating higher

[34:00] education and I don't think that saying everyone's going to learn with AI and that's going to be the only way you learn or you won't need skills anymore are viable outcomes of in this world. I think that education will change. Um I think there's a easy imagine a world given early evidence that AI when used properly can be a good tutor. I can imagine a flipped classroom setting where my students are engaging with AI outside of class and inside of class we're doing more experiential active learning based case discussions other things >> but that's in the margins. I

[34:30] actually think that the value of education, especially professional education goes up because I teach people to be generalist at Wharton, right? I teach them to be really good at like, you know, business and maybe they have a little bit of consulting or strategy focus or entrepreneurship focus and then I send them off into the world and they go like you did to work at Morgan Stanley or whatever and they learn how to do their job the same way we've taught people for 4,000 years, which is apprenticeship, right? They work they if you're a middle manager you get this advantage of a junior person who's desperate to prove themselves who is willing to work really hard but isn't

[35:00] very good but will be and they write deal memos over and over again and they get yelled at or given nice feedback and eventually they learn how to write a deal memo and that's how we teach people. You don't have to be good at managing for them to learn. Ideally, you're good at teaching, but you don't have to be. But that's all broken down. Like already this summer, it broke down, right? If you're an intern at a company this last summer, you absolutely were using Claude or, you know, chatbt and just turning those answers into people because it's better than you at your job. And middle managers were increasingly turning to using AI instead of interns because it does the work

[35:30] and doesn't cry, right? And so, as a result, you saw this loop where nobody was learning the sort of entry skills before. So I actually think in a world where the skill destruction happens at the intern level, we're going to need to think more about how we educate people formally. In a world where informal education becomes harder to do, >> how has it changed your role as an academic in terms of research, how you prep for class or you know quite frankly how you make money outside of the school or in the traditional confines of academia. How has AI impacted the way

[36:00] you approach uh your job? uh in tons of ways and I think by the way that's indicative right because you know the way we tend to model jobs right now in academia is that they're bundles of tasks right so as a professor I do a ton of things right I am supposed to teach classes and design classes and grade you know assignments and be emotionally available to my students and also be a good administrator and review papers and write papers and be on podcast write books all of that stuff right tons of stuff and it's an impossible set of

[36:30] tasks I mean most people's jobs have a ton of tasks that they're not getting to or doing badly. So, if the AI the AI has already taken some of these things from me, right? Some of them I won't do for social reasons. The AI is a better grader than me, but as of yet, I haven't let it do grading because my students expect me to grade the papers, but maybe that will change. Um, you know, it does there's a lot of administrative tasks I've handed over to AI to do. When I do research, my research time is cut dramatically because the AI can do all the code writing and everything else and I can look at the answers. It's like I'm an RA. But, it's gotten better than

[37:00] that. I can throw a full academic paper that I've written a couple years ago into chatb 5.2 Pro, which is the smartest model out there. It will find errors that require it to have run its own Monte Carlo analysis on assumptions from a table from table three and table five put together and will say actually you should have done find errors that I couldn't have found otherwise. So especially when used by a skilled human I'm finding everything I do is more efficient. Like I write you know this one useful thing substack. There's a lot

[37:30] of readers. I do not I write all my own first drafts, right? Because I want to be my voice. But if I didn't have Claude checking all the, you know, what I write to make sure it makes sense, it would take me days to put out a piece that takes me a few hours to write because I know I have a good voice as a as a cross checker to work with as a researcher. So, in almost every aspect of what I do, I mean, I use AI for everything. And sometimes it's huge efficiency gains of hours and sometimes it's, you know, a couple minutes here or there. >> Absolutely hear you. I'm everything right now. fact check this. What additional data would be illuminating to

[38:00] my points? Where am I redundant? And the idea of peerreview research in academia, it feels like we're just going to let need fewer peers to review. And one of those peers probably should be AI. No, >> I mean it that peerreview research is in a middle of it was always is always in crisis, right? Just like everything associated with universities, academia, but the crisis is pretty bad right now because um all the signaling associated with papers, right? So peer review depended on you being able to filter out the crap so that you could at least say

[38:30] okay this paper is worth looking at more and worth a couple hours of my time. The problem with AI and there's a nice paper showing this. The problem with AI produced content is it scrambles our signals and it makes it very hard for you to tell whether it's crap or not without a lot of effort. So human peer review is suffering under a flood of tons of papers being produced with AI help and harder to signal which papers are good or bad in advance. So, it's hard for us to spend the time doing this, right? And then, of course, who's reading all these papers now that AI is producing all of them. So, I think we're going to have to include AI in the peer

[39:00] reviewview process, like you said. But then the question is, is AI producing research for AI that it gets published in AI journals that no human ever reads? Like there's a there's sort of a you could fear the fear you hear the creaking underneath the whole edifice of academic publishing as we try and figure out what comes next. It feels like one spot if you really wanted to be hopeful would be medical research. Uh Grant, you're not at the med school, but you are at the business school and healthcare in America is basically been it's monetized. It's now about profits.

[39:30] Are you excited about the potential the intersection between AI and drug discovery and you know my friend Whitney Tilson said that basically Chad GP I'm sorry Gemini diagnosed his father and saved his life. Let's start there. The the health industrial complex in America, how excited are you about the intersection of AI in that industry? And what other industries do you also think really stand to benefit exceptional returns with the advent of AI? >> So, you know, and with the usual caveat

[40:00] that the more uh the more complicated the industry and the more regulated, the slower adoption of AI tends to be, I think medicine is an incredibly exciting area. So you talked about a few areas like one of them is Google especially but other companies are deeply dedicated to how do we automate research or accelerate academic research and I think that there's a lot of value there. We're starting to see actual reasonable scientific work being done by AIS and the hope is that agentic systems can autonomously do directed research in the

[40:30] near future which will lead to a flood of you know because we're researcher constrained a flood of of new discoveries. So there's hope there in that space. I think there's also, you know, when you talk to AI companies like Madna has been very open, I drug companies like Madna has been very open about their use of AI. There's tons of things that companies have to do that slow down the drug development, discovery, and testing process that are administrative. And the AI helps with all of those things. You get huge value legally and in building forms and materials. On the doctor's side, you

[41:00] know, we even just things like translation. It turns out that if you use AI to give people a pre-operative form that they understand, they actually are happier with their surgery, have less issues, and are more likely to report success because they got the information away they understood. Right? Second opinions, you obviously should be using an LM for a second opinion. I can't say you should use it to replace your doctor, but they're good enough that as in every kind of controlled experiment that they're worthwhile, especially where imaging is not

[41:30] involved. They're they're not as good at imaging. So I would not trust a radiologist report from a large language model. But in terms of you know giving a second opinion or if you're stuck amazing at that people that have access to good healthcare or good doctors terrific. And then there's just the administrative breakthrough piece right if the if the forms get filled out by AI if some of the processing gets done by AI doing the grunt work behind the scenes there's possibilities for gains of efficiency over administration. None of these things are automatic though right? They require actual leadership and structural change to make happen.

[42:00] And that's I think the level where things get stuck is not so much can AI do this but how will organizations respond. >> You brought up Mona and I think of vaccines as a technology that the big winners were all of us and that is Mona stock I think is off 90%. And I don't think a lot of companies have, you know, made huge companies on or huge market cap companies in the back of vaccines. When I think about, you know, I've been in four countries in the last 5 days and the ability to skirt along the surface

[42:30] of the atmosphere at 710 the speed of sound. I don't think there's any technology that's changed my life more. And yet airlines and aircraft manufacturers without government subsidies have basically all of them, you know, either gone out of business or going out of business. And it feels like lately we become used to believing that any innovation in technology the market share or the stakeholder gains gets sequestered to a small number of companies. Do you think there's any possibility that the real winners of AI will be us? And that is the sense that we're under this illusion that a small

[43:00] number of companies are going to build multi-trillion dollar market cap companies. But this technology because of the inability to create ring fence distribution or IP that the real value might be disseminated to the general public and we won't see and quite frankly just these current valuations will not hold up which isn't to say AI isn't going to change the world. It's just that change isn't going to involve a small number of companies that are multi-trillion dollar market cap. Could we see a huge destruction in shareholder value across these companies while seeing huge stakeholder value similar to

[43:30] what happened with um you know vaccines or even PCs? >> Well, any frontier company, frontier model company can destroy the market anytime they want given the condition that they release their models open weights, right? Um which is what the Chinese models are doing. They're so it all comes down to whether or not >> Explain open weight. >> Yeah. So an openw weight model is the is so AI is basically are a bunch of math right and the weights inside these models are basically what determine how they operate. So if you have the

[44:00] weights, this set of, you know, the mathematical equations the AI needs, you can run your own AI model, right? So um and once they're out there, no one can claim the back. There's no other piece to it. You just need this piece of information. So increasingly, um what the strategy for the also rans which are the Chinese companies and uh Mistral, which is a French European company, is to release all their AI models open weights. So you can find a ton of people in the United States who run those models. They can run them in their internal safe data centers. They can

[44:30] have you know a third party run them and the only money that you make from that is the money that you have to pay for the power and electricity and you know security and network access to the models. So you don't have to pay anyone a fee for using them. And so right now it's such that those models are much uh are less capable um than what you get from OpenAI or um or anthropic or Gemini. But it's possible that at some point in the future they catch up

[45:00] because the development of a process slows down and at that point then a lot of value flows out of the system. >> And Ethan, are you a father? >> I am. Yes. >> Uh how many kids? >> I have two kids. And when you're you're kind of the helm of the bobsled here of seeing AI and the impact it's going to have on the next generation of you know on the next generation how if and how has it changed your view of the future your kids are going to face and has it in any way

[45:30] changed your approach to parenting or what you'd like to see them prepare for or what skills you think they need to acquire like looking this at through the lens of a dad who als also really understands and is probably going to guess more right than wrong about where this all heads. Has it changed your viewpoint of your kid's future? >> I mean, yes. Right. I mean, there's more uncert There's always uncertainty. As a parent, you worry, right? Are you making the right choices? Are your kids making the right choices? They're their own people. They make their own decisions. It certainly has changed, you know, my

[46:00] view on careers a little bit. I think that thinking about jobs, I don't know what jobs are going to be in the future. One thing we know about work, I'm a professor of entrepreneurship, is that, you know, jobs change. people find all sorts of things to do. I'm less certain that they pick one path and stick with it. I want them to pick jobs that are diverse where they do many different tasks in case AI takes some of them, but I also want them to do what they love. So, I don't know enough what the future holds to discourage them from being a lawyer or a doctor or whatever they want to be because I don't know what that future holds. In terms of actual parenting, I find uh you know, AI useful

[46:30] in a cautious way. I'm kind of lucky enough that my kids were old enough when LLM came out that I weren't wor wasn't worried they'd build a like a parasocial relationship with them. We've we've worked a lot on internet and, you know, how to how to work with these systems. And I don't not worry that they're going to turn to these for, you know, as as serious relationships, but we have spent a lot of time thinking about how you use them for education. So, when they were a little bit younger, I would insist if we used if I used AI to help them, I would actually ask the AI, help me explain this the way I would to a ninth grader. Um, and I take a picture

[47:00] of an assignment and be like, okay, now I can help explain this to you. As they get older, uh, I've, you know, they've increasingly used the kind of quizzing mode. They they know the AI won't teach them unless they ask to be taught. So they use either the study modes for the AI systems or they actually ask them like don't give me an answer, challenge me and quiz me and prepare me and tell me what I don't know. So there's lots of like little talented stuff to use it. Now in terms of the wider future, I don't know what happens. Like I mean I grew up in an age of like we thought nuclear war would happen any moment. Uh I think now we have new anxieties. I'm

[47:30] an anxious parent. Who can't be? But I also think that preparing resilient kids who are self-reliant and have some ability to improvise is more important than ever. >> When I first when my parents got divorced, I moved to this new um elementary school in Tarzana. I think it was Emolina. Anyways, I walked in and the teacher introduced me and then she was started writing and then she turned around and screamed duck and cover. And everyone dove under their desk. I'm like

[48:00] what the [&nbsp;__&nbsp;] And I'm sitting there like not knowing what to do. And she's like, "We do this in case you see a nuclear flash." We were doing duck and cover drills. I mean, it was just we were as if that was going to save us, that this wooden desk was going to protect us from nucle nuclear blast, but we were doing we even had films on it. What to do when the when the Ruskies detonate a nuclear bomb. Do you think the catastrophizing around the offensive nature or possibility for these this AI

[48:30] is uh is overestimated? And then a more personal question, you know, I don't have to ask you. Do you have a go bag? Do you have a plan for if all of a sudden, you know, we lose control and okay, Mollik meet here and we're headed to the, you know, the Appalachian Mountains or whatever. >> Uh, you know, um, so it's funny. I think that like I want some people catastrophizing because that's what government should be doing. Like we need policies and procedures in place

[49:00] catastrophic stuff, right? Like I don't stay up at night which might be dumb, right? There's a lot of very smart people who think AI is going to murder us all. There's a bunch of smart people who think it's going to, you know, become a god and save us all. I you know, maybe it's the business school professor and me or something, but I tend to be really focused on like, oh, there's actually a lot of like humans are flexible. There's a lot of way like we get used to doing many different things and living in many different lifestyles. Our goal should be to guide things in the best direction that we can right now. I am not preparing for the apocalypse on a on a regular basis. I

[49:30] for part of the reason that I think catastrophizing like that isn't that helpful and uh you know I don't know what world you're preparing for a catastrophe in. There's a thousand things that could that could end the world. Um, and so, but I understand and appreciate the anxiety of other people and think it's valuable that they're there as long as we're channeling that into, you know, stop gap measures. I mean, I'd like to see the government think more about catastrophic risk. Not because it's my giant concern, but because very smart people are concerned about it, right? And you don't just get

[50:00] through crises and hope you muddle through. You make plans. I don't think the plans to be made at the individual level. I think it's a societal and governmental level that we need to be starting to think about how to shape AI. And by the way, it's not just catastrophic will AI murder us all or, you know, invent a chemical weapon that would, you know, that kills everybody or will a bad guy using AI do these things, but it's all the other risks they worry about, too. Deep fakes are a real problem. I can create an image of anybody saying anything I want. How do we respond to that as a society, right, of being able to do that stuff? How

[50:30] do we start responding to make sure that, as we talked about earlier, that AI is not automatically translated to job loss, but is that there's a period of exploration to try and figure out how to make it do something better? How do we think about using this in education a positive way? How do we think about avoiding parasocial relationships with AI systems that are negative for us? I mean, these are policy decisions and we can help make that I think are really important. >> Do you think we should agegate synthetic relationships? >> I I think we don't know enough. So, probably caution is warranted, right? Like there is mixed research right now

[51:00] on there are some papers that suggest that AI lowers the uh you know rates of suicide ideation for the very lonely or decreases loneliness of short term. We have no idea what the long-term effects are. I don't think that age gating is a particularly bad idea for synthetic AI characters that try and act like people. Um you know I because we don't know what the effects are. Um I think it's easy to be alarmist and catastrophic about it. The effects may end up being very good. I don't know. But neither does anyone else. Just as we wrap up here and you've been

[51:30] generous with your time, a lot of young people listening to the podcast, you're kind of rounding third. You've you've built a great career for yourself. My sense is you have influence. You do something you enjoy. You're at the right place at the right time. You make a good living. Talk a little bit about your career path and what lessons you can provide to younger people who might be thinking about a career in academia or just general professional advice more generally. You know, the first thing I said and my colleague, Professor Matthew Bidwell talks about this a lot

[52:00] is like careers are long. Like I've studied careers and like there are many different things and mine's an example. I actually uh went uh you grew up in Wisconsin um and uh lived my whole life there and then went to the East Coast for school. uh did the mandatory job of being a consultant for like you know 18 months and then launched a startup company with a with a brilliant friend and roommate uh in 1998 or 1997 where we invented the payw wall. I still feel a little bad about that. Um but nobody really understood what the payw wall was because the internet was

[52:30] new. Um but we were you know through 20some people trying to sell this product to everyone. I personally made every possible mistake in this company. It did well but not not that much thanks to me. decided to get an MBA to figure out how to do it right. Realized nobody knew how to do startups, right? Got a PhD um and then started studying games and education and AI and have had that in the whole thing. So like I've done many things in my career and my main advice to people is that careers are long and there's a tendency especially for young people

[53:00] today who come out of very regimented system to think that they have to have a plan like the next thing you have to be completely prepped for like I need to know everything I need to know to be you know to do something entrepreneurship I hear this all the time like I need to you know learn this and I've worked at this company and that's not how this works right there's no perfect moment there's no perfect skill set and it's an evolution exploratory process I don't think that'll change in the near term with AI. And I think the idea of being flexible, of trying different things, of experimenting, of getting your own skills out there and using your own agency to try and find path forward

[53:30] is the way to go. It's never easy and I've been lucky in a lot of these choices. Um, but I think that there is, you know, that thinking about how you want to, you know, take your next step on your own rather than following a predefined path can be very useful. Ethan Mollik is a professor at the Wharton School and a leading voice on how AI is changing work, creativity, and education. He also writes the popular Substack, one useful thing, and has coined terms including the what is

[54:00] it? The jagged frontier and co-intelligence. And he joins us from his home outside of Philadelphia. Ethan, I love uh seeing people such as yourself who've just put in a ton of work be as successful and as influential as you are. Congratulations on all your success. I trust you're taking time to pause and just register that you, you know, you have arrived, so to speak. >> I haven't taken time to pause, but it is nice to know that I could do that at some point. >> At some point. Thanks, Ethan.

