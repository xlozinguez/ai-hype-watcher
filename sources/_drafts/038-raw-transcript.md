# Raw Transcript: 038

[0:00] Almost everything you see on your computer screen is a construct. Something people have imagined to present to you. Constructs are not technology in themselves. Word processors, email, windows, movie editing systems, playlists, chat rooms. These are all constructs, not technology. Somebody made them up. Yes, there is technology underneath, but the idea generally comes first and then

[0:30] the idea generally comes first and then people figure out mechanisms to make it work. Ted Nelson, geeks bearing gifts, 2008. Welcome to interface studies. My name is Sal. For most of computing history, software sat behind visible interfaces. We clicked and dragged and scrolled because that was how software could interpret our desires. The interface existed because the machine could not predict intent.

[1:00] machine could not predict intent. Now things are changing. Tools like Claude Code, Claude Cowwork, agent swarms and various terminalbased assistants are not built around navigating screens. They're built around interpreting natural language requests, orchestrating APIs, performing multi-step tasks without requiring the user to traverse menus. After a bit of connections and setup, users can speak or type a request and watch the system

[1:30] or type a request and watch the system mobilize services on their behalf, grabbing data from one service, formatting it, combining it with others, then publishing the result. The sequence of steps once requiring page after page of UI interactions now collapses into a short description. This is not automation. It is a delegation without a visible interface. The system infers steps rather than waiting for

[2:00] infers steps rather than waiting for each click. This is happening for a variety of things from coding to checking email and scheduling events. Now if we focus on the interface element only, it is clear that this is not just hype or novelty that traditional interfaces built around menus and controls are no longer the only way we make computers perform our intentions. Increasingly software acts for us. And when that happens, the interface starts

[2:30] when that happens, the interface starts to look strikingly minimal, almost invisible, except for one thing. this input or chat box. Gueies were invented for a reason. They made complex systems legible. Buttons, menus, icons, and panels helped users recognize possible actions, reducing the need to memorize sequences of commands. The principle was recognize, don't recall. We talked about that in previous

[3:00] recall. We talked about that in previous videos. Users who see what's possible feel empowered to manipulate systems with confidence. But as software grew in scale, complexity outpaced the UI's ability to encapsulate it. Settings sprawled out of control. Controls nested deeper. Workflows spanned multiple applications and dashboards of course became dense and intimidating. On any given productivity tool, the list of possible actions and settings could

[3:30] of possible actions and settings could outweigh a user's tolerance for exploration. Recognition becomes visual overload. Language came and offered a shortcut around this. You don't need to know exactly which menu to open to convert data or summarize text. You describe the outcome and the model infers the steps. When the cost of navigation becomes higher than the cost of description,

[4:00] higher than the cost of description, people default to text. That's why terminalike agent tools often feel faster than traditional interfaces, even to non-technical users. This shift is not about returning to the old command line where exact syntax was required. It's something else, a new surface of control where language itself becomes the interface. Instead of choosing from a visible array of possibilities, users describe goals and

[4:30] possibilities, users describe goals and let the system figure out the procedural details. The interface takes a step back. What remains is a conversational exchange. It is true that this might threaten traditional software business. We're saying that in the headlines these days. If a chatbox can handle what once required dedicated applications, the question naturally arises, why should I pay for complex software when I can just ask?

[5:00] ask? But here's where things become a bit more interesting. Something deeper is happening here. It is a bit deeper than industry disruption. What is actually emerging are two distinct paths. Both messy compromises and both still orbiting around that chat input box. In casual use, interacting with a large language model feels like a better version of search. You ask, it responds. Simple enough. But as tasks get more

[5:30] Simple enough. But as tasks get more complex like generating code, designing intricate visuals and images, synthesizing data across sources, casual language alone stops being sufficient. Precision becomes necessary. And this is where something interesting happens. Instead of speaking loosely, we must begin to supply formats, constraints, schemas. Think of JSON prompts, agent skills.

[6:00] Think of JSON prompts, agent skills. Essentially, specifications are now embedded in language. The only interface we have is that input box. But what goes into it starts to look less like a conversation and more like code. Take image generation. A single command like make a beautiful sunset produces something, but it's shaped by the model's default assumptions. A structured prompt with explicit parameters like resolution, style,

[6:30] parameters like resolution, style, composition, lightning angle, color palette not only yields more consistent results, but forces the user to think through their intentions in detail. You're not chatting here. And this image generation can become even better with JSON prompts. We define exact data structures. Agent skills outline stepbystep procedures. Coding assistants work best when we provide context, constraints, and desired outcomes

[7:00] constraints, and desired outcomes upfront. This is not natural language. It's programming through language. Research on structured prompting supports this. An experimental study found that unguided AI use fosters cognitive offloading without improving reasoning quality. When people are required to supply more explicit structure in their interaction, they engage more deeply and think more deliberately about the task at hand. The syntax is conversational.

[7:30] syntax is conversational. True, but the mental model required is specification and we only have this input box for it. It is a new form of control that sits between visual interfaces and traditional programming. Now, not everyone wants to write structured prompts, and not every task compresses neatly into text, so we're seeing another path emerging in this hybrid interface. Claude has its artifacts panel. Chat GPT has canvas.

[8:00] artifacts panel. Chat GPT has canvas. Coding tools like cursor and wind surf have sidebars with file trees, previews and controls. We see that even with image generation tools. Almost any new creative startup like love art has this side panel of chat, although it retains the canvas. These aren't pure language interfaces. They're attempts to give users visual handles on what's happening, ways to inspect and modify outputs without writing more instructions.

[8:30] instructions. But we end up doing so anyway. Many users just default to this sidebox and write. These interfaces are still figuring themselves out. They feel a bit patchy. Part chat, part traditional UI. You talk to generate something. Then you manipulate it visually. You might chat again to manipulate it again. Or you start with visual controls that help you construct better prompts. The boundaries are unclear. What both paths share is this. The chat

[9:00] What both paths share is this. The chat input has become the assumed starting point, the default mode of control. Whether you're learning to write JSON enhanced prompts or generate them or navigating a hybrid GUI, you're still ultimately working through a text field. We've compressed interaction into language. Language turned out to be more demanding than we expected. Either we adapt to think more like machines, speaking in specifications and JSON, or

[9:30] speaking in specifications and JSON, or we patch visual controls back in to help us manage the complexity. And this matters because the interface we use shapes how we think. People might start to internalize that structure, approaching problems by first identifying all constraints, context, and desired outcomes before speaking or typing. ambiguity begins to feel insufficient. Open-endedness can appear

[10:00] insufficient. Open-endedness can appear like an oversight rather than a creative space. Instead of exploring through trial and error, planning becomes more front-loaded. This is reminiscent of a well doumented idea in cognitive science known as cognitive offloading. The use of external tools to reduce mental effort. When we use calculators for arithmetic or search engines for facts, we save memory and calculation effort. We also

[10:30] memory and calculation effort. We also shape how we allocate attention and recall. Studies have shown that such offloading can mean that we remember where to find information rather than the information itself. Recent work on AI suggests similar dynamics at play. A large study investigating how AI tool usage relates to critical thinking found a significant negative correlation between frequent use of AI tools and critical thinking abilities with cognitive offloading acting as their

[11:00] cognitive offloading acting as their mediating factor. Over reliance on AI tools was associated with reduced independent problem solving and evaluation skills. Now, it's important to say that correlation isn't causation. There may be other factors at work here, but the pattern is worth paying attention to, especially for people who had no experience with traditional computing before and were only exposed to this AI paradigm.

[11:30] to this AI paradigm. It is also important that this does not mean that interacting with AI destroys thinking. It is not that AI makes us stupid, but tool use shapes the architecture of attention and deliberation. When interactions require structured specification, they reinforce habits of planning, constrainted articulation, and the ability to be explicit. Over time, this can extend beyond the interface and into other

[12:00] beyond the interface and into other domains of reasoning. There's also a positive to this. Language-based control can enhance clarity. It rewards users who think precisely and can articulate goals explicitly. For many tasks, once hidden behind layers of menus, language offers a direct path from intention to action. What matters here is the cost. There's a cost for everything. System state becomes hidden in this case. In visual interfaces, progress and configuration

[12:30] interfaces, progress and configuration are visible. We're forced to look at them. In agent mediated systems, actions occur behind the scenes. Verifying what occurred often requires deliberate inspections, and it's usually tedious. Lots of text is produced when these agents are reasoning and working, but it's ephemeral, and the underlying mechanics disappear from view. This tendency towards specification rather than exploration is a shift that may

[13:00] than exploration is a shift that may subtly change the kinds of problems we're primed to solve. We may begin to see every problem as something to be specified up front rather than something to be explored through iteration. As text interfaces become more capable, people will rely on them more. I am one. As people adapt their thinking to textbased interaction, they become better at using those interfaces. That fluency makes language feel even

[13:30] That fluency makes language feel even more efficient than anything else. Visual interfaces start to feel slow. More work moves the text, which further shapes cognition, which further privileges text. It's a reinforcing cycle. And while researching this video, I found that there's a term in psychology for a related pattern. It's called the analong effect. Once we learn a particular way to solve problems, that solution can block alternatives. It's a

[14:00] solution can block alternatives. It's a cognitive bias. When the prompt becomes the default tool, every problem starts to look like something that needs instructions. When language becomes the primary control layer, we don't just find new ways to tell machines what to do. We begin to think in those shapes ourselves. The shift from visible interfaces to language-based control is not inevitable, but it's already underway.

[14:30] inevitable, but it's already underway. As we build and adopt tools that execute our will through language, we should pay attention not just to what they do for us, but what they do to us. The design space is still open. The question isn't simply text versus guey. It's whether we can build language interfaces that don't require us to think like compilers or settle for patchy hybrids that still center everything around a chat box.

[15:00] center everything around a chat box. What would it mean then to get this right? One thing to think about is maybe building systems that know when to resist the urge to execute immediately. That encourages exploration before specification. Maybe it means interfaces that make their reasoning visible rather than hiding it behind ephemeral interfaces and walls of generated text. Maybe it means recognizing that not every problem should compress into a prompt.

[15:30] prompt. These were just few ideas. Here's what at stake. A generation of people learning to solve problems will internalize whatever interface paradigm dominates their formative years. If that paradigm teaches them that exploration is just failed planning, ambiguity is inefficiency. will have built tools that make us more productive in short term while quietly narrowing how we think in long term.

[16:00] narrowing how we think in long term. Language may be a powerful interface, but human experience does not compress cleanly into text. The tools we're building right now will shape not just how the next generation works, but how they think about problems, approach solutions, and understand what's possible. There's no doubt that we're going through a radical shift in the history of computing and it is indeed a new paradigm. But in this paradigm and even the previous ones, design was never

[16:30] the previous ones, design was never about boxes and buttons. It's about designing how minds think. And that's something worth getting right.

