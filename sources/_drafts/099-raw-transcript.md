In my previous video, I went over how AI agent can store memories. But this is only half the problem. These files aren't useful if you don't have an effective way to load them into context at the right times. So, how do agents actually read and use the memory? In this video, we're diving into how agents find the proper memory entries at the right time. We're covering a few key pillars. First, keyword search versus semantic search. Second, why a hybrid approach is actually the answer. Third, reranking and what it means for your AI systems. And finally, a real world example. We'll take a look at how it all works inside of Open Claw. Finding memory entries via keyword search is the easiest to implement. Cloud Code heavily relies on tools like GP to find what it needs. It's all about literal word matches. Interestingly, the Cloud Code team started with a vector database, but they found a GP at aentic search actually performed better and was easier to maintain. A step up from basic GP is an algorithm called BM25. Instead of just finding matches, it ranks them based on relevance. It factors in how often a term appears and how unique that term is across your content. This is what powers full text search in databases like Seagull's FTS5 extension. Keyword search looks for content based on matching keywords you provide to the text. Semantic search is different. The goal is to search by meaning and intent, not just literal words. To do this, the text gets converted into embeddings. An embedding is a list of numbers that represents the meaning of text. For example, how do I speed up my app? Might become a vector with potentially over a thousand values. Each number represents some aspect of meaning behind a text. Here's the magic. Text with similar meaning ends up having similar numbers. The phrase, "How do I speed up my app?" and tips for improving applications performance produce similar lists of numbers, but best restaurants near me produce vastly different numbers because it's unrelated. To generate those embeddings, you can use an external service or an open- source option. OpenI's embeddings API is one of the most popular. Their embedding model has been trained on massive amounts of text to understand relationships between words and concepts. You send a text and get back an embedding. You don't need to understand the math behind how embeddings are generated to use them. Once you have a way to map text to meaning, you need a way to store and search across possibly thousands or millions of embeddings. This is where vector databases come in. You can use a dedicated vector database like pine cone. Or if you're just starting out, popular databases like Postgress and even SQLite have extensions to support vector search. These are PG vector and SQLite vect. Open claw uses SQLite and SQLite vec. We'll dig into that more in a moment.

With text converted to embeddings and stored in the vector database, let's talk about how search actually works. A typical search has two steps. First, you take the user's query and you convert it into an embedding itself. Second, you take that embedding and search across the vector database. This search uses mathematical distance between vectors, how close or far apart they are in meaning. The most common approach is called nearest neighbor search. Think of it like plotting coordinates on a map. Your search query is where you are. The content in the database are all the other text embeddings. The closer a dot is on the map to your position, the more related it is. The difference is that embeddings have hundreds or even thousands of dimensions. Searching embeddings one at a time works for small data sets, but at scale you need a special index. Vector databases typically use an approximate nearest neighbor algorithm. This trades some accuracy for speed, but in practice, the trade-off is negligible. results are almost identical but significantly faster. You might think semantic search is the complete answer but it has its drawbacks. Semantic search does not perform well for literal string searches. For example, looking for an error code like error connection refused. Semantic search may not find irrelevant results. If you're looking for a function like say use state, the same problem might happen. It can't reliably handle exact text matches and specific identifiers. The answer is to combine both of these approaches. Run both a keyword search and a semantic search in parallel. Then use a technique called fusion to combine the results. There are two common approaches. First, there's weighted score fusion. This takes the scores from both searches and combines them with weights. Open claw uses this approach. They weigh vector scores at about 70% and keyword scores at 30%. Second, there's reciprocal rank fusion or RRF. You take two separately ranked searches, semantic and keyword. Instead of using raw scores, RRF looks at where each result ranked in both searches. A result that ranked number two in semantic search and number three in keyword gets a combined score based on both of those positions. Results that show up near the top in both lists naturally bubble up to the top of the final results. Weighted fusion gives you more control and preserves how strong each individual match was. RF is simpler, but treats a near-perfect match the same as a decent one. It only cares about position, not strength. Choose based on your needs. Now that we have a combined result using Fusion, we have a ranked list. But this ranking is based on purely math, vector distances, and keyword scores. it doesn't have the ability to understand nuance of what the user is actually looking for. Reranking is a technique where you take these results and pass them to a model that can evaluate each result against the user's original query. The model then applies

a nuance judgment about relevance. It could be an LLM or a specialized model like cohhere. The trade-off cost and latency. Adding another step and potentially another API call means that there's more time before getting the final results. If you're wondering why not just start with an LM for search, the answer is going to be speed and context window limits. The initial search needs to scan thousands of embeddings quickly. Ranking is slower but more accurate. That's fine. You've already offloaded the bulk of the search and you're providing it with a refined set of results to rerank. Now, let's take a look at a real world example in OpenClaw. In my last video, I mentioned that OpenClaw uses markdown files for memory, but it also has a memory management feature. It supports two systems for memory backend. The default system uses SQLite and vector embeddings. An optional system is managed by QMD which is an open source library by Toby Lutkkey. In this video, we're going to cover the default system. The default memory layer requires an embedding provider to be configured. Open clause supports four options. First, a local model available via OAMA. Second, OpenAI via text embedding 3 small. Third, Google Gemini via Gemini embedding 001. And fourth, Voyage AI via Voyage for large. You can specify which provider to use in your configuration or you can leave it set to auto. In auto mode, Open Claw checks for a local model first. If it's not available, it tries each remote provider in order. OpenAI, Gemini, then Voyage. It uses whichever API key is configured. If none are available, then memory search is simply disabled. The default system stores everything in a single SQLite database. A few key tables power this. The files table tracks each memory file. It stores the path, a content hash, and the last modified time. This enables incremental syncing. The chunks table is where things get interesting. Openclaw doesn't embed entire files. Instead, it breaks markdown files into chunks of roughly 400 tokens with an 80 token overlap between chunks. Each chunk stores its text, the embedding vector, and the line range it came from in the original file. That line range is what lets the agent site exactly where a memory came from. Then there are two virtual tables that power the actual search. Chunks FTS is an FTS5 full text search table. It enables BM25 keyword ranking. Chunks vec is a SQLite vect table that stores embeddings as flow 32 arrays for cosign similarity search. There's also an embedding cache table. This caches embeddings by a hash of the input text. It's key to the provider and model. So if you rechunk a file but that text hasn't changed, it skips the embedding API call entirely. This is a nice cost optimization since embedding calls can add up pretty quickly. Now for search itself, when the agent calls the memory search tool, here's what happens under the hood.

The query text gets embedded using the same provider that indexed the files. Then two searches run in parallel. The keyword search tokenizes the query and runs them against the FTS5 table. BM25 ranks the results. Those raw ranks get converted from a 0 to1 score using the formula 1 / 1 plus the rank. The vector search takes the query embedding. It uses SQLite's vec distance cosine functionality to find the nearest chunks. The cosine distance gets flipped into a similarity score one minus the distance. Both searches use a candidate multiplier. If you ask for six results, each search actually returns up to 24 candidates. That 4x multiplier gives the fusion step more to work with. Then the weighted score fusion combines them. 0.7 times the vector score plus 0.3 times the text score. Results that appear in both searches get both scores combined. Results from only one search get zero for the other. After fusion, everything is sorted by the final score. It's filtered by the minimum threshold of 0.35 and capped the requested result count. Open claw exposes two tools for the agent to interact with memory. First, there's memory search. The system describes it as a mandatory recall step. Its prompt tells the agent to search memory before answering questions about prior work, decisions, dates, people, preferences, or to-dos. The tool takes a query string and optionally a max results count and a minimum score threshold. It returns an array of snippets. Each has the file path, line numbers, relevant score, and a text preview of up to 700 characters. It also includes citation information so that the agent can reference exactly where the memory came from. The second available tool is the memory get tool. This is the follow-up tool. After finding relevant results with a search, the agent can use memory get to read specific section of a memory file. It takes a file path, an optional starting line number, and an optional line count. This lets the agent pull just the right context it needs without loading the entire files into the context window. This two-step pattern of search then get is deliberate. The search returns just enough context to decide what's relevant. Then the agent fetches only the specific content it actually needs. It keeps the context window lean and efficient. The last piece of the puzzle is how memory files become searchable. Open claw uses an incremental sync system with multiple triggers. A file watcher monitors the memory MD file and the memory directory. When a file changes, the watcher debounces for about 1 and a half seconds. It marks the index as dirty and then it triggers a sync. During the sync, OpenClaw lists all the memory files. It compares each file's content hash against what's stored in the files table. If the hash matches, that file is skipped entirely. Only files that actually changed get rechunked and re-mbed. This is where the efficiency happens. When a file does

need updating, it gets split into those roughly 400 token chunks. Each chunk's text is hashed and checked against the embeddings cache. If there's a cache hit, the embedding is reused. Otherwise, a new embedding is generated via the configured provider. All of this runs with a concurrency limit to avoid overwhelming the embedding API. There's also a full reindex trigger. If the embedding provider or model changes or if the chunk size configuration changes, OpenClaw detects the mismatch in its metadata table. It rebuilds the entire index from scratch. It does this safely by building the new index into a temporary database. Then it atomically swaps it into place. For session transcripts, there's a delta tracking system. It monitors how many bytes and messages have accumulated since the last sync. Once a threshold has crossed, it triggers a sync for just those session files. The entire system ensures that the agents memory stays searchable without expensive full reindex operation on every change. That's the complete picture. From storing memories to searching them to keeping them in sync, OpenClaw's memory system is a masterclass in practical AI architecture. Thanks for watching. If you found this breakdown helpful, please like and subscribe. And if you're curious about running OpenClaw yourself or want to learn more about building AI systems with cloud code, check out the links in the description. I'll see you in the next one.