# Raw Transcript: The Rise of WebMCP

- **URL**: https://www.youtube.com/watch?v=35oWt7u2b-g
- **Duration**: 10:05
- **Captured**: 2026-02-13
- **Method**: yt-dlp VTT extraction

---

[00:00] Right now, when your agent visits a website, it's basically a tourist who doesn't speak the language of that site. And this is the same whether you're building lang chain agents, claude code agents, or even new things like open core. Basically, the agent is guessing which buttons to press. It's scraping HTML, perhaps using screenshots. Now, if you've been building agents for a while, you know exactly how painful that is. Well, earlier this week, the Google Chrome team has shipped an early preview

[00:31] of WebM MCP. This is a whole new standard that lets any website expose structured tools directly to AI agents. No more need for scraping. No more need for thousands of screenshots. Your agent just calls functions. In this video, I'm going to talk about exactly how WebMCP works, the two different APIs, why this is set up to actually cut token costs, and perhaps some of the problems that this solves that people really haven't even been talking about yet. So, if

[01:02] you're building AI agents or you're building websites and products that you want agents to use, this literally could be the most important thing to land in Chrome in years. So, let me break it down. So, it's been pretty clear for a while that one of the biggest things you want agents to do is to interact with the web. And we're actually seeing this a lot in the way that people use Open Claw to do different kinds of research, to go to web pages, to download them, to take screenshots, etc. And for far too

[01:34] long, the sort of two main ways that people have done that is either through screenshots which they pass into a multimodal model. And then ideally the multimodal model can tell you not only what's on the screenshot but also where the buttons are, where form fields are, where text fields are, all these sorts of things. The second way that people have actually done this is to access the DOM directly and access often just the sort of raw HTML code and perhaps JavaScript code to use that as a way to

[02:06] work out what are the elements on the page that are interactive, how would you interact with them, etc. But really both of these is like speaking a foreign language to the actual website, right? you're having to use huge amounts of tokens, especially if you're sort of doing the whole images only route. You're going to be using thousands of tokens for each image that you process. And even if you go the HTML route, you're also speaking a foreign language in that you've got to translate from

[02:36] basically HTML to more summarized and simplified information for your agent so that it can actually act on that. We don't really care about things like paragraph tags or different kinds of CSS for making something look good. Yet at the moment, a lot of that is what gets used for agents to be able to do this. So the idea here is actually pretty simple. The idea is make it so that each web page can act as an MCP and a model

[03:08] can call it to get various information. And it's kind of like the model's going to say to the web page, hey, what's on this page? Tell me what I should be able to read. Tell me what I should be able to click. Tell me what I should be able to fill in. And this idea is not totally new. It's been kicking around a lot last year with both academic people sort of proposing papers around this. But also in the second half of last year, Microsoft and Google got together and actually proposed a whole idea of how

[03:38] you would actually treat this, how you could actually build something like this so that it would give agents a much simpler way to be able to interact with websites but not have to totally go the route of an API where everything has to be an API call to a backend. This would still allow normal users to be able to interact with the websites. it would just make it more efficient for agents to interact as well. So some of the things that were being proposed for this sort of thing last year in Chrome were things like script tools in Microsoft

[04:09] Edge. I think they had something like web model context. But around the end of the third quarter last year, Microsoft and Google got together to start putting together this sort of spec of how this would actually work. Now if you think back, this was around the time that we saw Perplexity release Comet, OpenAI release Atlas. you know, a lot of the ideas of web interaction were certainly taking off. The thing that makes this kind of different though is that the way they've approached this, it's actually

[04:39] going to be used for sort of human in the loop first stuff and that the agent will be able to work with the user, not just fully autonomously. So, humans are still going to use web pages, but we'd like to be able to speed up and improve the quality that they get from web pages by being able to have those interactions empowered with agents as well. So, at the web AI summit last year, there was a nice talk all about this and one of the Googlers sort of presented how there are three pillars to this. So the first one

[05:12] being sort of context and this is like all the data that agents need to understand what the user is doing. So if you're watching a video, if you're into reading an article or something like that, you want the agent to sort of know not only what's on the screen, but really you would like the agent to be able to know sort of what has come before this as well. So if the user were to ask a question or ask the agent to do something that relates to something that's not in that current screenshot,

[05:43] the agent should be able to still understand this. The second big pillar is the whole idea of capabilities. And this is actions on the user's behalf. So this is not just sort of things like answering questions, but this is actually doing stuff like, hey, I'm too lazy to fill out this form. Please go and fill out this form based on everything that you know about me, etc. And the third big pillar is the pillar of coordination. So this is sort of how do you control the flow between the user and the agent. And a nice example of

[06:14] this is what happens if you've asked your agent to go and buy a certain brand or type of milk and while the site has milk for sale, it doesn't have what you actually want. In this case, you want to be able to easily pass back to the user for a human in the loop kind of thing. Now, while you're not going to find all of this in every Chrome user and every website straight away, the latest version of Chrome will actually have all of this behind a flag where you're able

[06:44] to turn it on. You're able to actually engage with this. And you got to think that this is something that perhaps Google's going to roll out in full at Google Cloud Next or at Google IO in the next few months. So, I do think this is going to be something that's going to move quite quickly. Okay. So an example of how this would work is that your particular web page would expose certain functions. So if you had the ability for someone to be able to do some kind of visual search on the site, something like that, that would also be in the UI

[07:16] that would also be exposed as a function like search products. So, the site would basically have a registered search products tool that would make it much easier for your agent to actually go in and use the search and find what it is rather than having to sort of click through lots of different filters, scrolling through different pages, etc. The other thing too is that then the agent would actually get back structured results. So you can imagine this sort of

[07:46] one tool call replaces what could have been dozens of interactions that would have been needed if you were just using a browser use kind of thing. So the way that they've currently structured these interactions is that they've got two main APIs. They've got the declarative API and the imperative API. So the declarative API is all about standard actions that can take your existing HTML forms and can add things like a tool name description so that your agent can

[08:17] use that. Now if you've got pretty good well ststructured HTML forms, it sounds like you're probably kind of like 80% of the way done for that already. The second one is the imperative API. So this is for more complex dynamic interactions that perhaps going to require things like JavaScript execution. So the idea here is that this goes beyond just forms. And here you're going to decide okay what's going to be the sort of schema for this. And in many ways these are probably going to be like

[08:47] defining tools that you would send to say the open AI or anthropic API endpoints etc. Obviously though this is all going to be running client side in the browser. So, while this is still perhaps early days for this actually rolling out to the mass public and stuff like that, it is literally just around the corner. This is no longer just a debate about the specs of this. This is now already in Chrome, even if it's behind a flag. And if you want to get started, you can actually join the Chrome early preview program to

[09:19] basically get more access to some of these APIs and to what actually Google is doing here. So, I'm sure over the next month or two, we'll see more of this actually roll out. And we'll probably see a number of tools or perhaps even things like chord skills that allow you to take a particular website and convert it and set it up to actually have its own sort of web MCPS on it. But definitely for now, if you're looking at building things with agents, building things with AI, this is something that you definitely want to

[09:49] have on your radar. So, check out the links in the description and let me know in the comments if you've got an interesting take of where you could see this actually going. I definitely think this is one of the interesting things that's not getting a lot of coverage at the moment. Anyway, as always, I will talk to you in the next video. Bye for
