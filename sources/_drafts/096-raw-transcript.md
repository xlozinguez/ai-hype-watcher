If people started to pull money from open AI and they couldn't raise money, that would be a cascading effect in terms of the entire ecosystem. >> Exactly. That's exactly what I think is going to happen. Our guest today is Gary Marcus. He is a real critic of large language models and what they're capable of. >> The world has gone allin on neural networks and invested massively in it on an idea that to me never made sense. Large language models are not going to get us to the holy grail of so-called artificial general intelligence. In the investment community, people are starting to say, "Hey, I see all this circular financing results on return on investment are not that great. Maybe this doesn't make sense. [music] Hey, this is Steve Eisman and welcome to another edition of the Real Eman. And today we have a different kind of guest. Today, our guest today is Gary Marcus, who some of you may have heard of cuz I've discussed him on my podcast many, many times. He is a real critic of large language models and what they're capable of, which is really the foundation of the entire AI story. And so, Gary's is going to discuss his theory, what LLM models are all about. Gary, welcome to the Real Eman. >> Thanks for having me. And also thanks for that incredible shout out you gave me on CNBC a month or two ago. >> Oh, you're very welcome. Well, it was welld deserved. Before we get started, why don't you give my audience, most of them don't know you, so why don't you just give people your background to so that they understand you're actually entitled to have an opinion on this subject. >> So, I've been studying intelligence kind of all my life. I started working on AI when I was 10 years old, as soon as I learned to code. And I spent a lot of my career studying natural intelligence, humans, how kids learn language and things like that. But my dissertation, which I did at MIT, was on two things. It was on how kids learn language, but it was also on neural networks. Um, which are a particular approach to artificial intelligence or to modeling the human mind that is I would say loosely inspired by the brain. It's a very nice piece of marketing. It makes you think that it's really based on the brain. It's not really um it has a loose affiliation, but these things called neural networks were popular. I looked at them in the '9s and they weren't really such good models of how human minds work. But I got involved in trying to understand what they actually did. And then when deep learning came back in fashion in 2012, I was like, hold on, I've seen this stuff before. It's a lot like what I did my dissertation on. And I had written a book in 2001

called the algebraic mind where I actually anticipated the problem of hallucinations and some of the problems of reasoning that we're seeing >> which we're going to talk about >> and we'll get there. Uh and so a lot of the issues to me were familiar when this new thing came back in fashion and I knew immediately that it was going to have problems in certain things. I wrote a piece in the New Yorker in 2012 called is deep learning a revolution in artificial intelligence? And I said, 'Look, this stuff is really interesting. I admire Jeff Hinton for having stuck to his ground for a long time. >> Who was that? >> Jeff Hinton. He just won the Nobel Prize last year and he's one of the major players in deep learning. I see. >> Um, as is his his student Ilius, who's someone who recently came around to my side. So, he was a major player in this field. He kind of kept the torch going when neural networks were not very popular to his credit. Um, but you know, not everything goes to his credit. We don't have to go into all of it, but um you he kept the torch going. He resuscitated this um and relevant to your listeners. What re really resuscitated was his student Ilia Sitzkever and maybe a couple other people figured out how to take these systems that they had been building for a long time. I mean really they go back to the 1940s and Hinton did some important work in the mid 80s. They showed you could do the same stuff but with GPUs, graphics processing units that Nvidia was building. And at the time, Nvidia was just building them for video games mostly. >> And so these things existed for video games. And the student had the clever idea that what they were really good at was parallel computation, which is to say computing something um in many little pieces at the same time instead of one at a time. So the classic paradigm is step by step. the central processing unit goes through some software program line by line. That isn't entirely true anymore, but it's kind of the simplification that people might learn if they took computer science 101. And what GPUs did is they let you break up a a problem into a bunch of little pieces that you do at the same time. They were built to do this for computer graphics. You'd look at a screen or you would want to render a screen. You'd want to say, "What's the next frame in this video game?" Instead of doing it one thing at a time, which would take forever, you would do the whole scene at once. So, you'd have one little subprocessor doing this pixel and another one doing that pixel and so on. And I mean, they work great for that. I I've occasionally played video games and what

graphics processing units could do is incredible. Um so so Skaver and um I'm blanking on the other uh guy who was an author on the paper showed was a good way of running these so-called neural networks and we can talk about what those really are and what that means but they found a way to run them on graphics processing units and that mean meant that they could go two things meant they could go much faster and they could use much more data everything that people had done in that field at that point I don't know 60 years old or whatever it was was kind of like a toy model and they showed you could do it for real if you did it on a GPU. You could do it at a much bigger scale. And so everything that we look at was kind of born in that moment in 2012. And as soon as it happened, two things happened. The new New York Times wrote an article about how amazing this stuff was. And the next day, I wrote a piece in the New Yorker. I was writing a lot for the New Yorker then for their blog. I wrote a piece in the New Yorker saying, "It's great, but it does have these problems. It's always going to be good at some things but not good at other things. It's always going to be good at pattern recognition and statistical analysis and that's great but we also do a lot of abstraction. So we know how a family tree works or we can reason about things in the world and it's never going to be really good at that. It's not really suited to that. And this was obvious to me from the work I had done on these systems in their earlier carnations and also from the work I had done on human cognition understanding how human minds work. If you know Daniel Conorman's famous book, thinking fast and slow, that was >> I've read it. >> You know, he argues for what he calls system one and system two cognition. The system one is fast. It's automatic. It's statistical. It's reflexive. And system two is slower. It's more deliberative. It's more about reasoning. Neural networks are basically like system one. And that's fine. That's part of what we do as humans. But part of what we do is the system two stuff. Especially on a good day. We do the slower stuff. We're more deliberative. We're more reasoned about it. And these systems have never been good at that and they still aren't good at that. And what I said in 2012 is, you know, they do the one but not the other. And at one level of abstraction, what has happened in the 14 years since is that the world has gone allin on neural networks thinking >> and when you say neural networks, this this is what are called large

language models. >> So large language models are a form of neural networks. Sorry, I didn't spell that out. Um and in fact they didn't exist in 2012. So a few things happened but basically since 2012 people developed the large language models. The origins of that is really the transformer paper that came out in 2017 and invested massively in it. Trillions of dollars at this point or two trillion dollars would be my back of the envelope estimate on an idea that to me never made sense. You know, they their idea was we were going to get to everything we need for intelligence, artificial general intelligence, but they they weren't careful about the system two stuff. They were at first they just treated it like a giant black box and still many people do. It's like we'll throw all this data in there and we'll get this system that will do what we need for intelligence without any sophistication from a kind of scientific perspective about what that would really look like. I think these people were very naive and I tried to point this out and this it goes to the lone wolf and stuff. For a long time people were just dismissive. They're like we have this thing called >> they were more than dismissive of you. They were disdainful. I mean I >> really disdainful. >> Um we could probably come up with some some other stuff. I was disillusioned with them. We could go on that for a little while. Um and they they were openly hostile to me like uh Open AI has an emoji for Gary Marcus. I read this in [laughter] there now. >> Well, it's flattering in a way. >> It's flattering and next level at the same time, right? I tried to take it with a sense of humor as you can see, but um you know, that is is a measure and you know, Sam Alman called me a troll on Twitter or whatever. Um you know, they really didn't want to hear what I had to say. The core of what I had to say was in 2022, I wrote a paper called deep learning is hitting a wall. And what I said in that paper is you can keep scaling this stuff which was the idea that was already starting to get popular. Maybe I should pause for a second. The idea of scaling was that we could just pour more data and more GPU >> and make the model bigger bigger bigger bigger >> make the model bigger and bigger and it would get to be amazing. And they had some data in support of this but it was also naive. Um I call it the trillion pound baby fallacy. Right? So your baby weighs you know 8 pounds at birth. a month later at 16 pounds. That doesn't mean it's going to be 32 and 64 and that it's going to be a

trillion pounds when it goes to college, right? They made this very naive kind of inference, which I'm sure you see all the time um in the business world, but a lot of people, a lot of smart people with a lot of money made this kind of bet. They said, "We see these laws and we project that we will get to intelligence by putting in such and such amount of data." And it's >> could we just pause just for a second? >> Let's just backtrack for a second. the large language models like what do they what do they do and what do these people think they're supposed to do? But I I really want to kind of break that down. There's a really great way to ask the question. So what they do fundamentally is they predict next things in a sequence. So think about autocorrect on your iPhone if you have an iPhone, right? Similar >> which sometimes drives me absolutely insane. But go ahead. It doesn't always work, but the idea is you're typing out a sentence and it predicts what might come next. So you say, "Meet me at the in restaurants are pretty good guess, right?" So you make a statistical analysis of what people say and you know you do okay, not perfectly and sometimes it makes mistakes and it's annoying but that's what we call autocomplete and I call LLM's autocomplete on steroids. They're a special way of doing that prediction process. That's fundamentally what they do. And there's some interesting things about how they do it. One of which is they break everything into little bits and then they reconstruct things which means they actually lose connections between information which means they sometimes hallucinate. They make stuff up. >> Well, let's we'll come back to the hallucination. >> So come back to hallucinations. That's one of the characteristic errors and that's one I pointed out in 2001 before LLMs were even invented. I said if you keep following this approach to its kind of logical destination, you're going to have this problem. And we turned out to have that problem. So LLMs break stuff up into little bits and they make predictions about what might come next. That works surprisingly well if what you train them on, what you expose them to is the entire internet because almost any question, but almost is a critical word there. Almost any question that you might think of, somebody's asked before and somebody's answered before. And at some level, these are like glorified memorization machines. Um, >> there was a article in the Atlantic about this just the other day and there's lots of evidence for this right along. So, for example, if you type in part of Harry Potter, it will just finish the, you know, the paragraph or something like that because it's basically memorized things. And if you basically memorize the entire internet, that's kind of special. Like, because

when you go and ask a question like, you know, where did the Dodgers play um, you know, before they were in Los Angeles, there's lots of sentences. They'll tell you they were born in Brooklyn. And so you have a pretty good chance of coming up with the answer. >> However, just doing that doesn't give you abstract concepts and ideas. And sometimes you have problems where these little bits get broken up and then reassembled incorrectly. >> So can we talk about hallucinations for a second? That's what hallucinations. >> Define hallucination. Why does that happen? >> Hallucinations are when it makes something up, presents it with perfect confidence, and it's just not true. >> So give us an example. My favorite example involves a guy you might know, Harry Sheerer. I don't know if you ever saw Spinal Tap. >> Oh, sure. >> So, he plays the bass in in Spinal Tap. And he's, as it happens, he's a friend of mine. So, he plays bass in Spinal Tap. And also, and it matters to the story that he's reasonably wellknown. So, he he um he played bass and Spinal Tap. He was in a bunch of those movies with Christopher J. Guest. He was in the Truman Show. And he does the voices for Mr. Burns and the Simpson and a few other characters. >> Okay. Um, so this is what makes this story interesting. So I'm going to rewind for one second, which is to say my old favorite example of hallucinations involve me. Somebody sent me a biography of me that said I owned a pet chicken named Henrietta, which I don't. So that's a pretty good example of hallucination. It's just made up. Turns out there was an author named or an illustrator named Gary Oswald or something who wrote a book about Henrietta goes to school or and it's just like munching together all these little bits of information. Why does it hallucinate? >> So, this has to do with the breaking up of little bits of information. So, let me walk you through the Harry Sheer example. So, I kept using the pet chicken Henrietta thing. So, one day he writes to me and says, "No, Henrietta, but then he gives an example where the hallucination is about him." He's much more famous than I am, or at least than I used to be. I'm starting to get a little um and um it says that he's a British voiceover actor and comedian, only he's not British. If you went to Wikipedia for two seconds, you would see he was born in Los Angeles. And because he's famous, you could also go to Rotten Tomatoes or to IMDb and he's done lots of interviews and talked about where he's growing up. He was a child star on the Jack Benning Show in Los Angeles. Like, it's not hard to find the right information. We imagine falsely that large language models are

intelligent beings like us, but really all they're doing is reconstructing statistically probable relationships between bits of information >> and so they can be wrong >> and those can be wrong. Sometimes those reconstructions are wrong and in this case it was wrong. In some sense, what it does is it builds like a cluster of things that predict other things statistically. And it turns out there's a lot of British voiceover actors and comedians. There's, you know, Ricky Gerve and and um um John Cle and etc, etc., right? Um and so it just blurs all of that stuff together. And the blurring process on the whole works reasonably well, but you can never trust any particular thing is going to be right. And so these hallucinations happen all the time. There was a guy who was tracking legal cases where lawyers published or not published but submitted briefs where they were made up references, citations to cases that didn't exist. The first time I looked, he had found like 300 cases. The next time it was like 3 months later, he found 600 cases of lawyers not only did this but got caught, got busted by judges doing this. They they using these tools like chat GPT to do their work for them. But it makes mistakes and those mistakes, this is the insidious part, slip by. People don't notice. Another example is um CNET was one of the first places to use AI to write its articles. And in the first batch of 75, like half of them had errors. The editors didn't notice them because everything is grammatical and well-formed and there aren't typos and people tune out. I call it the looks good to me effect. LLM give you the looks good to me effect which has led to another new term that I wish I had invented called work slop. So work slop which um a couple professors I guess invented last year is a term for people write these reports they submit them you know to their employer they look superficially good but they're wrong they have mistakes in them because LLMs don't really understand >> and what you're saying is LLMs don't think. They really don't think >> they just slam things together that statistically normally makes sense to slam them together. >> They they they slam exactly or glom is another verb I like there. They glom things together >> and and you know a lot of them are right statistically speaking and some of them are wrong. And the systems don't know the difference. They can't tell you the difference. They don't ever say like, "Well, it seems to me that, you know, everything like Wikipedia says that Harry Sheer was born in in Los Angeles, but I have the vibe as an LLM that it's London and maybe you should check that." I mean, they never give you any of that. They just present everything as if they were

an encyclopedia, no matter whether it's true or not, which is one of the reasons why these systems are insidious. In my Substack that's going to come out before this airs that I've already written in my next substack >> subscriber, >> um, thank you for subscribing. [laughter] Um, I'm going to talk about a new article I saw this morning that blew my mind that talked about basically how LLMs are undermining the institutions of society, including things like democracy and civic order and so forth in no small part because of this tendency towards errors, >> right? So they're undermining the quality of almost all the institutions in life by making fake information and basically undermining the information ecosphere. I mean, just like think about democracy. Like democracy only works if the voters actually understand what's going on, right? The whole point is the voters reflect on the world and then they bring their perspective from, you know, their religion, their childhood, their neighborhood, or whatever. But if everybody's getting garbage all the time, that doesn't work anymore. >> Well, I was I was reading on the day that Maduro got taken by the United States, you you wrote here, we were all watching, you know, like the whole country sitting there watching like CNN or or whomever. We're literally watching this guy get taken out of Venezuela and Chat GPT is telling you that it hasn't happened. >> That's right. That it is >> that it's fake news. That we're watching it on the TV and they're telling you it's fake news. >> Yeah. I mean, there are many problems. That's a different problem, but it's not unrelated. That's >> what is that problem? >> That problem is these things all have a cut off date. So they get trained at a particular moment and the there's kind of a core model that only knows what's happened up to then and then they put different band-aids on it like they make it do web search and whatever but none of the band-aids are very well integrated and you know some of the systems do that better than others. So has has a problem with novelty, something new. >> They that is the deepest problem actually going, you know, going back to my work in 1998, I realized very early on that was the deepest problem with these systems is if you're basically a glorified memorization machine and I bring you something that's far enough away from what you've seen before, you're in trouble. amazing example of this. I don't know the underlying details, but um Tesla um does a lot of this kind of memorization kind of stuff, but its AI systems are not that sophisticated. And one day someone used the summon feature. You remember how Elon said that you're going to be able to call your car from Los Angeles to New York? You can't really do that, but you can call it across a

parking lot, apparently. So, somebody did this at an airplane trade show. You can find the video on YouTube. They they call their car. They want to be like, "Cool, I'm at an airplane shade show and I'm going to have my Tesla come over to me." And it ran directly into a $3.5 million jet, right? The system [laughter] didn't have in its training data what to do with a jet because who trains a car to avoid a jet? >> And so it didn't have a general understanding of the world, like don't drive into things that are expensive or big or physical objects in your path. Didn't understand any of that. It's just like looking for things that match bicycles and pedestrians and so forth. They didn't have a category for jets and so we ran into it. >> Let me let me press a question. So, >> hi Steve Eisman here. You listen to my show because I try to give you the facts about the market and my opinion. But it's not always easy to filter through all the information and hype out there, especially when it comes to figuring out how to invest. And if you've had some success in life, you are probably getting bombarded with investment advice. Suddenly, everyone has a great new idea or product for you, a new fund, a tax scheme, a once- ina-lifetime private deal. How do you know what's real? By joining a community of people who face the same concerns as you. That is why I'm recommending you take a look at Longle. Longle is not a wealth manager. They aren't trying to sell you a product. It is a private vetted community of high- networth investors, entrepreneurs, executives, and professionals. It's a community filled with people like you. Members use it to compare notes on everything from investment ideas to due diligence on private deals to navigating complex tax codes to the personal stuff like how to raise grounded kids when you have money. I talk a lot about old school due diligence and that is what this is all about. It's 6,500 vetted members sharing unbiased intelligence. No salesman allowed. If you're looking for a place to ask questions and get answers from people who aren't trying to earn a commission off you, this is it. Go to longangle.com/isman. Membership is free, but you have to qualify. Again, that's longangle.com/isman to apply. These these models have scaled. You know, it was chat GPT1, chatp2, 3, 4, 5, and now Gemini is supposed to be better. When when people say that the new Gemini is better than let's say Chat GPT5, what does that mean to in in in practicality? That's actually a really interesting question because there isn't a simple answer to it. What they're really saying is like for the things I do, I happen to get better results. Most of the people who are saying that are saying that intuitively without even

quantitative data. And the thing about these systems is we're pressing them into a very broad service unlike almost anything else. Like if you have a car, you can test it. How does it go in the snow? How does it go in in you know in the rain etc. There's a kind of regime that you can test and it's pretty well known. If you had a calculator we don't really need to do this because we know almost by proof by uh construction. We know that the calculator will be correct. when when the Intel uh chip made an error in floating point numbers, I think this is about 20 years ago, was a huge scandal that it wasn't 100% correct, but it was like 99.9999% correct. Um, but there's only so many things you can ask a calculator to do and so you can basically prove that it is correct. We can't do that with LLMs with large language models because they can be asked to do anything and different people ask them to do different things and so everybody has their own opinion about which model is better like almost certainly on almost everything with some qualification. GPT5 is going to be better than GPT4 and is going to be better than three and it's a question of how much better but it's a hard question to answer. It depends very much on what you're testing it on. So when people say you know Gemini is better than GPT 5.2 or whatever they really mean for the things I do and people do very different things. So some people use these systems for example to help them with coding. Other people use them to help them with brainstorming. Some people may do financial analyses and so forth and so on. There's sort of endless and it makes it very hard to make a definitive statement. Instead you know people kind of try it out and they have the intuition. I actually haven't played that much with Gemini. I played with a lot of these models and kept seeing the same patterns over and over again. And to me, that's boring. Like I come to all of this from a scientific perspective. I want to see something that works differently. And when I see something that works differently, I'm going to spend a lot of time testing it. >> So you you have said that that these models are reaching diminishing returns. So in other words, two was much better than one. Three was much better than two, five was better than four, but less than four was better than three. Like my question to you is first of all like how do you know that like based based on what did are you do you are you feel confident you could say that you can look at benchmarks and say how much better do they do on these standardized measures you can try them out intuitively

the with the earlier models the leaps really were so dramatic you you didn't even need formal measures it was just obvious if you played with the original GPT1 it was terrible like it was like the old joke about that that it can answer your questions at all was kind of impressive, but like you know it was almost always wrong. It was it was useless. GPT2 was legitimately better. You would ask it questions. It would be more grammatical. It wouldn't be completely out to lunch. Like you didn't It's like, you know, you go for an eye exam and they show you the two different lenses and you're like, I'm not really sure. Is this one better? You didn't need that for GPT2 versus GPT1. It was just better. Same thing with GPT3 versus GPT2. almost anything that you would test it on, it would give you a better, fuller, more grammatical answer, etc. Same with GPT4 versus GPT3. You didn't need to get to some, you know, obscure test. You just played with it for half an hour obviously give you a better answer for many things, but it's it's not the same giant leap forward. It's one where you do need to have measures in order to indicate the progress. So let's let's press that for a second because from let's bring it to my side, the business side. I mean these companies, these hyperscalers, Microsoft, they've been buying, you know, GPU chips, like they've been going out of style. And, you know, the amount of money that all these hyperscalers spent in two 2025 just the pure hyperscalers was probably something like $500 billion, more or less, which is not exactly chump change. >> That's right. And no one actually knows because I've asked as many people as I can. You know, how much of all the GPUs that Nvidia is selling are going to companies that are creating LLMs? But it's obviously not a small number. >> Presumably a lot, especially the higherend ones. >> Yes, there's no question it's a lot. Is it 30, 40, 60, 70? No one. There's no there's not I I haven't literally have not even found a number, but it's it's clearly a very high percentage. It's very important. So if if your view were suddenly tomorrow to become the dominant view and you know let's say I'm I'm in Microsoft or I'm at Meta and I'm in the department that's developing this stuff and I and all of a sudden I got converted to the Gary Marcus religion. I would have to say to myself, I gotta take a step back because we've basically gotten to more or less where we're gonna get doing what we've been doing. And you know, Ilas gave us things we need to go back to the drawing board, which we'll talk about what that means. I don't even I'm not sure exactly what that means. And so therefore, this the

the the amount of GPUs people will buy would have to slow dramatically as people took a step back and figure, okay, what do we do now? Where is the community, you know, the entire computer science scientific community, you think, right now in this debate, because you're not the lone wolf anymore, but just you you and your your people who agree with you, you're still in the minority. What's happening? What what are you hearing about what's going on? I'm >> not sure we're in the minority anymore. I think things have changed pretty dramatically in the last six months. So, we were definitely at least in the minority in certain conversations. I think among longtime AI researchers, I was never in the minority. I was in the minority in terms of how the media reported things and I was in the minority with respect to the people running these companies. >> Yes. >> But you know there was a survey done by triple AI which is the American or what is it the association for the advancement of artificial intelligence. It's the arguably the biggest most longstanding AI organization. They did a survey, a presidential survey, I think it was called about a year ago, and 85% of the people in that survey, which were mostly like college professors, industry researchers, but not necessarily the big companies, 85% of them said that large language models are not going to get us to the holy grail of so-called artificial general intelligence. So, I mean, a lot of people have seen the writing on the wall for a while. I've always been the most vocal. I think I've been the least afraid to like, you know, take the flack. And I think I've been articulated about it because I've thought about these issues for a very long time and they were kind of very central to my academic work. But there are lots of other people that have seen I think what I saw. Um what's really remarkable about the last several months and I'll tell you what I think the turning point was is how many other people have started to see it. So you just mentioned Ilaskever. That's really a big >> and just so people know he he was one of the founders of Open AI but he's left and he's starting his own founder of Open AI but he was also a co-author on this paper about using GPUs for for not large language models but their predecessor for G. >> So he was there at the beginning >> he was there at the beginning of what we might call the rebirth a 2012 rebirth that was such a big deal. So for him to give an interview on this podcast by Doresh saying we need to go back to the age of research this isn't really working that is a huge deal. Um now I don't think he said anything I wasn't saying all

along but you know coming from him like it carries a certain weight. Um so there was that but I think the real turning point was actually in August. So if you rewind, you go back and you look at interviews and stuff from 2023, 2024, Sam Alman basically kept hinting that GPT5 was going to be artificial general intelligence or close to it. For example, in January, just a year ago, he wrote in his blog that we now know how to make artificial general intelligence in the conventional sense. And people were for years talking about when is GPT5 gonna drop. It was kind of like waiting for GDAU or something like that. You you're you're kind of my era. Do you remember Airplane? Um, >> of course. >> You remember the last scene where they're landing and it's now arriving gate eight, now arriving gate nine as [laughter] it screams. >> Yes, I remember. I remember. >> So, I wrote in a bad day to quit sniffing glue. >> Bad day to quit sniffing glue. So, I wrote in my Substack about that phenomena as applied to GPT5 because in January of 2024, they were like, I think it's going to drop after the Super Bowl. it's going to be so amazing like the two and then you know then it didn't happen and they're like in July I you know I read on Twitter this guy named Strawberry said it's all going to happen and he's got inside information and like it never came it was always a lie right and so I wrote this thing I think it was actually over well over a year ago about now arriving gate 8 now you know GPT5 now arriving gate 8 now arriving gate 9 and so forth and it just kept getting delayed and pushed back and pushed back but Sam all the while kept implying that it was going to be this amazing thing and in fact when he introduced it which I happen to know exactly what the date was August 7th of 2025 it he finally introduced it there's an aside about an earlier model that called GPT4 and a half that was supposed to be five they didn't even want to call it that cuz it wasn't good enough but finally decided to release something called it that GPT5 and he's like it can do anything a PhD can do but they released it to the public and it took the public like an hour to realize this wasn't true that it was still making these hallucinations and stupid >> same problems. >> Same problems, you know, fewer of them. Like it was it was legit better, but not so much better, right? He sold it as if it was going to be a quantum leap forward the way that four was compared to three and three was compared to two. And it just wasn't that. And the community who had been

hoping it was going to be that was disappointed. And literally that day, people started writing stuff on Twitter that cracked me up. They'd be like, "I hate to say it, but Gary Marcus." >> Gary Marcus was right. I remember I remember I was reading it's like, "Oh, Gary Marcus, not just a lone wolf anymore." >> Exactly. Somebody wrote a book using probably chat GBT called Gary Marcus was right, you know, shortly after these events um took place. Um [snorts] like it became a thing that I was right after all this time. And it's really the disappointment of chat GPT5 that transformed that. But where do you think >> if I don't know if you know you you connected this way have you heard about what's going on at all within the companies now that this sort of tone of this debate is changing. I mean I guess a few things are happening. So one is I have been saying all along you can't use pure large language models. You have to use classical symbolic AI. And they all just scoffed and like that's scary. We don't need this stuff. We don't know how it works in the brain. And now they've all quietly started doing it to some extent. So they use things called code interpreters which run Python code which is classic symbolic tools. Um and so they're sneaking the system 2 stuff inside. They're not making a big deal of it but it's actually making a big difference. This was really clear at the presentation of Grock 4 um where Elon Musk showed a graph and I wrote about this in a essay called how 03 and Gro 4 accidentally vindicated neuros symbolic AI. I have this graph there and what it shows is really the addition of symbolic tools that they're not really talking about that's making the systems work better. You know they're still improving a bit and that's where they're getting most of the improvement. So part of it is they've quietly stopped doing just LLMs. But for your world that's significant because those symbolic things are not running on the GPUs, they're running on CPUs. >> Ah okay. >> And I mean for me for the technical issues it's vindicating of the approach that I was talking about. So that's one thing that's happening. Another thing that's happening is everybody's leaving to start their own startup. Now think about it. If you were at OpenAI and they really were going to release AGI next week, would you want to leave just before this cataclysmic event that was going to revolutionize the world and go start a little company that's going to take, you know, four years to develop or whatever? Of course, you wouldn't. You'd want to be there for the magical coordination. So, the fact that everybody is leaving tells you that they don't really have what they think. And then, um, what else is going on? you

know, Google is catching up because as I projected in my Substack a couple years ago because everybody's doing mostly the same thing. There's no moat here. There's no technical mode, >> right? All I think what you are you and some other people have argued is that if if all you're doing is constantly scaling an LLM, the winner is the person who can afford to scale the LLM. And who who can afford more than Google? Nobody. >> Yeah. I mean, I made a slightly different version of that argument, though that version is probably correct, which was that you were going to have a pile up near the top and that you're going to have a price war um as these things became commodities, prices would drop. And in fact, prices have dropped by 100, a factor of 100 um and how much you pay for a token. So, we have had that price score. I think it is also true that in the end that favors Google. I didn't really formulate the argument that way originally, but when I started writing about it, I think it was March of 24, maybe it was August of 23, I said, "Look, everybody's following the same recipe here. Nobody has some secret sauce." >> And so that means you're going to have a convergence at the top. >> It's a commodity at the top >> where everybody's going to make models that are a little bit better than last year's, but not that much better. And so it's not going to matter anymore. And what is what has that meant? It's meant Google has caught up, and it's meant that China has caught up, and it's meant that Anthropic has caught up. And so that has also driven, as you say, it's a commodity that has driven the prices down. Um, which is good for the end consumer, but not good for the business model where I'm going to pay you a ton of money for GPUs cuz I'm going to make a ton of money. >> Can we move on to inference models for a second? Could you just explain to my audience what's the difference between an inference model versus an LLM model? And are inference models based on LLM models? Inference models work over LLMs, but instead of sort of spitting out the first answer, they iterate. They take time to try to come up with the best answer. People have not been super um forthcoming about exactly how they're doing it. But the original idea of most of these neural networks was kind of oneshot in a certain sense although people use that term in different way in in the sense that you would feed something in it would immediately do a pass through the neural network which means that every node in the neural network roughly speaking um I'm simplifying a little bit would see something and generate its answer and now what they

did is say let's do multiple passes which is really very different right the first thing is what a friend of mine called constant time inference. You know how long it's going to take to compute your answer is everything takes basically the same time. You feed it through your pattern recognizer. It gives you the best answer based on the pattern in the scene before. This new paradigm inference time which I'll talk talk about where it's worked and where it has not means that you spend longer on some problems than others. Now, nobody's figured out entirely how to make that all work well, but there are some cases where it works pretty well. As far as I can tell, part of what they're doing is they're getting humans to talk through problems and it's trying to mimic that, right? These are mimicry systems. Now, what you're doing is trying to mimic what a human would do when they solve this geometry problem or this algebra problem or something like that. It's trying to mimic that process. And that process for human takes a bunch of steps and it for the neural network um that in includes this stuff it takes a bunch of steps. >> So what is this good for? What is this turned out to be not so good for? >> So before I answer that question I'll insert one other thing. It inherently is more expensive. So you're using more time on your GPUs to generate >> your answer each answer. >> Um okay so what is it good for and not good for? What it seems to be best for are places where you can generate formally correct verifiable data to train your model on. So good example of this is math or computer programming where you can write a program to generate different possible pieces of code to teach your system or different possible geometry proofs and things like that. Um and you can do that in those domains because they're kind of closed domains. There's limits on >> mean limits of how much knowledge is in the database. >> Exactly. How much and how much knowledge is relevant. >> Right. >> Right. Um and that is what they've been best at is things like geometry and things like computer coding. What they've not been good at is the open-ended real world. I always think from your world, although you'd know the example better than I do, of when long-term capital uh management went bust because nobody had really thought it was a different kind of model, but nobody had thought about what happens if the Russian bond uh uh market you know does something >> and so like you know there was a major collapse in in US financial markets because of what happened in the Russian bond market. Um, so that was outside of how the parameters of how those models which worked on different principles were built. We're seeing

something similar here, which is like the system doesn't actually know how to think about bonds or anything else. And if what it's trained on is or sorry, what it's tested on or used for is really similar to what it's been trained on, everything's hunky dory. But if you push it outside its comfort zone, and we talked about the Tesla um example as well, then they tend to break down. >> So it has a problem with novelty. And if something's not in the database, >> that is really the core problem even with the new inference models. It's sort of a slightly different variation, but it's still fundamentally an issue of novelty. And here's the thing, most things that are interesting in the world involve some measure of novelty. Not all of them. And it turns out that you can use techniques that are not all that great at novelty to do something in a narrow domain like chess or go where the rules haven't varied significantly. There's some variation um in the last thousand or you know 2,000 years. So you have a lot of data. You can do selfplay generate more data for yourself and so forth. But when you're talking about the open-ended real world, if you're talking about politics or military strategy, um, or, you know, there's always going to be stuff that isn't in there, like how do you deal with the notion that you have a president who seems to be willing to have um planes dressed up as civilian planes um, you know, attack people in in another country. Like that hasn't happened before. How do you reason about that? Well, you can't just look at the past data anymore. you have to start to think about concepts about like I don't know power and diplomacy and and and how things are constructed that you know other kinds of scholars would be better at than me but um you know you have to have the right training and the right abstractions and so forth you can't just rely on the data anymore and what has happened in business use I think is even when you're doing something like customer service that seems relatively easily easy to handle people are always asking some question a little bit differently than before and that's when the systems tend to break down in the face of novelty. >> So let's say I appointed you AI ZAR and you were in control of all of these companies and you you're going to direct them. If you were to sit all these people in a room, what would you tell them that they need to do to go to the next level? Really go to the next level. >> I would say that they need much more intellectual diversity. So in your world you tell people don't put all your money in one stock right diversify you know stocks and bonds and gold and real estate

right um basically the field went all in on one idea for the last >> scale the LLM >> scaling the LLM was the one idea and you know they got some mileage out of it's not useless there are things we can actually use them for but it did not bring us to the holy grail of artificial general intelligence and it turned out also be a very expensive and very inefficient approach. If you compare like how much data my children have needed to understand the world versus needing the entire internet like it's sort of comical how inefficient these systems are. So they made these inefficient in unreliable systems that have some use for a lot of money. We need to have other approaches that are more efficient that are more economical that are more reliable and they should be putting money into trying to think about that. But the problem is partly comes from your world is venture capitalists get you know 2% of investing on schemes that sound plausible right so if you're a venture capitalist and I'm curious your take on my take um because it's more your world than mine if you're a venture capitalist and you have a story about how I can invest your trillion dollars and I get my 2% I don't have to care that much how it's all going to play out because 2% of a trillion dollars would make me a very rich person. And so I'm not saying every venture capitalist thinks that way. I think, you know, I've met a lot and some of them I think really do want to advance technology and so forth, but a lot of them are cynical. Um, like you might find in any profession. And what pays if you are a cynical venture capitalist is to find an idea that sounds plausible, doesn't have to work, that is very expensive, and you can take your 2% cut on. And I think that that's what's happened is people have gone all in on the scaling because they get a cut and the cut is pretty big. But it has not been the intellectually right thing to do and it has not led to good results which means a lot of money has been squandered. The VCs got their 2% cut. The limited partners are going to lose a lot of money in the end I think. >> Do you think we're close to that happening or it's impossible to tell? >> I mean you know the saying as well as I do about shorting, right? the market can remain irrational longer than you can remain solvent. Correct. But the metaphor that I was using last year was wy coyote at the end of a cliff, right? In in Bugs Bunny, right? He doesn't fall down until he looks down, which is of course not how physics actually works, but makes it funny. >> I think that people have started

to look down in your world. In your world, in the investment community, people are I think this really began in November, circular financing. The results on return on investment are not that great. The systems still are not really working that well for me. Maybe this doesn't make sense." Right? I think personally that Nvidia makes a fantastic product. Their ecosystem is amazing. >> The chip is and it's not even just the chip. It's the software around it. has everything. >> I've met Jensen. I'm impressed by him. It's a great product. But the question is, how many of them are they going to sell at the end of the day? And I think that they've been selling them all on speculation. People are speculating. And I'll come back to how other people think in a second. People are speculating that there's going to be essentially infinite demand for this. But that speculation is based on the notion that these things are actually going to give us artificial general intelligence. real artificial general intelligence that could do anything a human could do would be worth a tremendous amount of money, certainly trillions of dollars every year. Um, but like there's another study that was just reported in the Washington Post a couple days ago, was done a month earlier, um, showing that only two and a half% of the jobs that people do with the AI systems actually able to do, you know, so most things that people are fantasizing that AI is going to do for them, >> they don't do, >> they don't really do. And that means at the end of the day that all these investments in chips don't make sense. It means at some point like I think Open AI is probably the most vulnerable company here. Open AI has you know trillion plus dollars in outstanding commitments has never made a profit. There's now in a commodity market. their biggest competitor, Google, caught up. And not only they caught up, but yesterday >> um got the Apple deal, which is a really big deal that you know, >> and so like I think that they're flailing. I don't see how they're going to warrant their valuation. >> Well, if Open AI, I mean, for my world, if Open AI, if people started to pull money from Open AI and they couldn't raise money, terms of the entire eos. I think in the end open AI maybe gets absorbed by Microsoft or something like that. I've been saying for a couple years I think they're going to be viewed as the wei work of AI and people are going to be like how did they get valued at that? Like it just didn't make sense, right? Like OpenAI's revenue is a few billion dollars and but they're losing billions of dollars every month. They have all these competitors. Like it just doesn't make sense. And if people pull out of open AI or

just don't put more money in, then they're in deep trouble, right? So they're losing whatever it is, $3 billion a month, you know, 30 some billion dollars a year, something like that. Right. And they just got a $40 billion funding round. That gives them like a year's runway, right? >> But that's it. And a lot of people are probably standing on the sidelines saying, "Hey, wait. Google's better suited to play this game. They've actually caught up. If it's only scale, then Google wins. If it's only scale, Google wins. Hands down. How could they not? >> Google can make the commitments. Google doesn't even need Nvidia because they making their own tensor pro processor units that do kind of the same work. And so like they're less vulnerable. They have a, you know, financial pipeline. They're going to win. Right. >> Right. And it doesn't take that many people to think that. OpenAI is in a position where the check size that they need to stay afloat is just massive, >> right? They're next going to need like a hundred billion dollar check. Not many people can write you a hundred billion dollar check. You know, five people in the world maybe can do that. And if four of them say no, you're okay. But the minute five of them say no, you're out of business or you're going to Microsoft saying, "What can you give us?" >> Right. So Gary, before we go, is there anything I should have asked you that I haven't asked you? >> I thought this was a great interview. Um I don't know. I mean, um I guess there's one thing we didn't talk about which is important. Um which is this notion of world model. >> Yes, I I did want to talk about it. You you have discussed that we need world models that is so outside like you know my area of expertise. Why why don't you just explain to what exactly you're talking about? So, so a world model is basically something inside a computer. I mean, different people have definitions, but I'll tell you what I think we need. You need something inside your software that represents the things outside in the world. So, if you're a GPS navigation system, you need, for example, to be able to represent what the roads are and how they connect, how long does traffic take on them. In classical artificial intelligence, world models are the starting point. Everybody has them. No, nobody would even think about not using them. Herb Simon wrote a who was one of the founders of artificial intelligence in the 1950s wrote a biography that I think was called models of my mind. He thought about models, world models all the time. And he realized that the right way to do AI was to have the right world model. Large language models try to do without that. You know, it's a lot of work

to build a model of a particular thing, especially a complicated thing. Like when people built expert systems, they would build a model of how a doctor thought about things, a model of how the patient worked, what the physiology was. Like it's a pain to do that. There's a whole field called knowledge engineering. It was expensive. Nobody wanted to do it. And so language models and and other kinds of neural networks came in and said, "You don't need to do that. We're just going to let everything percolate up from the data." That doesn't really work. So when the large language model tells you that Harry Shear was born in London when he was actually born in Los Angeles, it's because it doesn't have a proper world model, it can't just look that up like a properly designed piece of software would. And so we need to have world models fit in here somewhere to avoid the hallucinations. >> I still don't understand what a world model is. So a world model is it's hard to do this in non-technical terms is a representation >> of the world >> of the world and it the it doesn't even have to be the real world. So for example we have world models of um fictional domains like the Star Trek universe or the Star Wars or the Harry Potter universe, right? So when you go to a movie and this is the amazing thing to me about humans compared to current uh systems. When you go to a movie, you read a book, you build a model of how that world works in your head and you can ask questions like was that plausible in this real in this world or were the movies makers doing a cheat or you know if you watch Harry Potter then you understand that people can fly around in broomsticks but you don't confuse that with your world and you know jump on a broomstick and when you get home the window >> actually going to fly right >> so we actually maintain many of these and we build them very quickly. You watch an episode of a new science fiction show and you know 20 minutes in you start to understand a different world. So we're really good at this. In AI neither approach is that good at it. So classical AI is really good at doing it by hand. So you can pay a bunch of philosophers to spend six weeks and describe your problem. Um a a great researcher who passed away recently, Doug Lennett, did this with Romeo and Juliet. He showed that his system could you could handwire Romeo and Juliet and then it could understand you know key plot points and was very impressive and it could do that without you know reading secondhand reports from cliff notes that were in the internet it could just actually understand it was amazing but we don't know how

to get classical AI to acquire these models and large language models don't really do it at all they fake it my favorite example of this is if you well not if you one trains them on the whole internet so they get access to a lot of written rules of chess and lots of games of chess and they still make illegal moves. They never really abstract the model of how chess works. That's just so damning. You would not be able to learn chess after seeing a million games, reading the rules in Wikipedia and chess.com, reading books like wiki uh like um Bobby Fischer plays chess and all this stuff and he just never gets it right. So we need systems that can induce is the word I would use world models that can look at a set of data try to understand the causal principles the entities that are involved. This is a hard question. It's not a like oh somebody's going to go home tomorrow and fix it question. It's a question that the both approaches have avoided for a long time and that we need to face now. >> Well that's going to take time. >> It's going to take time. Going back to your world, I think the thing is artificial intelligence really is going to transform the world like in ways we can't even imagine. But it's not going to do it now. Not with this technology. Okay? >> And we need to like factor that in and make investments. Are we making research investments? Are we making scaling of investments [clears throat] on technology that already works? >> No. What we're doing mostly is speculating that something that doesn't work is going to magically work >> if we make it bigger. just making it bigger is not going to solve these problems. We need to do foundational research. That's what I was saying for the last 5 years and that's what Sudskver said in November. And when Skever says that too when we kind of come from opposite ends and converge on that, people should actually listen. >> Gary, that was wonderful. Thank you. It was incredibly educational and I really appreciate you coming on my show. >> Thanks a lot for having me. >> Thank you. [music] This podcast is forformational purposes only and does not constitute investment advice. The hosts and guests [music] may hold positions in stocks discussed. Opinions expressed on their own and not recommendations. Please do your own due diligence and consult a licensed financial advisor before [music] making any investment decisions. Let's start with a company that I think is a classic short, Stalantis. So, you recommended shorting it in June of 2024. The stock was 20 and as of I think yesterday was $9.40. So, that's a good short. Who is Stellantis and why was it a short? >> They bought American cars. They bought >> Dodge Ram. >> So, why was this company

in trouble? It goes back to 2020. COVID started and everything was shut down. So the government started giving stimulus checks to all consumers, everyone. And that started a whole batch of problems. When the stimulus checks went to consumers, some of them started to pay down the debt, >> their own debt, >> their own debt, right? >> And some of them started to buy cars and >> bought a car. So when the demand was like that and what we heard from our sources is that the quality wasn't that good >> because they were producing too many. >> So not good enough quality control. >> Not enough quality control. Everybody started to buy cars and people who paid down their personal debt, their credit scores started to go up. Now you have subprime borrowers became prime borrowers >> because the government gave them a check to pay off their debt. So all of a sudden there are more consumers who can afford to buy cars because they're going to get a better loan. >> Okay. >> And the car companies and dealerships got very greedy. >> Really? >> I'm shocked. [laughter] >> The problem started to happen. All the subsidy money stopped, >> right? >> And the reality started to kick in and that's when we realized, Steve, this car industry is such a an organized mess. They are all in it together. You don't think the Fed is like the big decision? >> I think this whole thing is laughable. I mean, when you think about it, the only time the Fed really matters is when it's being very aggressive at cutting rates or very aggressive at raising rates. If it's just tinkering around the edges, which is what it's clearly doing, it's irrelevant. >> Although Leeman made the argument this morning that what they're doing with the balance sheet might be a little more important. it on the margin, but you know, is is what they're doing right now going to make you buy Palunteer or not buy Palunteer? It has no impact. How far do you think we are away [music] from Steve Eisman saying, "I'm going to buy an EV cuz this has so many more features than I have than I could get with just buying a regular combustion engine car." >> So, Tesla is leading that effort right now in the industry though to [music] your point that there are other automakers. You know, GM has talked a lot about uh personal AVs. They made a substantial hire not too long ago. So, it is being pursued by others, but Tesla seems to be in a multi-year lead. I would say if the next 12 months, maybe 15 months, [music] there's nothing on this front from Tesla, that would be disappointing to to our to us and our thesis [music] around what they can do. But the rest of the industry is probably still a few years away. Maybe it's 3

4 years before this becomes a bit more of a reality more broadly.