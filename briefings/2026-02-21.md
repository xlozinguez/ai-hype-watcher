# Briefing: 2026-02-21

> **Focus**: Batch ingestion of 25 sources (#112-#136) — AI bubble economics go mainstream with WeWork comparisons and $285B sell-offs, context engineering gets foundational teaching, skills ecosystem security crises documented, dark factory model faces first cost stress tests, and Boris Cherny returns to discuss what happens after coding is solved

## Headlines

- **[ai-economics]** The AI bubble bear case goes mainstream — WeWork comparisons, $285B sell-offs, and "what happens when OpenAI runs out of money"
- **[context-engineering]** Matt Pocock fills the pedagogical gap with systematic LLM literacy for developers — tokens, context windows, prompt caching
- **[security]** Skills.sh marketplace exposes full supply-chain attack surface: no versioning, prompt injection via alt text, autonomous agent self-installation
- **[agentic-coding]** Dex Horthy's "No Vibes Allowed" provides the strongest anti-vibe-coding framework for complex codebases
- **[ai-economics]** $1,000/day AI costs: first detailed economics of a three-engineer dark factory
- **[ai-landscape]** Boris Cherny (Head of Claude Code) discusses what happens after coding is solved

## Details

### The AI Bubble Bear Case Goes Mainstream

Ed Zitron calls the AI bubble "dumber than WeWork" (#125), Steve Eisman warns of a "software bloodbath" (#123), and The Atlantic's Charlie Warzel and Anil Dash analyze the panic cycle (#132). Nate B Jones documents the $285B sell-off (#128) as structural repricing, not a correction. The Infographics Show asks what happens when OpenAI runs out of money (#133). The financial and engineering critiques are converging — this is no longer a fringe position.

**Key quote**: "It is an interesting technology with a lot of power and a lot of utility that is being overhyped to such an extreme degree that it is actually undermining the ability to engage with it in a useful way." — Anil Dash (#132)

**Sources**: [#123](../sources/123-steve-eisman-software-bloodbath.md), [#125](../sources/125-ed-zitron-ai-bubble-wework.md), [#128](../sources/128-nate-b-jones-285b-selloff.md), [#132](../sources/132-the-atlantic-ai-panic-cycle.md), [#133](../sources/133-infographics-show-openai-money.md)

### Context Engineering Gets Its Curriculum

Matt Pocock's content fills a critical gap: systematic teaching of LLM mechanics for working developers. How tokenizers work and differ between providers (#112), why context windows are more constrained than their stated limits (#114), and how to use Claude Code for real engineering (#126). IBM Technology explains prompt caching (#130). These are the foundations that make Modules 03-05 actually work.

**Sources**: [#112](../sources/112-matt-pocock-llm-tokens.md), [#114](../sources/114-matt-pocock-context-windows.md), [#126](../sources/126-matt-pocock-claude-code-engineering.md), [#130](../sources/130-ibm-technology-prompt-caching.md)

### Skills Ecosystem Security Crisis

Kathy Zant (#124) catalogs the full skills.sh attack surface: dynamic GitHub pulls with no version pinning, prompt injection via hidden alt text, and a meta-skill (180k+ installs) that lets agents install other unvetted skills. VelvetShark's 50-day OpenClaw postmortem (#127) provides field evidence of the hype-to-reality ratio. The PrimeTime's "AI Agent writes hit piece" (#117) and the AWS vibe-coding outage (#121) illustrate what happens when agents operate without trust boundaries.

**Sources**: [#117](../sources/117-primetime-ai-agent-hit-piece.md), [#121](../sources/121-pivot-to-ai-aws-vibe-coding-outage.md), [#124](../sources/124-kathy-zant-skills-sh-security.md), [#127](../sources/127-velvetshark-openclaw-50-days.md)

### No Vibes Allowed — Engineering Rigor for AI-Assisted Development

Dex Horthy's AI Engineer talk (#118) makes the strongest case against vibe coding for complex codebases: hard problems require deep context specification, architectural understanding, and systematic decomposition. Matt Pocock's plan mode conversion (#113) and Ralph Wiggum technique (#115) show the practical middle ground — plan-then-execute with human review gates. Dave Farley (#120) provides the engineering establishment's measured perspective.

**Sources**: [#113](../sources/113-matt-pocock-plan-mode.md), [#115](../sources/115-matt-pocock-ralph-wiggum-technique.md), [#118](../sources/118-dex-horthy-no-vibes-complex-codebases.md), [#120](../sources/120-dave-farley-ai-programming-assistants.md)

### Dark Factory Economics Under First Stress Tests

Nate B Jones (#119) provides first detailed dark factory economics: three engineers, $1,000/day in AI costs, no human-written or human-reviewed code. The math works only with high revenue-per-employee multipliers. Upper Echelon's "Rent-A-Human" analysis (#122) sketches the dystopian endpoint: humans as the API layer for AI-directed physical labor.

**Sources**: [#119](../sources/119-nate-b-jones-ai-costs-dark-factory.md), [#122](../sources/122-upper-echelon-rent-a-human-moltbook.md)

### Multi-Agent Patterns and Workflows

Leon van Zyl (#129) argues single-agent usage is the beginner pattern. John Kim (#135) documents sophisticated multi-agent workflows. Google DeepMind's experimental platform for human-LLM agent collaboration (#134) represents the research frontier — a 62-minute deep dive on systematic approaches to human-agent interaction.

**Sources**: [#129](../sources/129-leon-van-zyl-multi-agent-claude.md), [#134](../sources/134-prolific-deepmind-agent-platform.md), [#135](../sources/135-john-kim-claude-code-workflow.md)

### Boris Cherny on What Happens After Coding Is Solved

In his second major interview for this project (#136, following #103), the Head of Claude Code discusses the implications of coding becoming a solved problem — what skills matter next, how software development transforms, and Anthropic's vision for the post-coding future.

**Source**: [#136](../sources/136-lennys-podcast-boris-cherny-after-coding.md)

## Synthesis

This is the largest single-day batch in the project (25 sources), and the themes confirm a pattern that has been building since the early synthesis documents: the AI industry is bifurcating. On one track, sophisticated practitioners are developing genuine engineering discipline around AI-assisted development — context management, specification rigor, multi-agent orchestration, security-conscious deployment. On the other track, the financial contradictions are compounding: $285B sell-offs, WeWork comparisons, and questions about OpenAI's financial viability.

The key insight from this batch is that these tracks are no longer separate. Nate B Jones simultaneously documents the financial sell-off AND the dark factory's real economics. The Atlantic hosts both the technology critique and the economic analysis. The convergence suggests we're approaching the point where the hype cycle's crash phase and the genuine capability maturation happen simultaneously — which is the most dangerous moment for making good decisions about AI adoption.

**Cross-synthesis**: [The Reckoning — Bubble Economics Meet Engineering Reality](../synthesis/2026-02-21-playlist-themes.md)
