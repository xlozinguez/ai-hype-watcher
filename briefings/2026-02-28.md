# Briefing: 2026-02-28

> **Focus**: 15-source playlist batch — OpenClaw chaos escalates, Acemoglu says AI isn't working, Anthropic banned by DoD, agent orchestration gets a Stanford course

## Headlines

- **[security]** OpenClaw cascading failures: 40K exposed accounts, inbox deletion, 221K GitHub stars amid chaos
- **[economics]** Nobel laureate Daron Acemoglu argues AI's current direction serves capital, not workers
- **[critical]** Anthropic designated "supply chain risk" by Department of Defense over ethical AI limits
- **[patterns]** Agent orchestration taught at Stanford, practiced at Keyhole Software, formalized by Dylan Davis
- **[tools]** Claude Cowork gets scheduled tasks; Obsidian + Claude Code becomes a personal thinking partner

## Details

### OpenClaw's Week of Chaos

The OpenClaw saga intensifies. PrimeTime's standup panel (#176) catalogs the damage: Meta's AI safety lead lost her entire inbox, 40,000 user accounts were exposed with admin privileges, and the platform surpassed Linux in GitHub stars (221K vs 218K) despite these failures. Agentic Lab (#179) explains why — OpenClaw's architecture gives agents broad trigger-context-tools-output capabilities without proportional guardrails. Steve Sims (#172) adds that AI agents now autonomously discover real vulnerabilities, with Google's Big Sleep finding its first zero-day without human involvement.

**Sources**: [#172](../sources/172-soft-white-underbelly-ai-hacking-security.md), [#176](../sources/176-primetime-openclaw-assistant-chaos.md), [#179](../sources/179-agentic-lab-openclaw-architecture.md)

### Acemoglu's Productivity Reality Check

Nobel Prize-winning economist Daron Acemoglu (#180) delivers the most authoritative critique of AI economics to date: AI is structurally misaligned with human welfare, favoring automation over augmentation and capital owners over workers. Vinh Nguyen (#175) provides micro-level evidence — an Anthropic paper shows AI-assisted learning drops test scores 17%. Nate B Jones (#185) argues the solution is the same as the 1970s calculator moment: build the foundation first, then give the tool.

**Sources**: [#180](../sources/180-acemoglu-ai-productivity-critique.md), [#175](../sources/175-vinh-nguyen-ai-skill-tax.md), [#185](../sources/185-nate-b-jones-ai-education-parenting.md)

### Anthropic Banned by the Pentagon

Defense Secretary Pete Hegseth designated Anthropic as a "supply chain risk" to national security on February 24, 2026. The dispute centers on who draws ethical lines around military AI — the company that built it or the government that wants it. Anthropic's AUP explicitly prohibits weapons development and surveillance without consent. The standoff sets precedent for every AI company's relationship with government contracts.

**Sources**: [#184](../sources/184-caleb-writes-code-anthropic-dod-ban.md)

### Agent Orchestration Gets a Playbook

Mihail Eric (#178) teaches Stanford's first AI-across-the-SDLC course, arguing agent orchestration is fundamentally a management skill. Dylan Davis (#177) provides a three-level framework: Do → Make → Know. Keyhole Software (#186) demonstrates the full lifecycle — specification-first prompting through parallel agents — building a trading signal generator from scratch. Leslie Lamport (#181) supplies the theoretical foundation: 50 years of distributed systems prove that specification-first thinking produces correct systems.

**Sources**: [#177](../sources/177-dylan-davis-claude-cowork-system.md), [#178](../sources/178-eo-multi-agent-orchestration.md), [#181](../sources/181-lamport-distributed-systems-thinking.md), [#186](../sources/186-keyhole-software-claude-code-delivery.md)

### Memory Architecture and Personal AI

Damian Galarza (#182) breaks down agent memory into session vs long-term layers with episodic/semantic/procedural types. Greg Isenberg (#174) shows Obsidian as the richest context source for Claude Code — bidirectional linking creates navigable knowledge graphs. Eliot Prince (#183) reveals Claude Cowork's new scheduled tasks and consolidated skills management. Palisade Research (#173) provides the epistemological backdrop: nobody fully understands how these systems work.

**Sources**: [#173](../sources/173-palisade-ai-risk-understanding.md), [#174](../sources/174-greg-isenberg-obsidian-claude-code.md), [#182](../sources/182-damian-galarza-agent-memory.md), [#183](../sources/183-eliot-prince-cowork-scheduled-tasks.md)

## Connections to Existing Sources

- OpenClaw chaos extends [#163: OpenClaw Inbox Deletion](../sources/163-primetime-openclaw-inbox.md) and [#162: Steinberger on OpenClaw](../sources/162-openai-openclaw-steinberger.md)
- Acemoglu joins [#156: Ezra Klein on AI Agents](../sources/156-ezra-klein-ai-agents-economy.md) and [#050: Sam Harris Economic Alarm](../sources/050-sam-harris-ai-economy-emergency.md) in the mainstream economics critique
- Vinh Nguyen's skill tax connects to [#141: Harvard AI Education Data](../sources/141-harvard-ai-education.md)
- Agent orchestration builds on [#010: IndyDevDan Multi-Agent](../sources/010-indydevdan-multi-agent-orchestration.md) and [#108: Five Levels of AI Coding](../sources/108-nate-b-jones-five-levels-ai-coding.md)
- Anthropic-DoD standoff follows from [#002: Anthropic CEO Philosophy](../sources/002-nate-b-jones-anthropic-ceo-philosophy.md)
- Memory architecture extends [#011: Context Engineering Origin](../sources/011-confluent-developer-context-engineering.md)

## Action Items

- [ ] Track Anthropic-DoD standoff resolution — this will set governance precedent for the entire industry
- [ ] Monitor OpenClaw security incident response — patterns of failure here predict patterns across all agentic tools
- [ ] Acemoglu's "Power and Progress" framework may warrant its own curriculum section in Module 06
- [ ] Lamport's specification-first philosophy connects deeply to Module 02 — consider dedicated section
