# Briefing: 2026-02-14

> **Focus**: The measurement gap — multi-agent hype meets empirical reality, AI safety fractures deepen, the economic displacement timeline compresses from theoretical to urgent, and Amodei bets the company on the end of the exponential

## Headlines

- **[critical]** Cursor's $8-16M browser stunt fails to compile — removing quality gates to boost throughput is the oldest engineering antipattern, now running at agent scale
- **[economics]** Sam Harris frames AI success itself as an emergency: even the optimistic scenario concentrates wealth into a "fall of Saigon" scramble for the last available jobs
- **[security]** AI browsers face an inescapable paradox — sandboxed enough to be safe means no advantage over a website; powerful enough to be useful means prompt injection owns your bank account
- **[patterns]** Brainqub3 ships an open-source measurement rig proving multi-agent systems collapse under coordination costs when single agents already perform well
- **[critical]** Anthropic safety lead resigns warning "the world is in peril" — Novara Media connects the safety-economy threads into a polycrisis framework
- **[tools]** WebMCP lands in Chrome preview: websites can now expose structured tools directly to AI agents, replacing thousands of screenshot-and-click interactions with single function calls
- **[critical]** Amodei tells Dwarkesh Patel we are "near the end of the exponential" — 90% confidence on country-of-geniuses-in-a-datacenter within 10 years, 50/50 on 1-2 years, while the METR study shows a 20% productivity *downlift* for experienced developers
- **[economics]** Griffonomics maps the SaaSpocalypse into the credit markets — PE-loaded SaaS companies now represent 31% of distressed syndicated loans, with PIK at 11.3%, creating contagion risk into pension funds and insurance companies
- **[patterns]** Google DeepMind's Gemini 3 Deep Think achieves 100x compute reduction for Olympiad-level performance in six months — inference-time scaling replaces "bigger models" as the dominant paradigm, and agentic harnesses outperform raw models at every compute scale
- **[security]** IBM maps zero trust principles onto agentic AI — six attack vectors, just-in-time credentials, tool registries, and AI firewalls form the first structured enterprise security blueprint for agent deployments

## Details

### Cursor's Browser: $16 Million Worth of Code That Does Not Compile

Java Brains ([054](../sources/054-java-brains-cursor-browser-hype.md)) delivers the most thorough debunking of multi-agent hype this week. Cursor claimed hundreds of GPT-5.2-powered agents built a web browser "from scratch" in one week. The reality: the code has 32+ build errors, an 88% CI failure rate, relies on Mozilla's Servo engine libraries for core functionality, and when manually coaxed into building, basic interactions like clicking do not work. A Servo maintainer since 2016 described it as "a tangle of spaghetti, a uniquely bad design that could never support anything resembling a real-world web engine." The estimated cost: $8-16 million in API tokens. The most architecturally damning detail is buried in Cursor's own blog post — they removed the "integrator" agent responsible for quality control because it was slowing throughput. This is the same antipattern that plagues human teams optimizing for velocity metrics by cutting code review. Cursor's own Solid-to-React migration, a bounded and verifiable task, actually succeeded — but that story does not go viral on Twitter.

**Source**: [054: The Cursor Situation](../sources/054-java-brains-cursor-browser-hype.md)

### Success Is the Emergency: Sam Harris on the AI Economy

Sam Harris ([050](../sources/050-sam-harris-ai-economy-emergency.md)) reacts to Mustafa Suleyman's prediction that "most if not all professional tasks" will be fully automated within 12-18 months. Rather than debating the timeline, Harris works through the consequences if it holds. The sharpest insight is the white-collar inversion: AI threatens lawyers, accountants, and software engineers before it can replace plumbers and nurses, inverting the historical relationship between education investment and job security. Harris frames the transition as a "fall of Saigon" moment — a narrow window where the last people to grab the helicopter of employment get out. Even the best-case scenario (genuine abundance) is itself an emergency because concentrating the output of thousands of workers into one person wielding AI agents creates an unprecedented wealth distribution crisis. The entry-level hiring freeze is already here — every manager is asking "is this even a job?" before opening a requisition.

**Source**: [050: We're Not Ready for What AI Is About to Do to the Economy](../sources/050-sam-harris-ai-economy-emergency.md)

### Measuring Multi-Agent Systems Before They Fail in Production

Brainqub3 ([055](../sources/055-brainqub3-multi-agent-measurement.md)) fills a critical gap in the multi-agent conversation: empirical measurement. Brain Cube Agent Labs is an open-source rig that pairs every multi-agent run with a single-agent baseline on the same task, model, and tool configuration. Built on a Google Research paper achieving R-squared 0.52 on coordination metrics, the tool measures efficiency, overhead, error amplification, redundancy, and message density — then extrapolates how those dynamics shift as agent count scales. The key finding: when single-agent performance is already strong, adding agents yields diminishing returns and eventually coordination collapse. More tools amplify this collapse. The practical heuristic is simple: measure your single-agent baseline first and only reach for multi-agent when the single agent genuinely struggles. This is the empirical counterweight to the vibes-based multi-agent enthusiasm that has dominated the conversation.

**Source**: [055: I Built an Open-Source Rig That Measures Multi-Agent Architectures](../sources/055-brainqub3-multi-agent-measurement.md)

### The Anthropic Safety Fracture

Novara Media ([052](../sources/052-novara-media-anthropic-safety-crisis.md)) examines Mrinank Sharma's resignation from Anthropic's Safeguards Research Team — the team responsible for jailbreak robustness, automated red-teaming, and misuse monitoring. Sharma's public letter warned "the world is in peril" and that employees "constantly face pressures to set aside what matters most." The segment's strongest contribution is connecting safety and economic threads into a polycrisis framework: if AI systems with documented alignment failures are simultaneously replacing the professional services backbone of Western economies, the compound risk is far greater than either threat alone. Walker highlights that Eric Schmidt's own red lines — models reasoning in incomprehensible languages, recursive self-improvement, weapons integration — are arguably already crossed. The geopolitical framing is blunt: the Manhattan Project proceeded despite known risks because of wartime imperatives, and the AI race operates under identical logic.

**Source**: [052: Claude Isn't Safe. This Anthropic Whistleblower Has the Proof.](../sources/052-novara-media-anthropic-safety-crisis.md)

### AI Browser Security: The Paradox Has No Middle Ground

TheStandupPod ([047](../sources/047-standuppod-ai-browser-security.md)) dissects OpenAI's Chromium-based AI browser and identifies an inescapable dilemma. Sandboxed enough to be safe, it offers nothing the ChatGPT website does not already provide. Given enough access to be useful, it becomes a prompt injection attack surface for every authenticated session. The Brave browser team demonstrated hidden text in images tricking an AI browser into unauthorized banking transactions. A panelist showed a PDF with hidden text causing ChatGPT to spin up a rogue Python server. The punchline: prompt injection is currently "intractable" with no known solution, and the ship-to-Mac-only launch undercuts OpenAI's own narrative about AI as a productivity force multiplier for engineering.

**Source**: [047: OpenAI's AI Browser Is a Security Nightmare](../sources/047-standuppod-ai-browser-security.md)

### Amodei Bets the Company: Near the End of the Exponential

Dwarkesh Patel ([056](../sources/056-dwarkesh-patel-dario-amodei-interview.md)) conducts a two-hour interview with Anthropic CEO Dario Amodei that is the most transparent public accounting of frontier AI economics to date. Amodei places 90% confidence on "country of geniuses in a data center" within 10 years and 50/50 on 1-2 years. The financial model is brutally simple: Anthropic is scaling revenue 10x per year ($100M in 2023, ~$10B in 2025), but compute must be purchased 1-2 years ahead of demand. Being off by one year at a 10x growth rate is "ruinous." The most important unresolved tension: Dwarkesh cites the METR study showing a 20% productivity *downlift* for experienced developers using AI, while Amodei insists the gains within Anthropic are "unambiguous." Amodei carefully distinguishes six milestones people conflate on the path to software engineering automation — from "AI writes 90% of code lines" (already happening) through "90% less demand for software engineers" (not close) — and estimates current total factor productivity from coding models at 15-20%. On safety, he opposes the proposed 10-year federal moratorium on state AI regulation, calling it reckless.

**Source**: [056: Dario Amodei — The highest-stakes financial model in history](../sources/056-dwarkesh-patel-dario-amodei-interview.md)

### The SaaSpocalypse Hits the Credit Markets

Griffonomics ([065](../sources/065-griffonomics-saaspocalypse.md)) delivers the most rigorous economic analysis of AI's impact on the software industry in the collection. The central thesis: the shift from the "tool era" to the "agentic era" does not just hurt SaaS stock prices — it detonates a hidden debt bomb. Private equity firms spent a decade buying "lazy SaaS" companies at 20x EBITDA via leveraged buyouts, betting on perpetual seat growth. With agents collapsing per-seat revenue, software now represents 31% of distressed names in the syndicated loan market, and PIK usage at 11.3% means these companies are paying debt by taking on more debt. This debt is packaged into CLOs and BDCs held by pension funds, insurance companies, and endowments — contagion beyond tech. The "hollow middle" thesis is equally sharp: youth unemployment (16-24) has spiked above 10% as entry-level cognitive work — the first rung of the white-collar career ladder — is exactly what agents automate first. The result is a wealth boom without a jobs boom.

**Source**: [065: SaaS-pocalypse: The Death of Seat-Based Software vs. The $600B AI Arms Race](../sources/065-griffonomics-saaspocalypse.md)

### Inference-Time Scaling: The 100x Compute Reduction

Prompt Engineering ([060](../sources/060-prompt-engineering-100x-breakthrough.md)) unpacks Google DeepMind's Gemini 3 Deep Think release and argues the real story is not benchmarks but a paradigm shift. Deep Think achieved a 100x reduction in compute required for Olympiad-level performance between July 2025 and January 2026 — not by making the model bigger, but by allocating compute more intelligently at inference time. The Althia research agent adds a generate-verify-revise loop that outperformed raw Deep Think at every compute scale tested, hitting 91.9% on Advanced Proof Bench (previous record: 65.7%). The meta-lesson is consistent across multiple teams: Poetic's agentic harness on Gemini 3 Pro beat earlier Deep Think at one-third the cost, and changing just the tools a model has access to yielded 5-8% performance gains — more than a typical model generation upgrade. DeepMind's own research taxonomy is refreshingly honest: they explicitly claim no Level 3 or Level 4 results, only Level 0-2. The practical implication is clear — invest in the orchestration layer around the model, not just the model itself.

**Source**: [060: The 100x AI Breakthrough No One is Talking About](../sources/060-prompt-engineering-100x-breakthrough.md)

### Zero Trust for AI Agents: IBM's Security Blueprint

IBM Technology ([057](../sources/057-ibm-zero-trust-ai-agents.md)) builds the first structured enterprise security framework for agentic AI by mapping zero trust principles onto the agent threat surface. The core insight: autonomous agents hold non-human identities (API keys, service accounts, tokens) at a fundamentally different scale than human users, and each can spawn sub-agents that need their own credentials. The framework identifies six attack vectors — prompt injection, policy/model poisoning, interface interception (including MCP calls), tool/API compromise, credential theft, and privilege escalation — and maps concrete countermeasures to each: credential vaults with dynamic just-in-time access, tool registries with vetted inventories, AI firewalls for input/output inspection, immutable audit logs, and human-in-the-loop kill switches with throttling. The "assume breach" principle — designing every control assuming the agent is already compromised — is positioned as the most important and most overlooked tenet for agentic deployments.

**Source**: [057: Securing AI Agents with Zero Trust](../sources/057-ibm-zero-trust-ai-agents.md)

## Also Ingested

- **[044: I Just Did a Full Day of Analyst Work in 10 Minutes](../sources/044-nate-b-jones-claude-excel-powerpoint.md)** — Nate B Jones demonstrates Claude in Excel/PowerPoint producing Goldman-Sachs-validated operating models in 10 minutes. The deeper thesis: value is migrating from the application layer (Microsoft) to a "context layer" (Anthropic) — the AI's accumulated understanding of your data. Every model upgrade silently improves every Claude-powered document. The flip side is "work slop" — polished AI artifacts that look competent but say nothing, estimated at $186/employee/month in lost productivity.

- **[045: Sam Altman said what???](../sources/045-primetime-altman-townhall-biosecurity.md)** — ThePrimeTime reacts to Altman's town hall prediction of 100x cheaper GPT-5.2 intelligence by end of 2027 and his candid admission that AI-enabled bioterrorism could be what goes "visibly really wrong" in 2026. Prime identifies the core paradox: the company creating the risk positions itself as the indispensable fix, while model distillation makes access restrictions structurally impossible.

- **[046: The Rise of WebMCP](../sources/046-sam-witteveen-webmcp.md)** — Sam Witteveen covers WebMCP, a Google/Microsoft standard in Chrome preview that lets websites expose structured tools to AI agents. Two APIs: declarative (HTML form annotation) and imperative (JavaScript tool definitions). Replaces thousands of screenshot-based interactions with single function calls. Human-in-the-loop by design with explicit coordination handoff points.

- **[048: Before You Build Another Agent, Understand This MIT Paper](../sources/048-brainqub3-recursive-language-models.md)** — Brainqub3 breaks down the Recursive Language Models paper showing context rot is two-dimensional (length x complexity). Complex documents are dependency graphs, not linear text. The RLM architecture assigns documents to Python REPL variables and uses code execution + recursion to traverse them — cheaper and more reliable than context stuffing, summarization, or RAG for high-complexity tasks.

- **[049: Anthropic Found Out Why AIs Go Insane](../sources/049-two-minute-papers-assistant-axis.md)** — Two Minute Papers covers the "assistant axis" research: a geometric direction in activation space encoding the helpful-assistant persona. Models drift from this axis during extended conversations, especially around philosophy and emotional topics. "Activation capping" (lane-keep assist, not locked steering) halved jailbreak success rates while preserving capability. The axis geometry is universal across model families.

- **[051: You're using Claude Code Wrong](../sources/051-simon-scrapes-claude-code-tips.md)** — Simon Scrapes provides the clearest disambiguation of Claude Code's extensibility stack: slash commands (user-triggered), skills (auto-invoked context bundles), hooks (zero-token programmatic checks), and plugins (distributable packages). Key practices: keep CLAUDE.md under 30 lines as a pointer file, install Gemini CLI as a research fallback, and add systematic fact-checking as a final content workflow step.

- **[053: From AI Curiosity to Operational Traction](../sources/053-hr-com-ai-operational-traction-wfm.md)** — HR.com interviews a WFM industry veteran who argues organizations are stuck framing AI agents as "cheaper humans." The reframe: map AI to decisions, not tasks. His autonomy tier model (observe, recommend, execute, escalate) is the most practical enterprise deployment framework in the collection. The 80% stat on payroll errors tracing to bad time data is a reminder that upstream data quality outweighs downstream AI capability.

- **[058: The TRUTH About OpenClaw AI Agents](../sources/058-krakowski-openclaw-agents.md)** — Krakowski runs 14 OpenClaw agents as "virtual employees" across M1 Mac Minis and Hetzner VPS instances, communicating via per-agent Slack channels. Despite the autonomous branding, he admits "I'm directing everything" — matching the 70/30 human-agent split seen in source 032. The most substantive content is the isolation practice (dedicated hardware, separate credentials, no email access), though his claim that model quality alone mitigates prompt injection is a significant misunderstanding. Useful as a case study of how agent tech is marketed to non-technical business audiences.

- **[061: Become an AI-first construction company](../sources/061-fairley-ai-first-construction.md)** — Tim Fairley delivers one of the most grounded enterprise adoption frameworks in the collection. His position — "the future is massively overhyped but current capabilities are largely underhyped" — rejects both techno-utopianism and dismissiveness. The sharpest admission: AI quantity takeoff tools charge $20-30 per drawing without 100% accuracy, when an Upwork contractor delivers better results at a quarter of the cost. His four-part sweet spot test (clear inputs/outputs, definable rules, human-verifiable outputs, limited hallucination risk) is immediately actionable for any enterprise adoption decision.

- **[062: Every Level of Claude Code Explained](../sources/062-simon-scrapes-claude-code-levels.md)** — Simon Scrapes maps Claude Code proficiency into seven levels from basic prompting through fully autonomous pipelines (RAWL). Key insight: most users plateau at level 2-3 and never reach the agent orchestration tiers. The skills-commands-hooks mental model (skills = how Claude thinks, hooks = what happens automatically, commands = what you trigger manually) is the cleanest disambiguation in the collection. Context rot data point: at ~10,000 tokens, approximately 50% of context fidelity is lost.

- **[063: Claude Cowork Plugins Guide](../sources/063-ben-ai-cowork-plugins.md)** — Ben AI walks through Anthropic's new plugin system for Claude Cowork, which bundles skills, connections (MCPs, built-in connectors, browser access), and slash commands into department-specific packages. The broader claim: plugins position Claude as the meta-layer above all existing SaaS tools, potentially replacing the multi-tool workflow. When Anthropic launched plugins, companies like Salesforce, ServiceNow, and Adobe saw immediate stock drops. Three plugin economies are emerging: Anthropic-built, third-party, and custom.

- **[064: One Prompt Every Agentic Codebase Should Have](../sources/064-indydevdan-agentic-prompt.md)** — IndyDevDan argues every codebase should have a standardized install/maintain prompt that combines deterministic scripts with agentic intelligence for setup, onboarding, and maintenance. The pattern: run deterministic hooks first, log results, then let an agentic prompt validate, resolve issues, and walk engineers through remaining configuration interactively. The justfile serves as a universal launchpad for both humans and agents. Core thesis: "agents combined with code beats either alone."

- **[066: Claude Cowork Full Tutorial](../sources/066-brooke-wright-cowork-tutorial.md)** — Brooke Wright demonstrates five real-world Claude Cowork use cases (file organization, podcast clipping, content gap analysis, email auditing, productivity dashboards). The gap between the stock-market narrative ("this tool crashed the stock market") and the demonstrated capabilities (sorting downloads, trimming podcast clips) is itself a data point on hype dynamics. Confirms Anthropic's strategy to extend Claude Code's agentic architecture to non-developer audiences.

- **[067: Learn 90% of Claude Code Agent Teams](../sources/067-bart-slodyczka-agent-teams-course.md)** — Bart Slodyczka's follow-up to his earlier agent teams demo ([004](../sources/004-bart-slodyczka-agent-teams.md)) shifts from "look what this can do" to "here's how to use it properly." The three-mode mental model (default, sub-agent, agent teams) is driven by token conservation: each mode manages finite context windows differently. Practical details include tmux split-pane setup, race condition protection for shared task lists, model selection per teammate for cost optimization, and the skill-creation workflow for turning ad hoc team processes into repeatable operations.

## Connections to Existing Sources

- **054 ↔ [041: The $20K C Compiler](../sources/041-awesome-claude-compiler-critique.md)**: Back-to-back multi-agent failure case studies — Anthropic's $20K compiler and Cursor's $8-16M browser both demonstrate that scaling agent count without validation produces impressive volume and broken software. The pattern is now well-established across companies.
- **054 ↔ [004: Agent Teams Are Insane](../sources/004-bart-slodyczka-agent-teams.md)**: Java Brains' debunking is the direct empirical counterpoint to the enthusiastic agent-team demos — same multi-agent architecture, opposite real-world results when subjected to scrutiny.
- **054 ↔ [029: 150 Developer Study](../sources/029-modern-software-engineering-ai-study.md)**: Cursor removing the integrator agent mirrors Farley's finding that AI produces "more code than necessary" — at 3 million lines with 88% CI failure, code volume without quality is anti-value.
- **055 ↔ [010: Multi-Agent Orchestration](../sources/010-indydevdan-multi-agent-orchestration.md)**: Brainqub3's measurement rig provides the empirical framework that IndyDevDan's practical orchestration patterns need — measure first, then decide whether multi-agent is justified.
- **055 ↔ [020: Simon Scrapes Agent Teams](../sources/020-simon-scrapes-agent-teams.md)**: Simon's complexity ranking system for when to use agent teams maps to Brainqub3's empirical finding that single-agent baseline performance is the decision heuristic.
- **050 ↔ [025: AI Productivity Bubble](../sources/025-natasha-bernal-ai-productivity-bubble.md)**: Harris's "success is the emergency" extends Bernal's productivity reckoning argument from employer-level to civilizational-level — even if AI delivers on promises, the wealth concentration is its own crisis.
- **050 ↔ [036: Did AI Just Kill Software?](../sources/036-prof-g-ai-kill-software.md)**: Harris adds the political dimension Galloway avoids — both analyze AI economic disruption, but Harris argues the optimistic scenario demands the same urgency as the pessimistic one.
- **050 ↔ [012: Career Collapse](../sources/012-nate-b-jones-career-collapse.md)**: Harris's "fall of Saigon" metaphor puts a timeline on Jones's career disruption thesis — the entry-level hiring freeze is the first visible signal.
- **052 ↔ [002: Anthropic CEO Philosophy](../sources/002-nate-b-jones-anthropic-ceo-philosophy.md)**: Sharma's resignation directly contradicts Amodei's stated safety-first philosophy — internal departures are stronger signals than public positions.
- **052 ↔ [039: SaaSpocalypse](../sources/039-pivot-to-ai-saaspocalypse.md)**: Novara Media's polycrisis framework connects Gerard's SaaS sell-off to safety — if alignment-flawed systems are replacing the professional services backbone, the compound risk is multiplicative.
- **047 ↔ [017: Skills Security](../sources/017-primeagen-skills-security.md)**: TheStandupPod's AI browser security analysis extends Prime's skills supply chain concerns to the browser layer — prompt injection attacks on agents now have a delivery mechanism through any web page.
- **048 ↔ [011: Context Engineering](../sources/011-confluent-developer-context-engineering.md)**: The RLM paper's context rot finding (two-dimensional: length x complexity) provides the theoretical foundation for why Berglund's context engineering practices matter — bigger windows do not solve harder problems.
- **044 ↔ [033: Ethan Mollick — Why CEOs Get AI Wrong](../sources/033-prof-g-ethan-mollick-ai-wrong.md)**: Jones's "context layer" thesis and Mollick's "jagged frontier" are complementary — value migrates to understanding where AI excels and where it hallucinates, which is the judgment layer both describe.
- **046 ↔ [030: Playwright CLI vs MCP](../sources/030-playwright-cli-vs-mcp.md)**: WebMCP represents the next evolution beyond Playwright's browser automation approach — structured tool exposure replaces DOM manipulation entirely.
- **053 ↔ [025: AI Productivity Bubble](../sources/025-natasha-bernal-ai-productivity-bubble.md)**: Jensen's "assistive intelligence" framing and autonomy tiers are the enterprise deployment answer to Bernal's observation that most AI projects remain stuck in experimentation — map AI to decisions, not tasks.
- **056 ↔ [052: Anthropic Safety Crisis](../sources/052-novara-media-anthropic-safety-crisis.md)**: Amodei's claims about Anthropic's safety commitment in the Dwarkesh interview are directly contradicted by Sharma's resignation and "the world is in peril" warning — CEO messaging vs. internal reality.
- **056 ↔ [050: AI Economy Emergency](../sources/050-sam-harris-ai-economy-emergency.md)**: Amodei provides the insider economics (10x revenue scaling, 15-20% TFP improvement) that Harris's "success is the emergency" thesis needs to ground its timeline — the CEO's own numbers make the displacement argument more concrete.
- **056 ↔ [029: 150 Developer Study](../sources/029-modern-software-engineering-ai-study.md)**: Amodei's dismissal of the METR 20% productivity downlift finding creates the sharpest insider-vs-empirical tension in the collection — the CEO of Anthropic disagrees with the only rigorous measurement study.
- **065 ↔ [039: SaaSpocalypse](../sources/039-pivot-to-ai-saaspocalypse.md)**: Griffonomics extends Gerard's SaaS sell-off analysis into the credit markets — the PE debt bomb (31% of distressed syndicated loans) is the systemic risk that equities-focused coverage missed entirely.
- **065 ↔ [050: AI Economy Emergency](../sources/050-sam-harris-ai-economy-emergency.md)**: Griffonomics's "hollow middle" thesis (youth unemployment above 10% as entry-level cognitive work is automated) is the specific mechanism behind Harris's "fall of Saigon" metaphor — the career ladder is being sawed off from the bottom.
- **065 ↔ [063: Cowork Plugins](../sources/063-ben-ai-cowork-plugins.md)**: The SaaS stock crash Griffonomics documents was triggered by the same Cowork plugin launch Ben AI demonstrates — connecting the investor panic to the actual product capabilities reveals the hype gap.
- **060 ↔ [055: Multi-Agent Measurement](../sources/055-brainqub3-multi-agent-measurement.md)**: DeepMind's finding that agentic harnesses outperform raw models at every compute scale validates Brainqub3's measurement approach — the orchestration layer is where real gains come from, not bigger models or more agents.
- **060 ↔ [048: Recursive Language Models](../sources/048-brainqub3-recursive-language-models.md)**: Althia's generate-verify-revise loop is a concrete implementation of the recursive decomposition pattern the RLM paper theorizes — breaking complex problems into verifiable sub-steps outperforms brute-force context stuffing.
- **057 ↔ [017: Skills Security](../sources/017-primeagen-skills-security.md)**: IBM's tool registry and vetted inventory concept is the enterprise-grade answer to Prime's skills supply chain concerns — register, vet, and audit every tool before agents can use it.
- **057 ↔ [047: AI Browser Security](../sources/047-standuppod-ai-browser-security.md)**: IBM's AI firewall layer (inspecting inputs for prompt injection, outputs for data leakage) addresses the exact attack vectors TheStandupPod identified in AI browsers — defense in depth at the agent boundary.
- **062 ↔ [051: Claude Code Tips](../sources/051-simon-scrapes-claude-code-tips.md)**: Simon's levels framework expands his earlier tips into a structured progression — the same skills-commands-hooks mental model, now placed in context of seven mastery levels from prompting through autonomous pipelines.
- **064 ↔ [001: Claude Code Task System](../sources/001-indydevdan-claude-code-task-system.md)**: IndyDevDan's install/maintain prompt extends his earlier task system work to codebase lifecycle management — the same "deterministic + agentic" pattern applied to onboarding and maintenance rather than feature development.
- **067 ↔ [004: Agent Teams](../sources/004-bart-slodyczka-agent-teams.md)**: Bart's follow-up shifts from "look what this can do" to "here's how to use it properly" — the three-mode mental model and tmux-based session management are the operational foundation his earlier demo was missing.
- **061 ↔ [053: Operational Traction](../sources/053-hr-com-ai-operational-traction-wfm.md)**: Fairley's construction-industry adoption framework and Jensen's WFM autonomy tiers are converging on the same insight from different industries — map AI to decisions where intelligence was previously uneconomical, not to tasks where humans already work.
- **066 ↔ [039: SaaSpocalypse](../sources/039-pivot-to-ai-saaspocalypse.md)**: Wright's practical Cowork demos (sorting downloads, trimming podcasts) provide the reality check against the "billions in SaaS market cap wiped out" narrative — the gap between investor panic and actual product capability is a hype signal.

## Action Items

- [ ] Create a synthesis doc on "multi-agent economics and measurement" — sources [004](../sources/004-bart-slodyczka-agent-teams.md), [010](../sources/010-indydevdan-multi-agent-orchestration.md), [020](../sources/020-simon-scrapes-agent-teams.md), [041](../sources/041-awesome-claude-compiler-critique.md), [054](../sources/054-java-brains-cursor-browser-hype.md), [055](../sources/055-brainqub3-multi-agent-measurement.md) now form a complete arc from hype to debunking to empirical measurement
- [ ] Evaluate Brain Cube Agent Labs ([055](../sources/055-brainqub3-multi-agent-measurement.md)) for hands-on testing — the tool is open-source and could validate multi-agent patterns discussed in Module 05
- [ ] Create a synthesis doc on "AI safety signals" — sources [002](../sources/002-nate-b-jones-anthropic-ceo-philosophy.md), [017](../sources/017-primeagen-skills-security.md), [045](../sources/045-primetime-altman-townhall-biosecurity.md), [047](../sources/047-standuppod-ai-browser-security.md), [049](../sources/049-two-minute-papers-assistant-axis.md), [052](../sources/052-novara-media-anthropic-safety-crisis.md) form a safety thread spanning interpretability research, browser attack surfaces, biosecurity risks, and institutional safety culture failures
- [ ] Track WebMCP ([046](../sources/046-sam-witteveen-webmcp.md)) rollout at Google Cloud Next / Google I/O — if it gains traction, it fundamentally changes the MCP-vs-browser-automation calculus covered in Module 04
- [ ] Add Jensen's autonomy tier model ([053](../sources/053-hr-com-ai-operational-traction-wfm.md)) to Module 06 as a practical enterprise deployment framework — observe/recommend/execute/escalate is the most actionable schema in the collection
- [ ] Update Module 06 economics section with Harris's ([050](../sources/050-sam-harris-ai-economy-emergency.md)) white-collar inversion thesis and "success is the emergency" framing as a counterpoint to the productivity-optimist sources
- [ ] Create a synthesis doc on "SaaS disruption and credit contagion" — sources [036](../sources/036-prof-g-ai-kill-software.md), [039](../sources/039-pivot-to-ai-saaspocalypse.md), [063](../sources/063-ben-ai-cowork-plugins.md), [065](../sources/065-griffonomics-saaspocalypse.md), [066](../sources/066-brooke-wright-cowork-tutorial.md) form a complete arc from investor panic to PE debt bomb to actual product capabilities, with the gap between narrative and reality as the throughline
- [ ] Add Amodei's six-milestone software engineering automation spectrum ([056](../sources/056-dwarkesh-patel-dario-amodei-interview.md)) to Module 06 as the definitive insider framing of the automation timeline — it disambiguates the "AI replaces developers" discourse into measurable stages
- [ ] Track the METR productivity study vs. Amodei's "unambiguous gains" claim — this is the sharpest insider-vs-empirical disagreement in the collection and resolving it is critical for the Module 01 hype-vs-reality foundation
- [ ] Add IBM's zero trust agent security framework ([057](../sources/057-ibm-zero-trust-ai-agents.md)) to the safety synthesis — the six attack vectors and countermeasures map should be cross-referenced with the skills supply chain concerns from [017](../sources/017-primeagen-skills-security.md)
- [ ] Evaluate inference-time compute scaling ([060](../sources/060-prompt-engineering-100x-breakthrough.md)) implications for Module 04 — if orchestration layers consistently outperform raw models, the agentic patterns curriculum should prioritize harness design over model selection
- [ ] Add IndyDevDan's install/maintain prompt pattern ([064](../sources/064-indydevdan-agentic-prompt.md)) to Module 03 as a standard practice for codebase lifecycle management
